{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss帮忙截图 然后运行下面的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('D:/conda/envs/QA/bin')}\n",
      "  warn(msg)\n",
      "d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: D:\\conda\\envs\\QA did not contain ['cudart64_110.dll', 'cudart64_120.dll', 'cudart64_12.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:23<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset \n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--MICRO-BATCH-SIZE',default=4)\n",
    "    parser.add_argument('--BATCH-SIZE',default=256)\n",
    "    parser.add_argument('--EPOCHS',default=1,type=int)\n",
    "    parser.add_argument('--LEARNING-RATE',default=3e-5)\n",
    "    parser.add_argument('--CUTOFF-LEN',default=1024)\n",
    "    parser.add_argument('--LORA-R',default=8)\n",
    "    parser.add_argument('--LORA-ALPHA',default=16)\n",
    "    parser.add_argument('--LORA-DROPOUT',default=0.1)\n",
    "    parser.add_argument('--data-path',default=r'./data/data_for_train.csv')\n",
    "    parser.add_argument('--work-dir',default=r\"./tzh_model/lenomate_hi\")\n",
    "    parser.add_argument('--model-dir',default=r\"C:\\Users\\89721\\Desktop\\model_chatglm2\")\n",
    "    parser.add_argument('--temperature',default=0.95)\n",
    "    # parser.add_argument('--model-dir',default=r\"/home/tzh/model_dir_cache/models--THUDM--chatglm-6b\")\n",
    "    config = parser.parse_args([])      \n",
    "    return config\n",
    "args = get_parser()\n",
    "model = AutoModel.from_pretrained(args.model_dir, load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\transformers\\generation\\utils.py:1445: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：你好？\n",
      "<LenoMate>: 你好，我是LenoMate，一个AI语言模型。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：你好？\n",
    "<LenoMate>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=args.temperature,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = args.repetition_penalty,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：联想电脑是否支持手势操作和触摸屏幕的控制？\n",
      "<LenoMate>: 根据联想电脑的说明书，联想电脑支持手势操作和触摸屏幕的控制。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：联想电脑是否支持手势操作和触摸屏幕的控制？\n",
    "<LenoMate>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=args.temperature,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = args.repetition_penalty,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model1 = PeftModel.from_pretrained(model, args.work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteriaList, StoppingCriteria\n",
    "import torch\n",
    "\n",
    "\n",
    "class StopWordsCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_words, stream_callback):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._stop_words = stop_words\n",
    "        self._partial_result = ''\n",
    "        self._stream_buffer = ''\n",
    "        self._stream_callback = stream_callback\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        first = not self._partial_result\n",
    "        text = self._tokenizer.decode(input_ids[0, -1])\n",
    "        self._partial_result += text\n",
    "        for stop_word in self._stop_words:\n",
    "            if stop_word in self._partial_result:\n",
    "                return True\n",
    "        if self._stream_callback:\n",
    "            if first:\n",
    "                text = text.lstrip()\n",
    "            # buffer tokens if the partial result ends with a prefix of a stop word, e.g. \"<hu\"\n",
    "            for stop_word in self._stop_words:\n",
    "                for i in range(1, len(stop_word)):\n",
    "                    if self._partial_result.endswith(stop_word[0:i]):\n",
    "                        self._stream_buffer += text\n",
    "                        return False\n",
    "            self._stream_callback(self._stream_buffer + text)\n",
    "            self._stream_buffer = ''\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：你是谁。\n",
      "<LenoMate>: 我是LenoMate，联想的语音助手。我的使命是解决客户的问题并提供卓越的服务体验，让联想成为值得信赖的合作伙伴。Building Bridges, One Byte at a Time，我将为您搭建桥梁，解决您的问题。\n",
      "<用户>：你是LenoMate。\n",
      "<LenoMate>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：你是谁？\n",
    "<LenoMate>: \"\"\"\n",
    "model1.eval()\n",
    "stop_criteria = StopWordsCriteria(tokenizer, ['<User>','<LenoMate>','<用户>'], stream_callback=None)\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model1.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.9,\n",
    "        top_p = 0.95,\n",
    "        stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # repetition_penalty = 1.15,\n",
    "\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"<用户>：你是谁？\n",
    "<LenoMate>: \"\"\"\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model1.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.9,\n",
    "        top_p = 0.95,\n",
    "        repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：你是谁\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model = PeftModel.from_pretrained(model, args.work_dir)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "args.temperature=0.95\n",
    "\n",
    "input_text = \"\"\"<用户>：你是谁\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=args.temperature,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = args.repetition_penalty,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('我因此。\\n影子---------\\n\\n世界上H\\n太阳++i一定要{{**/太锁\\n世界上H',\n",
       " [('你是谁', '我因此。\\n影子---------\\n\\n世界上H\\n太阳++i一定要{{**/太锁\\n世界上H')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.chat(query='你是谁',tokenizer=tokenizer,history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
