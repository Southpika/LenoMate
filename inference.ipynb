{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss帮忙截图 然后运行下面的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\admin\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Users\\admin\\anaconda3\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.qwen.configuration_qwen.QWenConfig'> for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\LenoMate\\inference.ipynb 单元格 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/LenoMate/inference.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m config\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/LenoMate/inference.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m args \u001b[39m=\u001b[39m get_parser()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/admin/LenoMate/inference.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(args\u001b[39m.\u001b[39;49mmodel_dir, load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/admin/LenoMate/inference.ipynb#W1sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(args\u001b[39m.\u001b[39mmodel_dir, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:519\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    518\u001b[0m     )\n\u001b[1;32m--> 519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    520\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    521\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.qwen.configuration_qwen.QWenConfig'> for this kind of AutoModel: AutoModel.\nModel type should be one of AlbertConfig, AlignConfig, AltCLIPConfig, ASTConfig, AutoformerConfig, BarkConfig, BartConfig, BeitConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitConfig, BlenderbotConfig, BlenderbotSmallConfig, BlipConfig, Blip2Config, BloomConfig, BridgeTowerConfig, CamembertConfig, CanineConfig, ChineseCLIPConfig, ClapConfig, CLIPConfig, CLIPSegConfig, CodeGenConfig, ConditionalDetrConfig, ConvBertConfig, ConvNextConfig, ConvNextV2Config, CpmAntConfig, CTRLConfig, CvtConfig, Data2VecAudioConfig, Data2VecTextConfig, Data2VecVisionConfig, DebertaConfig, DebertaV2Config, DecisionTransformerConfig, DeformableDetrConfig, DeiTConfig, DetaConfig, DetrConfig, DinatConfig, Dinov2Config, DistilBertConfig, DonutSwinConfig, DPRConfig, DPTConfig, EfficientFormerConfig, EfficientNetConfig, ElectraConfig, EncodecConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FlavaConfig, FNetConfig, FocalNetConfig, FSMTConfig, FunnelConfig, GitConfig, GLPNConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GPTSanJapaneseConfig, GraphormerConfig, GroupViTConfig, HubertConfig, IBertConfig, IdeficsConfig, ImageGPTConfig, InformerConfig, JukeboxConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LevitConfig, LiltConfig, LlamaConfig, LongformerConfig, LongT5Config, LukeConfig, LxmertConfig, M2M100Config, MarianConfig, MarkupLMConfig, Mask2FormerConfig, MaskFormerConfig, MaskFormerSwinConfig, MBartConfig, MCTCTConfig, MegaConfig, MegatronBertConfig, MgpstrConfig, MobileBertConfig, MobileNetV1Config, MobileNetV2Config, MobileViTConfig, MobileViTV2Config, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NatConfig, NezhaConfig, NllbMoeConfig, NystromformerConfig, OneFormerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, OwlViTConfig, PegasusConfig, PegasusXConfig, PerceiverConfig, PLBartConfig, PoolFormerConfig, ProphetNetConfig, PvtConfig, QDQBertConfig, ReformerConfig, RegNetConfig, RemBertConfig, ResNetConfig, RetriBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SamConfig, SegformerConfig, SEWConfig, SEWDConfig, Speech2TextConfig, SpeechT5Config, SplinterConfig, SqueezeBertConfig, SwiftFormerConfig, SwinConfig, Swin2SRConfig, Swinv2Config, SwitchTransformersConfig, T5Config, TableTransformerConfig, TapasConfig, TimeSeriesTransformerConfig, TimesformerConfig, TimmBackboneConfig, TrajectoryTransformerConfig, TransfoXLConfig, TvltConfig, UMT5Config, UniSpeechConfig, UniSpeechSatConfig, VanConfig, VideoMAEConfig, ViltConfig, VisionTextDualEncoderConfig, VisualBertConfig, ViTConfig, ViTHybridConfig, ViTMAEConfig, ViTMSNConfig, VivitConfig, Wav2Vec2Config, Wav2Vec2ConformerConfig, WavLMConfig, WhisperConfig, XCLIPConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YolosConfig, YosoConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset \n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--MICRO-BATCH-SIZE',default=4)\n",
    "    parser.add_argument('--BATCH-SIZE',default=256)\n",
    "    parser.add_argument('--EPOCHS',default=1,type=int)\n",
    "    parser.add_argument('--LEARNING-RATE',default=3e-5)\n",
    "    parser.add_argument('--CUTOFF-LEN',default=1024)\n",
    "    parser.add_argument('--LORA-R',default=8)\n",
    "    parser.add_argument('--LORA-ALPHA',default=16)\n",
    "    parser.add_argument('--LORA-DROPOUT',default=0.1)\n",
    "    parser.add_argument('--data-path',default=r'./data/data_for_train.csv')\n",
    "    parser.add_argument('--work-dir',default=r\"./tzh_model/lenomate_hi\")\n",
    "    parser.add_argument('--model-dir',default=r\"C:\\Users\\89721\\Desktop\\model_chatglm2\")\n",
    "    parser.add_argument('--temperature',default=0.95)\n",
    "    # parser.add_argument('--model-dir',default=r\"/home/tzh/model_dir_cache/models--THUDM--chatglm-6b\")\n",
    "    config = parser.parse_args([])      \n",
    "    return config\n",
    "args = get_parser()\n",
    "model = AutoModel.from_pretrained(args.model_dir, load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\transformers\\generation\\utils.py:1445: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：你好？\n",
      "<LenoMate>: 你好，我是LenoMate，一个AI语言模型。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：你好？\n",
    "<LenoMate>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=args.temperature,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = args.repetition_penalty,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：联想电脑是否支持手势操作和触摸屏幕的控制？\n",
      "<LenoMate>: 根据联想电脑的说明书，联想电脑支持手势操作和触摸屏幕的控制。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：联想电脑是否支持手势操作和触摸屏幕的控制？\n",
    "<LenoMate>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=args.temperature,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = args.repetition_penalty,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model1 = PeftModel.from_pretrained(model, args.work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot merge LORA layers when the model is loaded in 8-bit mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model1\u001b[39m.\u001b[39;49mmerge_and_unload()\n",
      "File \u001b[1;32md:\\conda\\envs\\QA\\lib\\site-packages\\peft\\tuners\\lora.py:339\u001b[0m, in \u001b[0;36mLoraModel.merge_and_unload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGPT2 models are not supported for merging LORA layers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mis_loaded_in_8bit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot merge LORA layers when the model is loaded in 8-bit mode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    341\u001b[0m key_list \u001b[39m=\u001b[39m [key \u001b[39mfor\u001b[39;00m key, _ \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnamed_modules() \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlora\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m key]\n\u001b[0;32m    342\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_list:\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot merge LORA layers when the model is loaded in 8-bit mode"
     ]
    }
   ],
   "source": [
    "model1.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteriaList, StoppingCriteria\n",
    "import torch\n",
    "\n",
    "\n",
    "class StopWordsCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_words, stream_callback):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._stop_words = stop_words\n",
    "        self._partial_result = ''\n",
    "        self._stream_buffer = ''\n",
    "        self._stream_callback = stream_callback\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        first = not self._partial_result\n",
    "        text = self._tokenizer.decode(input_ids[0, -1])\n",
    "        self._partial_result += text\n",
    "        for stop_word in self._stop_words:\n",
    "            if stop_word in self._partial_result:\n",
    "                return True\n",
    "        if self._stream_callback:\n",
    "            if first:\n",
    "                text = text.lstrip()\n",
    "            # buffer tokens if the partial result ends with a prefix of a stop word, e.g. \"<hu\"\n",
    "            for stop_word in self._stop_words:\n",
    "                for i in range(1, len(stop_word)):\n",
    "                    if self._partial_result.endswith(stop_word[0:i]):\n",
    "                        self._stream_buffer += text\n",
    "                        return False\n",
    "            self._stream_callback(self._stream_buffer + text)\n",
    "            self._stream_buffer = ''\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：你叫什么名字\n",
      "<LenoMate>: 您好！我是LenoMate，联想的语音助手。Building Bridges, One Byte at a Time，我将为您搭建沟通的桥梁，解决您的问题。\n",
      "<用户>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：你叫什么名字\n",
    "<LenoMate>: \"\"\"\n",
    "model1.eval()\n",
    "stop_criteria = StopWordsCriteria(tokenizer, ['<User>','<LenoMate>','<用户>'], stream_callback=None)\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model1.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.9,\n",
    "        top_p = 0.95,\n",
    "        stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # repetition_penalty = 1.15,\n",
    "\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"<用户>：你是谁？\n",
    "<LenoMate>: \"\"\"\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model1.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.9,\n",
    "        top_p = 0.95,\n",
    "        repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>：你是谁\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model = PeftModel.from_pretrained(model, args.work_dir)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "args.temperature=0.95\n",
    "\n",
    "input_text = \"\"\"<用户>：你是谁\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=args.temperature,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = args.repetition_penalty,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('我因此。\\n影子---------\\n\\n世界上H\\n太阳++i一定要{{**/太锁\\n世界上H',\n",
       " [('你是谁', '我因此。\\n影子---------\\n\\n世界上H\\n太阳++i一定要{{**/太锁\\n世界上H')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.chat(query='你是谁',tokenizer=tokenizer,history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
