{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ModelWhale 是什么"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "def display_answer(query, history = []):\n",
    "    # resp, history = get_knowledge_based_answer(query=query,\n",
    "    #                                            vector_store=vector_store,\n",
    "    #                                            chat_history=history)\n",
    "    display(Markdown(query))\n",
    "display_answer(query=\"ModelWhale 是什么\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 22 15:55:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.24       Driver Version: 528.24       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 18%   32C    P8    14W / 215W |      0MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile('./data/document_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 319/319 [00:00<?, ?B/s] \n",
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Opti7080\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 2.06MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 856/856 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 409M/409M [00:22<00:00, 18.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"shibing624/text2vec-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-4.4399e-04, -2.9735e-01,  8.5790e-01,  ..., -5.2770e-01,\n",
      "         -1.4316e-01, -1.0008e-01],\n",
      "        [ 6.5362e-01, -7.6667e-02,  9.5962e-01,  ..., -6.0122e-01,\n",
      "         -1.6797e-03,  2.1458e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "\n",
    "@nb.vectorize()\n",
    "def nb_vec_sin(a):\n",
    "    return sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25494302  0.35510558  0.65787946 ...  0.23777344  0.89951332\n",
      "  -0.69283581]\n",
      " [-0.21394402 -0.02146462  0.73679789 ...  0.53819302  0.95069866\n",
      "  -0.97943308]\n",
      " [ 0.73773584  0.5728007   0.56698357 ...  0.6696758  -0.91263461\n",
      "  -0.87383107]\n",
      " ...\n",
      " [-0.53538724 -0.62236235  0.46455885 ... -0.72469746 -0.43196779\n",
      "   0.1900131 ]\n",
      " [ 0.30064271 -0.95715459  0.9723309  ...  0.68655798  0.98785573\n",
      "  -0.93550842]\n",
      " [-0.49164463 -0.22650817  0.64530563 ...  0.73041068  0.87517238\n",
      "   0.74054606]]\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel, cos_sim\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "a = np.random.randn(100000,768)\n",
    "b = np.random.randn(1,768)\n",
    "start_time = time.time()\n",
    "c = cos_sim(a,b)\n",
    "duration_1 = time.time()-start_time\n",
    "start_time = time.time()\n",
    "print(nb_vec_sin(a))\n",
    "duration_2 = time.time()-start_time\n",
    "\n",
    "a = torch.tensor(a).to('cuda')\n",
    "b = torch.tensor(b).to('cuda')\n",
    "s_time = time.time()\n",
    "c_ = cos_sim(a,b)\n",
    "duration_3 = time.time()-s_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19809770584106445, 0.3446838855743408, 0.0020568370819091797)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_1,duration_2,duration_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91 µs ± 2.31 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "@jit(nopython=True) \n",
    "def t():\n",
    "    x = 0\n",
    "    for i in np.arange(5000):\n",
    "        x += i\n",
    "    return x\n",
    "%timeit t() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515 µs ± 768 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "232 µs ± 2.36 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(a)\n",
    "%timeit nb_vec_sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "from argparse import Namespace\n",
    "model_args = Namespace(do_mlm=None,pooler_type='cls', temp=0.05, mlp_only_train=False,init_embeddings_model=None)\n",
    "tokenizer = AutoTokenizer.from_pretrained('silk-road/luotuo-bert')\n",
    "model = AutoModel.from_pretrained('silk-road/luotuo-bert',trust_remote_code=True,model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "text_left = ['如何更换花呗绑定银行卡']\n",
    "\n",
    "queries = [\n",
    "    '如何更换花呗绑定银行卡',\n",
    "    'A man is eating pasta.',\n",
    "    'Someone in a gorilla costume is playing a set of drums.',\n",
    "    'A cheetah chases prey on across a field.']\n",
    "\n",
    "text_right = [\n",
    "    '花呗更改绑定银行卡',\n",
    "    '我什么时候开通了花呗',\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A man is riding a horse.',\n",
    "    'A woman is playing violin.',\n",
    "    'Two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]\n",
    "text_left = queries\n",
    "inputs = tokenizer(text_left, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_left = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
    "inputs = tokenizer(text_right, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_right = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
    "    \n",
    "cos_sim_matrix = torch.matmul(embeddings_left, embeddings_right.t())\n",
    "cos_sim_matrix /= torch.matmul(torch.norm(embeddings_left, dim=1, keepdim=True), torch.norm(embeddings_right, dim=1, keepdim=True).t())\n",
    "tensor_cpu = cos_sim_matrix.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    '如何更换花呗绑定银行卡',\n",
    "    'A man is eating pasta.',\n",
    "    'Someone in a gorilla costume is playing a set of drums.',\n",
    "    'A cheetah chases prey on across a field.']\n",
    "\n",
    "text_right = [\n",
    "    '花呗更改绑定银行卡',\n",
    "    '我什么时候开通了花呗',\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A man is riding a horse.',\n",
    "    'A woman is playing violin.',\n",
    "    'Two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)iguration_chatglm.py: 100%|██████████| 4.38k/4.38k [00:00<?, ?B/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)/modeling_chatglm.py: 100%|██████████| 59.4k/59.4k [00:00<00:00, 192kB/s]\n",
      "Downloading (…)main/quantization.py: 100%|██████████| 31.0k/31.0k [00:00<00:00, 581kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.89G/3.89G [04:53<00:00, 13.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (word_embeddings): Embedding(130528, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): QuantizedLinear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): QuantizedLinear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): QuantizedLinear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): QuantizedLinear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Load default cpu kernel failed:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Opti7080/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization.py\", line 178, in __init__\n",
      "    kernels = ctypes.cdll.LoadLibrary(kernel_file)\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\ctypes\\__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: [WinError 193] %1 不是有效的 Win32 应用程序。\n",
      "\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息,简洁和专业的来回答用户的问题。\n",
      "\n",
      "##已知内容:\n",
      "本机物理MAC地址是:cc:d9:ac:d8:57:06,\n",
      "本机电脑名是:DESKTOP-JHRQTGU.lenovo.com\n",
      "本机IP是:10.176.130.235\n",
      "本机硬盘序列号是:E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
      "本机CPU序列号是:BFEBFBFF000A0655\n",
      "本机主板序列号是:/41JYJ53/CNFCW0007K00LH/\n",
      "本机bios序列号是:41JYJ53\n",
      "##问题:\n",
      "我的GPU型号是什么\n",
      "##回答: 根据您提供的信息,您的GPU型号是 NVIDIA GeForce GTX 1050 Ti。\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"本机物理MAC地址是：cc:d9:ac:d8:57:06,\n",
    "本机电脑名是：DESKTOP-JHRQTGU.lenovo.com\n",
    "本机IP是：10.176.130.235\n",
    "本机硬盘序列号是：E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
    "本机CPU序列号是：BFEBFBFF000A0655\n",
    "本机主板序列号是：/41JYJ53/CNFCW0007K00LH/\n",
    "本机bios序列号是：41JYJ53\"\"\"\n",
    "inp = '我的GPU型号是什么'\n",
    "input_text = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n",
    "如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。\n",
    "##已知内容:\n",
    "{context}\n",
    "##问题:\n",
    "{inp}\n",
    "##回答：\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=300,\n",
    "        temperature=0,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息,简洁和专业的提取出数字,回答仅需填写一个数字。\n",
      "\n",
      "##已知内容:\n",
      "电脑当前音量为50\n",
      "##问题:\n",
      "请帮我降低音量,请问调整后音量的数字是多少\n",
      "##回答:\n",
      " 将音量降低到50,则调整后的音量为50。\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"电脑当前音量为50\"\"\"\n",
    "inp = '请帮我降低音量'\n",
    "input_text = f\"\"\"基于以下已知信息，简洁和专业的提取出数字，回答仅需填写一个数字。\n",
    "\n",
    "##已知内容:\n",
    "{context}\n",
    "##问题:\n",
    "{inp}，请问调整后音量的数字是多少\n",
    "##回答：\n",
    "\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        temperature=0,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.3\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>:假设你可以直接控制电脑,我将给你两个例子,请按照例子的形式进行回答。\n",
      "例子:\n",
      "已知电脑当前屏幕亮度为30,用户需要“帮我调高屏幕亮度”,则直接回答“由于您现在电脑当前亮度为30,已调至60”\n",
      "已知电脑当前屏幕亮度为50,用户需要“帮我调低屏幕亮度”,则直接回答“由于您现在电脑当前亮度为50,已调至20”\n",
      "问题:已知电脑当前屏幕亮度为80,用户需求“帮我调低屏幕亮度”,请模仿\"“由于您现在电脑当前亮度为X,已调至Y”\"进行回答:\n",
      "<bot>: 由于您现在电脑当前亮度为80,已调至50\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"电脑当前屏幕亮度为80\"\"\"\n",
    "inp = '帮我调低屏幕亮度'\n",
    "input_text = f\"\"\"<用户>：假设你可以直接控制电脑，我将给你两个例子，请按照例子的形式进行回答。\n",
    "例子：\n",
    "已知电脑当前屏幕亮度为30，用户需要“帮我调高屏幕亮度”，则直接回答“由于您现在电脑当前亮度为30，已调至60”\n",
    "已知电脑当前屏幕亮度为50，用户需要“帮我调低屏幕亮度”，则直接回答“由于您现在电脑当前亮度为50，已调至20”\n",
    "问题：已知{context}，用户需求“{inp}”，请模仿\"“由于您现在电脑当前亮度为X，已调至Y”\"进行回答：\n",
    "<bot>:\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.3,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.3\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai_user=pN1vaNEM/JFkEod5DPoEha|2023-06-08T02:24:16.518Z; ARRAffinity=4b55dda02c61fa6825af1a96193ff027e26cd4d1081dd1ee2ed88d88d93d93e9; ARRAffinitySameSite=4b55dda02c61fa6825af1a96193ff027e26cd4d1081dd1ee2ed88d88d93d93e9; .AspNetCore.Session=CfDJ8O%2ByR7SqPqxFlVz8eFNh6JFGsWDGqDT7Lzpzs4ImYU0jrnMFe6eWHIsbE52VYc8FRZV37J6ndDaxwjSxbvzE1fmeq53Mql%2B14DTKglIfKe9XpbajQ9UhJNbNo0v%2FYuluHZ5PoiEKJfG47olCOAp9n9QgGSclIv1Iak3PMKe9BclD; .AspNetCore.CookiesSSWeb=CfDJ8O-yR7SqPqxFlVz8eFNh6JEUTNyTeMBuOFEOcOPMwTcO-l0vrOFk0N9BKHtm_bRVY3W0u88P3I8fkCISyX_O7u15qf6PwF_D61PzBTgXJqHfBxQyHjf6rKXSu6YOduUWvsZG0qc4GNtZ7Jerm9yDkshdbEorp2nLmRpQ8obd91phvpKuqUxVl0avaXLOsBH8tOYY4FU7Zx6FtyNflVe6_-IkXRszZuU-9RLtwLSoKfMPjwMkfpUzUHECpB_puDrx4Hm6a6prhGkGKWifGk68HUZVUA9STh43cJklDVNwfwl6eBvUp8joU8TkaGoF1SHqN63HuOSpYv5gyjjS4Vd3Nls; ai_session=k434v/YYgul1PRSZB9GkaJ|1686191056621|1686192769861'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"请你帮我根据以下instruction的要求提取出关键信息：\n",
    "##instruction：要求格式为：\n",
    "- 是否为电脑操作：\n",
    "- 要求数字（未提及写未知）：\n",
    "##input：\n",
    "调整屏幕分辨率为1920*1080\n",
    "##output:\n",
    "\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.3,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>:请帮我打开微信\n",
      "<ChatGLM-6B>: 很抱歉,作为一个语言模型,我无法直接打开微信应用程序。但是,您可以使用您的设备上的微信应用程序图标来打开它。如果该图标没有打开,您可以尝试在设备的设置中找到微信并打开它。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：请帮我打开微信\n",
    "<ChatGLM-6B>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.1,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39m'\u001b[39m\u001b[39m./operation/exe_location.json\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     hash_map \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(\u001b[39m'\u001b[39;49m\u001b[39m./operation/exe_location.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if os.path.isfile('./operation/exe_location.json'):\n",
    "    hash_map = json.loads('./operation/exe_location.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = {\n",
    "    'QQMusic.exe':'C:/Program Files (x86)\\Tencent\\QQMusic',\n",
    "    'msedge.exe':'C:/Program Files (x86)\\Microsoft\\Edge\\Application'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./operation/exe_location.json', 'w') as write_f:\n",
    "\tjson.dump(hash_map, write_f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QQMusic.exe': 'C:/Program Files (x86)\\\\Tencent\\\\QQMusic', 'msedge.exe': 'C:/Program Files (x86)\\\\Microsoft\\\\Edge\\\\Application'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open('./operation/exe_location.json','r') as load_f:\n",
    "    hash_map = json.load(load_f)\n",
    "    print(hash_map)\n",
    "    print(type(load_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import screen_brightness_control as sbc\n",
    "sbc.get_brightness()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbc.get_brightness()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.set_brightness(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes libbitsandbytes_cuda116.dll\n",
      "function 'cadam32bit_grad_fp32' not found\n",
      "CUDA SETUP: Loading binary c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like THUDM/chatglm-6b-int4 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:1291\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1291\u001b[0m         \u001b[39mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1292\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConnection error, and we cannot find the requested files in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m the disk cache. Please try again or make sure your Internet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m connection is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[39m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel,LoraConfig,get_peft_model\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mTHUDM/chatglm-6b-int4\u001b[39;49m\u001b[39m\"\u001b[39;49m, load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mTHUDM/chatglm-6b-int4\u001b[39m\u001b[39m\"\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39msupports_gradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:444\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[39mif\u001b[39;00m kwargs_copy\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    442\u001b[0m         _ \u001b[39m=\u001b[39m kwargs_copy\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 444\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    445\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    446\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    447\u001b[0m         trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    449\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_copy,\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[0;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    927\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    929\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    576\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    627\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    630\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    631\u001b[0m         configuration_file,\n\u001b[0;32m    632\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    633\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    634\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    635\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    636\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    637\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    638\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    639\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    640\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    641\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    644\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\hub.py:452\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[0;32m    451\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    453\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this file, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory containing a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like THUDM/chatglm-6b-int4 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from peft import PeftModel,LoraConfig,get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM', inference_mode=True,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model, \"./tzh_model/medical_glm\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QQMusic.exe': 'C:/Program Files (x86)\\\\Tencent\\\\QQMusic'}\n"
     ]
    }
   ],
   "source": [
    "from operation.operation import Operation3\n",
    "inps = {'inputs':input('Input: ')}\n",
    "test = Operation3(**inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: Matching Task [-h] [--document-corpus DOCUMENT_CORPUS]\n",
      "                     [--new-embed NEW_EMBED] [--document-embed DOCUMENT_EMBED]\n",
      "                     [--k K] [--device DEVICE] [--model-name MODEL_NAME]\n",
      "                     [--index_location INDEX_LOCATION]\n",
      "Matching Task: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"6374c16d-bbe1-4643-92c1-507310150e28\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\Opti7080\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-21952m58WaRixzUOk.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "test.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.wav\n",
      "02.wav\n",
      "03.wav\n",
      "Untitled-1.ipynb\n",
      "__pycache__\n",
      "data\n",
      "exe_location.json\n",
      "generate_summary.py\n",
      "inference.py\n",
      "langchain-ChatGLM\n",
      "main.py\n",
      "map.py\n",
      "operation\n",
      "operation_idx.csv\n",
      "play.py\n",
      "recognition.py\n",
      "record.py\n",
      "renew_corpus.py\n",
      "requirements.txt\n",
      "result.wav\n",
      "search_doc.py\n",
      "search_doc_faiss.py\n",
      "search_doc_hf.py\n",
      "stop_criterion.py\n",
      "synthesis.py\n",
      "test_langchain.ipynb\n",
      "test_thred.py\n",
      "text2vec.ipynb\n",
      "zh.wav\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_que = model(**inputs)\n",
    "    embeddings_ans = model(**tokenizer(text_right, padding=True, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 22, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_que.last_hidden_state.shape,embeddings_que.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19582476, -0.22542259,  0.5878763 , ...,  0.51889557,\n",
       "        -0.97035754,  0.72239023],\n",
       "       [-0.36700755,  0.29916915,  0.31173083, ...,  1.2112145 ,\n",
       "         0.05619805,  0.93658125],\n",
       "       [-0.9832982 , -0.7832372 , -1.5102569 , ...,  1.4223759 ,\n",
       "        -1.4264588 ,  0.75840706],\n",
       "       [-1.4391133 ,  0.4106356 , -0.12989327, ..., -0.03974489,\n",
       "        -1.0173508 , -0.79071593]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_que.last_hidden_state[:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "device = 'cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")\n",
    "model = AutoModel.from_pretrained(\"GanymedeNil/text2vec-large-chinese\").to(device)\n",
    "\n",
    "def l2_normalization(data):\n",
    "    if data.ndim == 1:\n",
    "        return data / np.linalg.norm(data).reshape(-1,1)\n",
    "    else:\n",
    "        return data/np.linalg.norm(data,axis=1).reshape(-1,1)\n",
    "def cls_pooling(model_output,return_tensors=False):\n",
    "    if not return_tensors:\n",
    "        return l2_normalization(model_output.last_hidden_state[:,0].cpu().numpy())\n",
    "    else:\n",
    "        return l2_normalization(model_output.last_hidden_state[:,0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open('../data/document_corpus.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        corpus.append(line.strip('\\n'))\n",
    "def mean_pooling(model_output, attention_mask, return_tensors=False):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    if not return_tensors:\n",
    "        return output.cpu().numpy()\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "model.eval()\n",
    "input_data = tokenizer(corpus, padding=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    corpus_embeddings = model(**input_data.to(device))\n",
    "# corpus_embeddings = cls_pooling(corpus_embeddings)\n",
    "corpus_embeddings = mean_pooling(corpus_embeddings,attention_mask=input_data['attention_mask'])\n",
    "corpus_embeddings = l2_normalization(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  '帮我记录一下明天上午十点要买菜'\n",
    "input_data = tokenizer(query, padding=True, return_tensors=\"pt\")  \n",
    "model.eval()     \n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**input_data.to('cuda'))\n",
    "    # query_embeddings = cls_pooling(query_embeddings)\n",
    "    query_embeddings = mean_pooling(query_embeddings,attention_mask=input_data['attention_mask'])\n",
    "    feature_search = l2_normalization(query_embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23497748],\n",
       "       [0.2247234 ],\n",
       "       [0.27397212],\n",
       "       [0.2500224 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings@feature_search.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '进行事件提醒', '打开某个软件']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "features = l2_normalization(np.load(\"./data/document_embed.npy\"))\n",
    "dim = features.shape[1]\n",
    "index_ip = faiss.IndexFlatIP(dim)\n",
    "index_ip.add(features)\n",
    "print(index_ip.ntotal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['调节屏幕亮度', '调整屏幕分辨率', '打开软件', '询问显存', '询问GPU占用', '创建记事本备忘录'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::IndexIDMapTemplate<struct faiss::Index>::IndexIDMapTemplate(struct faiss::Index *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIDMap.cpp:32: Error: 'index->ntotal == 0' failed: index must be empty on input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\3782806113.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maux_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIDMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_ip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\faiss\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplacement_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0moriginal_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreferenced_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparameter_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   9120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9121\u001b[1;33m         \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIDMap_swiginit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_IndexIDMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in __cdecl faiss::IndexIDMapTemplate<struct faiss::Index>::IndexIDMapTemplate(struct faiss::Index *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIDMap.cpp:32: Error: 'index->ntotal == 0' failed: index must be empty on input"
     ]
    }
   ],
   "source": [
    "aux_index = faiss.IndexIDMap(index_ip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Loading Model\n",
      "[INFO]Creating Document Embeddings\n",
      "['调节屏幕亮度', '询问显存占用', '进行事件提醒', '打开软件', '询问GPU占用']\n",
      "[INFO]Finishing Saving Indexing\n"
     ]
    }
   ],
   "source": [
    "from search_doc_faiss import faiss_corpus\n",
    "torch.cuda.empty_cache()\n",
    "corpus = faiss_corpus()\n",
    "corpus.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建备忘录进行事项提醒', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01932951,  0.0241784 ,  0.02330983, ...,  0.02350627,\n",
       "        -0.03408533,  0.0198594 ], dtype=float32),\n",
       " array([-0.04149882,  0.04571409, -0.02394647, ..., -0.03245361,\n",
       "         0.04087428, -0.01606109], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.reconstruct(0),index_ip.reconstruct(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03973337,  0.00838783,  0.01816207, ..., -0.00785893,\n",
       "         -0.05274578,  0.00404025]], dtype=float32),\n",
       " array([-0.03973337,  0.00838783,  0.01816207, ..., -0.00785893,\n",
       "        -0.05274578,  0.00404025], dtype=float32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_normalization_1(features[1,:]),l2_normalization(features[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "query = '提醒我明天十点抢电影票'\n",
    "index = faiss.read_index('./data/test.index')\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to(device))\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search = query_embeddings\n",
    "# 检索最相似的topK个特征\n",
    "topK = 5\n",
    "D, I = index.search(feature_search, topK)\n",
    "# 返回的D表示相似度（或者距离）, I表示检索的topK个特征id（索引）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '帮我记录一下明天十点提醒帮我买菜'\n",
    "corpus_1 = \"\"\"进行事件提醒\"\"\"\n",
    "corpus_2 = \"\"\"调节屏幕亮度\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to(device))\n",
    "    feature_search = model(**tokenizer(corpus_1, padding=True, return_tensors=\"pt\").to(device))\n",
    "    feature_search_2 = model(**tokenizer(corpus_2, padding=True, return_tensors=\"pt\").to(device))\n",
    "feature_search = cls_pooling(feature_search)\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search_2 = cls_pooling(feature_search_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42430413]], dtype=float32), array([[0.3600703]], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings@feature_search.T,query_embeddings@feature_search_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1024), (1, 1024))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape,feature_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.38714254, 0.37287977, 0.3670662 , 0.3359719 , 0.30211502]],\n",
       "       dtype=float32),\n",
       " array([[0, 2, 3, 1, 4]], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建备忘录进行事项提醒', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.38714254, 0.3670662 , 0.36462313, 0.3359719 , 0.30211502]],\n",
       "       dtype=float32),\n",
       " array([[0, 3, 2, 1, 4]], dtype=int64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_ip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 保存索引\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m faiss\u001b[39m.\u001b[39mwrite_index(index_ip, \u001b[39m'\u001b[39m\u001b[39m./data/test.index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# 读取索引\u001b[39;00m\n\u001b[0;32m      5\u001b[0m index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mread_index(\u001b[39m'\u001b[39m\u001b[39m./data/test.index\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_ip' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存索引\n",
    "faiss.write_index(index_ip, './data/test.index')\n",
    "\n",
    "# 读取索引\n",
    "index = faiss.read_index('./data/test.index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_pooling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     query_embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer(query, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m query_embeddings \u001b[39m=\u001b[39m cls_pooling(query_embeddings)\n\u001b[0;32m      6\u001b[0m feature_search \u001b[39m=\u001b[39m query_embeddings\n\u001b[0;32m      7\u001b[0m \u001b[39m# 检索最相似的topK个特征\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cls_pooling' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "query = '提醒我明天十点抢电影票'\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to('cuda'))\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search = query_embeddings\n",
    "# 检索最相似的topK个特征\n",
    "topK = 5\n",
    "D, I = index.search(feature_search, topK)\n",
    "# 返回的D表示相似度（或者距离）, I表示检索的topK个特征id（索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x000001A3B59EF900> >"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.7988719 , 0.56085855, 0.4258615 , 0.41730604, 0.3314243 ]],\n",
       "       dtype=float32),\n",
       " array([[1, 5, 3, 0, 2]], dtype=int64))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建记事本备忘录', '调整屏幕分辨率', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38731813"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    " \n",
    " \n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=1)\n",
    "    sigma = np.std(data, axis=1)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "\n",
    "\n",
    "np.sum(l2_normalization(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(l2_normalization(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19582501, -0.2254237 ,  0.5878755 , ...,  0.51889545,\n",
       "        -0.9703573 ,  0.7223894 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[835.5797 , 564.0149 , 397.68317, 339.4082 , 246.25342]],\n",
       "       dtype=float32),\n",
       " array([[0, 1, 4, 3, 2]], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ids = faiss.IndexFlatIP(2048)\n",
    "index_ids = faiss.IndexIDMap(index_ids)\n",
    "\n",
    "# 添加特征，并指定id，注意添加的id类型为int64\n",
    "ids = 20\n",
    "feature_ids = np.random.random((1, 2048)).astype('float32')\n",
    "index_ids.add_with_ids(feature_ids, np.array((ids,)).astype('int64'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x000001D6C6EFF570> >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9465, 0.6756, 0.2798, 0.3837, 0.4044, 0.2968, 0.2742, 0.3308, 0.3206,\n",
      "         0.4471, 0.4081],\n",
      "        [0.3055, 0.2763, 0.7725, 0.6567, 0.3291, 0.7498, 0.2632, 0.4862, 0.5845,\n",
      "         0.4109, 0.4650],\n",
      "        [0.3777, 0.3504, 0.3729, 0.5051, 0.5157, 0.4793, 0.5053, 0.4642, 0.5326,\n",
      "         0.6118, 0.5007],\n",
      "        [0.3411, 0.3380, 0.3204, 0.4363, 0.4000, 0.4164, 0.3400, 0.4188, 0.5530,\n",
      "         0.3807, 0.5776]])\n"
     ]
    }
   ],
   "source": [
    "from text2vec import cos_sim\n",
    "cos_similarity = cos_sim(embeddings_que.last_hidden_state[:,0],embeddings_ans.last_hidden_state[:,0])\n",
    "print(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2vec import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9719, 0.8737, 0.7577, 0.7623, 0.7647, 0.7870, 0.7640, 0.7554, 0.7812,\n",
       "         0.7794, 0.7791],\n",
       "        [0.7596, 0.8073, 0.9713, 0.9700, 0.8659, 0.9233, 0.8840, 0.8633, 0.9166,\n",
       "         0.8967, 0.9084],\n",
       "        [0.7571, 0.7907, 0.8844, 0.8987, 0.8838, 0.9239, 0.9227, 0.8731, 0.9319,\n",
       "         0.9432, 0.9264],\n",
       "        [0.7539, 0.7718, 0.8788, 0.9024, 0.8693, 0.9033, 0.8655, 0.8638, 0.9149,\n",
       "         0.8890, 0.9299]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import screen_brightness_control as sbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbc.get_brightness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.set_brightness(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 8192.0 MB\n",
      "Free memory: 6604.09375 MB\n",
      "Used memory: 1587.90625 MB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(f\"Total memory: {info.total/1024**2} MB\")\n",
    "print(f\"Free memory: {info.free/1024**2} MB\")\n",
    "print(f\"Used memory: {info.used/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 8192.00 MB\n",
      "Free memory: 6604.09 MB\n",
      "Used memory: 1587.91 MB\n"
     ]
    }
   ],
   "source": [
    "# from operation.screen_brightness import operation\n",
    "# opt = operation(50)\n",
    "# opt.fit()\n",
    "from operation.display_graphic import operation\n",
    "opt = operation()\n",
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operation import operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = 0\n",
    "inp = {\"brightness\":100}\n",
    "opt = eval(f\"operation.Operation{selected_idx}\")(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'已调至100%'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_suffix():\n",
    "    return time.strftime(\"%Y%m%d%H%M%S\", time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230529144958'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "time_suffix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LenoMate\\\\20230529145105\\\\.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('LenoMate',time_suffix(),'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary='love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LenoMate_love_20230529145339.txt'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'LenoMate_'+summary+'_'+time_suffix()+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_args_kwargs(arg1, arg2, arg3,arg4=5):\n",
    "    print(\"arg1:\", arg1)\n",
    "    print(\"arg2:\", arg2)\n",
    "    print(\"arg3:\", arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg1: 5\n",
      "arg2: two\n",
      "arg3: 3\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"arg3\": 3, \"arg2\": \"two\", \"arg1\": 5}\n",
    "test_args_kwargs(**kwargs)\n",
    "\n",
    "#result\n",
    "arg1: 5\n",
    "arg2: 'two'\n",
    "arg3: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "default_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "class operation():\n",
    "    def __init__(self,inputs,path=default_path,summary='notebook'):\n",
    "        self.path = path\n",
    "        self.inputs = inputs\n",
    "        self.summary = summary\n",
    "    def fit(self):\n",
    "        file_name = 'LenoMate_'+self.summary+'_'+time_suffix()+'.txt'\n",
    "        file_name = os.path.join(default_path,file_name)\n",
    "        with open(file_name,'w',encoding='utf-8') as f:\n",
    "            f.write(self.inputs)\n",
    "        f.close()\n",
    "inp = {'inputs':'s','summary':'love'}\n",
    "tst = operation(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10', '10']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string=\"\"\" instruction:\n",
    "- 任务: 调整屏幕亮度\n",
    "- 要求数字: 10\n",
    "output:\n",
    "- 将屏幕亮度调低到10\"\"\"\n",
    "re.findall(r\"\\d+\\.?\\d*\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载中...\n",
      "模型加载完成\n",
      "模型输出： 2\n",
      "模型输出： 4\n",
      "模型输出： 6\n",
      "模型输出： 8\n",
      "模型输出： 10\n",
      "模型输出： 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3 (load_and_run_model):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Opti7080\\AppData\\Local\\Temp\\ipykernel_8504\\1912693514.py\", line 19, in load_and_run_model\n",
      "  File \"<string>\", line 0\n",
      "    \n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "\n",
    "# 模拟加载和运行模型的函数\n",
    "def load_and_run_model(input_queue):\n",
    "    # 模型加载过程，可以根据实际情况进行编写\n",
    "    print(\"模型加载中...\")\n",
    "    # 模型加载完成\n",
    "    print(\"模型加载完成\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 从输入队列中获取输入数据\n",
    "            input_data = input_queue.get()\n",
    "            # 在这里进行模型处理\n",
    "            # 模型处理逻辑...\n",
    "            # 将输入数据转换为numpy数组\n",
    "            input_array = eval(input_data)\n",
    "            # 运行模型，这里我们简单地将输入数据乘以2\n",
    "            output_array = input_array * 2\n",
    "            # 将结果转换为字符串\n",
    "            output_data = str(output_array)\n",
    "\n",
    "            # 处理完成后，可以将结果返回给主线程，或者进行其他操作\n",
    "            print(\"模型输出：\", output_data)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "# 创建一个输入队列\n",
    "input_queue = queue.Queue()\n",
    "\n",
    "# 创建一个线程，用于加载和运行模型\n",
    "model_thread = threading.Thread(target=load_and_run_model, args=(input_queue,))\n",
    "# 设置线程为后台线程，使程序可以退出\n",
    "model_thread.daemon = True\n",
    "# 启动线程\n",
    "model_thread.start()\n",
    "\n",
    "while True:\n",
    "    # 接收用户输入\n",
    "    input_data = input(\"请输入数据（输入'exit'退出）：\")\n",
    "    if input_data == \"exit\":\n",
    "        break\n",
    "    # 将输入数据放入输入队列\n",
    "    input_queue.put(input_data)\n",
    "\n",
    "# 退出程序时，清空输入队列并等待模型线程结束\n",
    "input_queue.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['要求', '1111'], ['数字:'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern =  \"数字(?:\\(.+?\\))?:\\s*\"\n",
    "text = '要求数字(没有则写不含):1111'\n",
    "re.split(pattern,text),re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "answer = \"\"\"\n",
    "数字:未知\n",
    "\"\"\"\n",
    "try:\n",
    "    num = int(re.findall(r\"\\d+\\.?\\d*\",answer.split('数字:')[1])[0])\n",
    "except:\n",
    "    num = answer.split('数字:')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m volume_slider\u001b[39m.\u001b[39mpack(pady\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[39m# 运行GUI主循环\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m window\u001b[39m.\u001b[39;49mmainloop()\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtk\u001b[39m.\u001b[39;49mmainloop(n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from pycaw.pycaw import AudioUtilities, ISimpleAudioVolume\n",
    "\n",
    "# 创建一个Tkinter窗口\n",
    "window = tk.Tk()\n",
    "window.title(\"音量控制\")\n",
    "window.geometry(\"300x100\")\n",
    "\n",
    "# 获取默认的音频会话管理器\n",
    "sessions = AudioUtilities.GetAllSessions()\n",
    "for session in sessions:\n",
    "    volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "    if session.Process and session.Process.name() == \"python.exe\":\n",
    "        volume.SetMasterVolume(0.5, None)  # 初始化音量为50%\n",
    "\n",
    "# 函数：调节音量\n",
    "def set_volume(vol):\n",
    "    for session in sessions:\n",
    "        volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "        if session.Process and session.Process.name() == \"python.exe\":\n",
    "            volume.SetMasterVolume(vol / 100, None)\n",
    "\n",
    "# 创建一个音量控制滑块\n",
    "volume_slider = ttk.Scale(window, from_=0, to=100, orient=\"horizontal\", command=set_volume)\n",
    "volume_slider.set(50)\n",
    "volume_slider.pack(pady=20)\n",
    "\n",
    "# 运行GUI主循环\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "sorted([f for f in os.listdir('../') if re.match('pytorch_model-(\\d+)-of-(\\d+).bin',f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.idea',\n",
       " 'audio',\n",
       " 'data',\n",
       " 'lenochat.py',\n",
       " 'main.py',\n",
       " 'maintext.py',\n",
       " 'operation',\n",
       " 'QA.html',\n",
       " 'README.md',\n",
       " 'requirements.txt',\n",
       " 'stop_criterion.py',\n",
       " 'test_file',\n",
       " 'utils',\n",
       " 'web.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get -10.814563751220703\n",
      "get -56.9900016784668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# 获取音量值，0.0代表最大，-65.25代表最小\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)\n",
    "\n",
    "\n",
    "# 设置静音，mute为1代表是静音，为0代表不是静音\n",
    "volume.SetMute(0, None)\n",
    "volume.SetMasterVolumeLevel(-65.25 + 8.26, None)\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)\n",
    "# 设置音量大小为60%\n",
    "# volume.SetMasterVolumeLevel(-7.63, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get -9.064839363098145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get -8.649999618530273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# 获取音量值，0.0代表最大，-65.25代表最小\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)\n",
    "\n",
    "\n",
    "# 设置静音，mute为1代表是静音，为0代表不是静音\n",
    "volume.SetMute(0, None)\n",
    "volume.SetMasterVolumeLevel(-8.65, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.649999618530273"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume.GetMasterVolumeLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_sim =  AutoModel.from_pretrained(\"GanymedeNil/text2vec-large-chinese\").to('cuda')\n",
    "tokenizer_sim = AutoTokenizer.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")\n",
    "\n",
    "class vol_ctrl:\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"初始化音量对应\"\"\"   \n",
    "        # 音量对应关系\n",
    "        # 0% --- -65.25\n",
    "        # 55% --- -8.92\n",
    "        # 100% --- 0.0\n",
    "        self.volume_match={0: -65.25, 1: -56.99, 2: -51.67, 3: -47.74, 4: -44.62, 5: -42.03, 6: -39.81, 7: -37.89, 8: -36.17, 9: -34.63, 10: -33.24,\n",
    "            11: -31.96, 12: -30.78, 13: -29.68, 14: -28.66, 15: -27.7, 16: -26.8, 17: -25.95, 18: -25.15, 19: -24.38, 20: -23.65,\n",
    "            21: -22.96, 22: -22.3, 23: -21.66, 24: -21.05, 25: -20.46, 26: -19.9, 27: -19.35, 28: -18.82, 29: -18.32, 30: -17.82,\n",
    "            31: -17.35, 32: -16.88, 33: -16.44, 34: -16.0, 35: -15.58, 36: -15.16, 37: -14.76, 38: -14.37, 39: -13.99, 40: -13.62,\n",
    "            41: -13.26, 42: -12.9, 43: -12.56, 44: -12.22, 45: -11.89, 46: -11.56, 47: -11.24, 48: -10.93, 49: -10.63, 50: -10.33,\n",
    "            51: -10.04, 52: -9.75, 53: -9.47, 54: -9.19, 55: -8.92, 56: -8.65, 57: -8.39, 58: -8.13, 59: -7.88, 60: -7.63,\n",
    "            61: -7.38, 62: -7.14, 63: -6.9, 64: -6.67, 65: -6.44, 66: -6.21, 67: -5.99, 68: -5.76, 69: -5.55, 70: -5.33,\n",
    "            71: -5.12, 72: -4.91, 73: -4.71, 74: -4.5, 75: -4.3, 76: -4.11, 77: -3.91, 78: -3.72, 79: -3.53, 80: -3.34,\n",
    "            81: -3.15, 82: -2.97, 83: -2.79, 84: -2.61, 85: -2.43, 86: -2.26, 87: -2.09, 88: -1.91, 89: -1.75, 90: -1.58,\n",
    "            91: -1.41, 92: -1.25, 93: -1.09, 94: -0.93, 95: -0.77, 96: -0.61, 97: -0.46, 98: -0.3, 99: -0.15, 100: 0.0}\n",
    "        #音量值反对应\n",
    "        self.volume_match_2={-65.25:0, -56.99:1, -51.67:2, -47.74:3, -44.62:4, -42.03:5, -39.81:6,-37.89:7, -36.17:8, -34.63:9, -33.24:10,\n",
    "            -31.96:11, -30.78:12, -29.68:13, -28.66:14, -27.7:15, -26.8:16, -25.95:17, -25.15:18,  -24.38:19, -23.65:20,\n",
    "            -22.96:21, -22.3:22, -21.66:23, -21.05:24, -20.46:25, -19.9:26, -19.35:27,  -18.82:28,  -18.32:29,  -17.82:30,\n",
    "            -17.35:31, -16.88:32, -16.44:33, -16.0:34, -15.58:35,-15.16:36, -14.76:37, -14.37:38,  -13.99:39, -13.62:40,\n",
    "            -13.26:41, -12.9:42, -12.56:43, -12.22:44,-11.89:45, -11.56:46, -11.24:47, -10.93:48,  -10.63:49, -10.33:50,\n",
    "            -10.04:51, -9.75:52, -9.47:53, -9.19:54, -8.92:55, -8.65:56, -8.39:57, -8.13:58,  -7.88:59,  -7.63:60,\n",
    "            -7.38:61, -7.14:62, -6.9:63, -6.67:64, -6.44:65, -6.21:66, -5.99:67,  -5.76:68,  -5.55:69, -5.33:70,\n",
    "            -5.12:71,-4.91:72, -4.71:73, -4.5:74, -4.3:75, -4.11:76,  -3.91:77, -3.72:78,  -3.53:79,  -3.34:80,\n",
    "            -3.15:81, -2.97:82, -2.79:83, -2.61:84, -2.43:85, -2.26:86, -2.09:87,  -1.91:88, -1.75:89, -1.58:90,\n",
    "           -1.41:91, -1.25:92, -1.09:93, -0.93:94, -0.77:95, -0.61:96, -0.46:97, -0.3:98,  -0.15:99,  0.0:100}\n",
    "        self.devices = AudioUtilities.GetSpeakers()\n",
    "        self.interface = self.devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "        self.volume = cast(self.interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "    def mute_all(self,mute=True):\n",
    "        self.mute = mute\n",
    "        if self.mute: self.volume.SetMute(1, None)\n",
    "        else: self.volume.SetMute(0, None)\n",
    "\n",
    "    def alter(self,num):\n",
    "        if num > 0:\n",
    "            self.mute_all(mute = False)\n",
    "        vl = self.volume.GetMasterVolumeLevel()\n",
    "        print('get current volumn',vl)\n",
    "        self.volume.SetMasterVolumeLevel(self.volume_match[int(num)], None)\n",
    "        print('alter to ',num)\n",
    "\n",
    "class Operation5():\n",
    "    def __init__(self,inputs,model_sim,tokenizer_sim) -> None:\n",
    "        \n",
    "        self.vol_ctrl = vol_ctrl()\n",
    "        self.inputs = inputs\n",
    "        self.model_sim = model_sim\n",
    "        self.tokenizer_sim = tokenizer_sim\n",
    "\n",
    "    def fit(self,model,tokenizer):\n",
    "        self.judge()\n",
    "        if not self.selected: self.vol_ctrl.mute_all()\n",
    "        pass\n",
    "\n",
    "    def judge(self):\n",
    "        _mute = '静音，取消静音'\n",
    "        _normal = '调节音量'\n",
    "        corpus = [_mute,_normal]\n",
    "        self.model_sim.eval()\n",
    "        input_data = self.tokenizer_sim(self.inputs, return_tensors=\"pt\")\n",
    "        corp = self.tokenizer_sim(corpus, return_tensors=\"pt\",padding=True)\n",
    "        with torch.no_grad():\n",
    "            corpus_embeddings = self.model_sim(**corp.to('cuda'))\n",
    "            input_embeddings = self.model_sim(**input_data.to('cuda'))\n",
    "        corpus_embeddings = self._pooling(corpus_embeddings,attention_mask=corp['attention_mask'])\n",
    "        input_embeddings = self._pooling(input_embeddings,attention_mask=input_data['attention_mask'])\n",
    "        self.selected = corpus[(corpus_embeddings@input_embeddings.T).argmax(axis=0)[0]]\n",
    "        print(f\"[音量调节功能]音量功能匹配为'{self.selected}'\")\n",
    "\n",
    "    def _pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return self._l2_normalization(output.cpu().numpy())\n",
    "    \n",
    "    def _l2_normalization(self,data):\n",
    "        if data.ndim == 1:\n",
    "            return data / np.linalg.norm(data).reshape(-1,1)\n",
    "        else:\n",
    "            return data/ np.linalg.norm(data,axis=1).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI bot: 联想集团（Lenovo Group Limited）是一家总部位于中国北京的跨国科技公司，成立于1984年。它在全球范围内提供消费类电子产品，如个人电脑、笔记本电脑、平板电脑、智能手机、服务器、工作站、网络存储设备和智能电视等。联想是全球最大的个人电脑制造商之一，同时也是全球最大的智能手机供应商之一。\n",
      "\n",
      "联想的创始人兼董事局主席是柳传志，他是中国著名的企业家之一。在过去的几十年里，联想通过不断创新和扩大市场份额，逐渐成为全球科技产业的领导者。2005年，联想收购了IBM的个人电脑业务，使其在全球个人电脑市场中占据了重要地位。此次收购使联想的品牌知名度大大提高，同时也为公司在全球市场的发展奠定了基础。\n",
      "\n",
      "近年来，联想还不断扩大其业务领域，涉足包括人工智能、大数据、云计算和物联网等新兴科技领域。联想致力于为客户提供高质量、高性能的产品和解决方案，满足不同客户的需求。\n",
      "\n",
      "总的来说，联想集团是一家具有全球影响力的科技公司，为全球数以亿计的用户提供了优质的科技产品和服务。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://openaipoc-kepler.openai.azure.com/\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = '14639a3cad0f4ba38483194f82c6920b'\n",
    "\n",
    "messageshistory = [\n",
    "                    {\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                    {\"role\":\"user\",\"content\":\"请给我介绍一下联想企业\"},\n",
    "                    ]\n",
    "response = openai.ChatCompletion.create(\n",
    "      engine=\"Kepler1\",\n",
    "      messages = messageshistory,\n",
    "      temperature=0.5,\n",
    "      max_tokens=300,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "print('AI bot:',response['choices'][0]['message']['content'])\n",
    "bot = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpu': {'used': 10.5, 'used_list': [3.7, 8.9, 14.6, 6.5, 8.5, 1.8, 5.2, 2.2, 2.2, 4.9, 2.7, 1.2, 2.3, 1.3, 5.0, 2.9, 2.4, 6.1, 6.0, 2.4], 'cpu_count': 1, 'cpu_name': 'Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', 'cpu_core': 10, 'cpu_threads': 20}, 'load': {'one': 0, 'five': 0, 'fifteen': 0, 'max': 40, 'limit': 40, 'safe': 30.0}, 'mem': {'memTotal': 32480, 'memFree': 21799, 'memRealUsed': 10681, 'menUsedPercent': 32.88534911199906}, 'disk': [{'path': 'C:/', 'size': {'total': 508453597184, 'used': 251668832256, 'free': 256784764928, 'percent': 49.5}, 'fstype': 'NTFS', 'inodes': False}], 'network': {'up': 0, 'down': 0, 'upTotal': 83814717, 'downTotal': 390884500, 'downPackets': 466817, 'upPackets': 446254}, 'io': {'write': 0, 'read': 0}, 'boot': {'timestamp': 1687137044.5692391, 'runtime': 90476.80121970177, 'datetime': '2023-06-20 10:18:41'}, 'time': 1687227521.3704588}\n",
      "{'cpu_count': 1, 'cpu_name': 'Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', 'cpu_core': 10, 'cpu_threads': 20}\n",
      "{'cpu': {'used': 10.8, 'used_list': [3.1, 7.1, 67.3, 5.1, 19.4, 4.1, 24.5, 14.3, 9.2, 3.1, 14.3, 0.0, 6.1, 2.0, 6.1, 10.2, 2.0, 10.2, 6.1, 1.0], 'cpu_count': 1, 'cpu_name': 'Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', 'cpu_core': 10, 'cpu_threads': 20}, 'load': {'one': 0, 'five': 0, 'fifteen': 0, 'max': 40, 'limit': 40, 'safe': 30.0}, 'mem': {'memTotal': 32480, 'memFree': 21753, 'memRealUsed': 10727, 'menUsedPercent': 33.027008337317085}, 'disk': [{'path': 'C:/', 'size': {'total': 508453597184, 'used': 251668832256, 'free': 256784764928, 'percent': 49.5}, 'fstype': 'NTFS', 'inodes': False}]}\n",
      "{'up': 5.92, 'down': 1.87, 'upTotal': 83823941, 'downTotal': 390887417, 'downPackets': 466836, 'upPackets': 446278}\n",
      "{'write': 217088, 'read': 4096}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "@name: 系统信息 / SystemInfo\n",
    "@author: PurePeace\n",
    "@time: 2020年8月17日\n",
    "@version: 0.1\n",
    "'''\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import platform\n",
    "import hashlib\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "from cachelib import SimpleCache\n",
    "cache = SimpleCache()\n",
    "\n",
    "\n",
    "UNIX: bool = os.name == 'posix'\n",
    "SYS: str = platform.system()\n",
    "\n",
    "\n",
    "class CpuConstants:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        初始化CPU常量（多平台）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.\n",
    "\n",
    "        '''\n",
    "        self.WMI = None\n",
    "        self.initialed: bool = False\n",
    "        self.cpuList: list = [] # windows only\n",
    "\n",
    "        self.cpuCount: int = 0 # 物理cpu数量\n",
    "        self.cpuCore: int = 0 # cpu物理核心数\n",
    "        self.cpuThreads: int = 0 # cpu逻辑核心数\n",
    "        self.cpuName: str = '' # cpu型号\n",
    "\n",
    "        self.Update(True)\n",
    "\n",
    "\n",
    "    def Update(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        更新cpu数据\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        '''\n",
    "        if UNIX: self.GetCpuConstantsUnix(update)\n",
    "        else: self.GetCpuConstantsWindows(update)\n",
    "\n",
    "        self.initialed: bool = True\n",
    "\n",
    "\n",
    "    @property\n",
    "    def getDict(self) -> Dict[int, str]:\n",
    "        '''\n",
    "        以字典格式获取当前cpu常量\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[int, str]\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if not self.initialed: self.Update()\n",
    "        return {\n",
    "            'cpu_count': self.cpuCount,\n",
    "            'cpu_name': self.cpuName,\n",
    "            'cpu_core': self.cpuCore,\n",
    "            'cpu_threads': self.cpuThreads\n",
    "        }\n",
    "\n",
    "\n",
    "    def GetCpuConstantsUnix(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        获取unix下的cpu信息\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        update : bool, optional\n",
    "            DESCRIPTION. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if update or not self.initialed:\n",
    "            ids: list = re.findall(\"physical id.+\", readFile('/proc/cpuinfo'))\n",
    "\n",
    "            # 物理cpu个数\n",
    "            self.cpuCount: int = len(set(ids))\n",
    "\n",
    "            # cpu型号（名称）\n",
    "            self.cpuName: str = self.getCpuTypeUnix()\n",
    "\n",
    "\n",
    "            self.GetCpuConstantsBoth()\n",
    "\n",
    "\n",
    "    def InitWmi(self) -> None:\n",
    "        '''\n",
    "        初始化wmi（for windows）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        import wmi\n",
    "        self.WMI = wmi.WMI()\n",
    "\n",
    "\n",
    "    def GetCpuConstantsBoth(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        获取多平台共用的cpu信息\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        update : bool, optional\n",
    "            强制更新数据. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if update or not self.initialed:\n",
    "\n",
    "            # cpu逻辑核心数\n",
    "            self.cpuThreads: int = psutil.cpu_count()\n",
    "\n",
    "            # cpu物理核心数\n",
    "            self.cpuCore: int = psutil.cpu_count(logical=False)\n",
    "\n",
    "\n",
    "    def GetCpuConstantsWindows(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        获取windows平台的cpu信息\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        update : bool, optional\n",
    "            强制更新数据. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if update or not self.initialed:\n",
    "\n",
    "            # 初始化wmi\n",
    "            if self.WMI == None: self.InitWmi()\n",
    "\n",
    "            # cpu列表\n",
    "            self.cpuList: list = self.WMI.Win32_Processor()\n",
    "\n",
    "            # 物理cpu个数\n",
    "            self.cpuCount: int = len(self.cpuList)\n",
    "\n",
    "            # cpu型号（名称）\n",
    "            self.cpuName: str = self.cpuList[0].Name\n",
    "\n",
    "\n",
    "            self.GetCpuConstantsBoth()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def getCpuTypeUnix() -> str:\n",
    "        '''\n",
    "        获取CPU型号（unix）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            CPU型号.\n",
    "\n",
    "        '''\n",
    "        cpuinfo: str = readFile('/proc/cpuinfo')\n",
    "        rep: str = 'model\\s+name\\s+:\\s+(.+)'\n",
    "        tmp = re.search(rep,cpuinfo,re.I)\n",
    "        cpuType: str = ''\n",
    "        if tmp:\n",
    "            cpuType: str = tmp.groups()[0]\n",
    "        else:\n",
    "            cpuinfo = ExecShellUnix('LANG=\"en_US.UTF-8\" && lscpu')[0]\n",
    "            rep = 'Model\\s+name:\\s+(.+)'\n",
    "            tmp = re.search(rep,cpuinfo,re.I)\n",
    "            if tmp: cpuType = tmp.groups()[0]\n",
    "        return cpuType\n",
    "\n",
    "\n",
    "def GetCpuInfo(interval: int = 1) -> Dict[str, Any]:\n",
    "    '''\n",
    "    获取CPU信息\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    interval : int, optional\n",
    "        DESCRIPTION. The default is 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[float, list, dict]\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "    # cpu总使用率\n",
    "    used: float = psutil.cpu_percent(interval)\n",
    "\n",
    "    # 每个逻辑cpu使用率\n",
    "    usedList: List[float] = psutil.cpu_percent(percpu=True)\n",
    "\n",
    "\n",
    "    return {'used': used, 'used_list': usedList, **cpuConstants.getDict}\n",
    "\n",
    "\n",
    "def readFile(filename: str) -> str:\n",
    "    '''\n",
    "    读取文件内容\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        文件名.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        文件内容.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file: return file.read()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "def GetLoadAverage() -> dict:\n",
    "    '''\n",
    "    获取服务器负载状态（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    try: c: list = os.getloadavg()\n",
    "    except: c: list = [0,0,0]\n",
    "    data: dict = {i: c[idx] for idx, i in enumerate(('one', 'five', 'fifteen'))}\n",
    "    data['max'] = psutil.cpu_count() * 2\n",
    "    data['limit'] = data['max']\n",
    "    data['safe'] = data['max'] * 0.75\n",
    "    return data\n",
    "\n",
    "\n",
    "def GetMemInfo() -> dict:\n",
    "    '''\n",
    "    获取内存信息（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    if UNIX: return GetMemInfoUnix()\n",
    "    return GetMemInfoWindows()\n",
    "\n",
    "\n",
    "def GetMemInfoUnix() -> Dict[str, int]:\n",
    "    '''\n",
    "    获取内存信息（unix）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    mem = psutil.virtual_memory()\n",
    "    memInfo: dict = {\n",
    "        'memTotal': ToSizeInt(mem.total, 'MB'),\n",
    "        'memFree': ToSizeInt(mem.free, 'MB'),\n",
    "        'memBuffers': ToSizeInt(mem.buffers, 'MB'),\n",
    "        'memCached': ToSizeInt(mem.cached, 'MB'),\n",
    "    }\n",
    "    memInfo['memRealUsed'] = \\\n",
    "        memInfo['memTotal'] - \\\n",
    "        memInfo['memFree'] - \\\n",
    "        memInfo['memBuffers'] - \\\n",
    "        memInfo['memCached']\n",
    "\n",
    "    memInfo['memUsedPercent'] = memInfo['memRealUsed'] / memInfo['memTotal'] * 100\n",
    "\n",
    "    return memInfo\n",
    "\n",
    "\n",
    "def GetMemInfoWindows() -> dict:\n",
    "    '''\n",
    "    获取内存信息（windows）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    mem = psutil.virtual_memory()\n",
    "    memInfo: dict = {\n",
    "        'memTotal': ToSizeInt(mem.total, 'MB'),\n",
    "        'memFree': ToSizeInt(mem.free, 'MB'),\n",
    "        'memRealUsed': ToSizeInt(mem.used, 'MB'),\n",
    "        'menUsedPercent': mem.used / mem.total * 100\n",
    "    }\n",
    "\n",
    "    return memInfo\n",
    "\n",
    "\n",
    "def ToSizeInt(byte: int, target: str) -> int:\n",
    "    '''\n",
    "    将字节大小转换为目标单位的大小\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    byte : int\n",
    "        int格式的字节大小（bytes size）\n",
    "    target : str\n",
    "        目标单位，str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        转换为目标单位后的字节大小.\n",
    "\n",
    "    '''\n",
    "    return int(byte/1024**(('KB','MB','GB','TB').index(target) + 1))\n",
    "\n",
    "\n",
    "def ToSizeString(byte: int) -> str:\n",
    "    '''\n",
    "    获取字节大小字符串\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    byte : int\n",
    "        int格式的字节大小（bytes size）.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        自动转换后的大小字符串，如：6.90 GB.\n",
    "\n",
    "    '''\n",
    "    units: tuple = ('b','KB','MB','GB','TB')\n",
    "    re = lambda: '{:.2f} {}'.format(byte, u)\n",
    "    for u in units:\n",
    "        if byte < 1024: return re()\n",
    "        byte /= 1024\n",
    "    return re()\n",
    "\n",
    "\n",
    "def GetDiskInfo() -> list:\n",
    "    '''\n",
    "    获取磁盘信息（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        列表.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        if UNIX: return GetDiskInfoUnix()\n",
    "        return GetDiskInfoWindows()\n",
    "    except Exception as err:\n",
    "        print('获取磁盘信息异常（unix: {}）：'.format(UNIX), err)\n",
    "        return []\n",
    "\n",
    "\n",
    "def GetDiskInfoWindows() -> list:\n",
    "    '''\n",
    "    获取磁盘信息Windows\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    diskInfo : list\n",
    "        列表.\n",
    "\n",
    "    '''\n",
    "    diskIo: list = psutil.disk_partitions()\n",
    "    diskInfo: list = []\n",
    "    for disk in diskIo:\n",
    "        tmp: dict = {}\n",
    "        try:\n",
    "            tmp['path'] = disk.mountpoint.replace(\"\\\\\",\"/\")\n",
    "            usage = psutil.disk_usage(disk.mountpoint)\n",
    "            tmp['size'] = {\n",
    "                'total': usage.total,\n",
    "                'used': usage.used,\n",
    "                'free': usage.free,\n",
    "                'percent': usage.percent\n",
    "            }\n",
    "            tmp['fstype'] = disk.fstype\n",
    "            tmp['inodes'] = False\n",
    "            diskInfo.append(tmp)\n",
    "        except:\n",
    "            pass\n",
    "    return diskInfo\n",
    "\n",
    "\n",
    "def GetDiskInfoUnix() -> list:\n",
    "     '''\n",
    "    获取硬盘分区信息（unix）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "     temp: list = (\n",
    "         ExecShellUnix(\"df -h -P|grep '/'|grep -v tmpfs\")[0]).split('\\n')\n",
    "     tempInodes: list = (\n",
    "         ExecShellUnix(\"df -i -P|grep '/'|grep -v tmpfs\")[0]).split('\\n')\n",
    "     diskInfo: list = []\n",
    "     n: int = 0\n",
    "     cuts: list = [\n",
    "         '/mnt/cdrom',\n",
    "         '/boot',\n",
    "         '/boot/efi',\n",
    "         '/dev',\n",
    "         '/dev/shm',\n",
    "         '/run/lock',\n",
    "         '/run',\n",
    "         '/run/shm',\n",
    "         '/run/user'\n",
    "     ]\n",
    "     for tmp in temp:\n",
    "         n += 1\n",
    "         try:\n",
    "             inodes: list = tempInodes[n-1].split()\n",
    "             disk: list = tmp.split()\n",
    "             if len(disk) < 5: continue\n",
    "             if disk[1].find('M') != -1: continue\n",
    "             if disk[1].find('K') != -1: continue\n",
    "             if len(disk[5].split('/')) > 10: continue\n",
    "             if disk[5] in cuts: continue\n",
    "             if disk[5].find('docker') != -1: continue\n",
    "             arr = {}\n",
    "             arr['path'] = disk[5]\n",
    "             tmp1 = [disk[1],disk[2],disk[3],disk[4]]\n",
    "             arr['size'] = tmp1\n",
    "             arr['inodes'] = [inodes[1],inodes[2],inodes[3],inodes[4]]\n",
    "             diskInfo.append(arr)\n",
    "         except Exception as ex:\n",
    "             print('信息获取错误：', str(ex))\n",
    "             continue\n",
    "     return diskInfo\n",
    "\n",
    "\n",
    "\n",
    "def md5(strings: str) -> str:\n",
    "    '''\n",
    "    生成md5\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strings : TYPE\n",
    "        要进行hash处理的字符串\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str[32]\n",
    "        hash后的字符串.\n",
    "\n",
    "    '''\n",
    "\n",
    "    m = hashlib.md5()\n",
    "    m.update(strings.encode('utf-8'))\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def GetErrorInfo() -> str:\n",
    "    '''\n",
    "    获取traceback中的错误\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    import traceback\n",
    "    errorMsg = traceback.format_exc()\n",
    "    return errorMsg\n",
    "\n",
    "\n",
    "def ExecShellUnix(cmdstring: str, shell=True):\n",
    "    '''\n",
    "    执行Shell命令（Unix）\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmdstring : str\n",
    "        DESCRIPTION.\n",
    "    shell : TYPE, optional\n",
    "        DESCRIPTION. The default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : TYPE\n",
    "        DESCRIPTION.\n",
    "    e : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    a: str = ''\n",
    "    e: str = ''\n",
    "    import subprocess,tempfile\n",
    "\n",
    "    try:\n",
    "        rx: str = md5(cmdstring)\n",
    "        succ_f = tempfile.SpooledTemporaryFile(\n",
    "            max_size = 4096,\n",
    "            mode = 'wb+',\n",
    "            suffix = '_succ',\n",
    "            prefix = 'btex_' + rx ,\n",
    "            dir = '/dev/shm'\n",
    "        )\n",
    "        err_f = tempfile.SpooledTemporaryFile(\n",
    "            max_size = 4096,\n",
    "            mode = 'wb+',\n",
    "            suffix = '_err',\n",
    "            prefix = 'btex_' + rx ,\n",
    "            dir = '/dev/shm'\n",
    "        )\n",
    "        sub = subprocess.Popen(\n",
    "            cmdstring,\n",
    "            close_fds = True,\n",
    "            shell = shell,\n",
    "            bufsize = 128,\n",
    "            stdout = succ_f,\n",
    "            stderr = err_f\n",
    "        )\n",
    "        sub.wait()\n",
    "        err_f.seek(0)\n",
    "        succ_f.seek(0)\n",
    "        a = succ_f.read()\n",
    "        e = err_f.read()\n",
    "        if not err_f.closed: err_f.close()\n",
    "        if not succ_f.closed: succ_f.close()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    try:\n",
    "        if type(a) == bytes: a = a.decode('utf-8')\n",
    "        if type(e) == bytes: e = e.decode('utf-8')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "    return a,e\n",
    "\n",
    "\n",
    "def GetNetWork() -> dict:\n",
    "    '''\n",
    "    获取系统网络信息\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    networkIo: list = [0,0,0,0]\n",
    "    cache_timeout: int = 86400\n",
    "    try:\n",
    "        networkIo = psutil.net_io_counters()[:4]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    otime = cache.get(\"otime\")\n",
    "    if not otime:\n",
    "        otime = time.time()\n",
    "        cache.set('up',networkIo[0],cache_timeout)\n",
    "        cache.set('down',networkIo[1],cache_timeout)\n",
    "        cache.set('otime',otime ,cache_timeout)\n",
    "\n",
    "    ntime = time.time()\n",
    "    networkInfo: dict = {'up': 0, 'down': 0}\n",
    "    networkInfo['upTotal']   = networkIo[0]\n",
    "    networkInfo['downTotal'] = networkIo[1]\n",
    "    try:\n",
    "        networkInfo['up'] = round(\n",
    "            float(networkIo[0] - cache.get(\"up\")) / 1024 / (ntime - otime),\n",
    "            2\n",
    "        )\n",
    "        networkInfo['down'] = round(\n",
    "            float(networkIo[1] - cache.get(\"down\")) / 1024 / (ntime -  otime),\n",
    "            2\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    networkInfo['downPackets'] = networkIo[3]\n",
    "    networkInfo['upPackets'] = networkIo[2]\n",
    "\n",
    "    cache.set('up',networkIo[0],cache_timeout)\n",
    "    cache.set('down',networkIo[1],cache_timeout)\n",
    "    cache.set('otime', time.time(),cache_timeout)\n",
    "\n",
    "    return networkInfo\n",
    "\n",
    "\n",
    "def GetSystemInfo() -> dict:\n",
    "    systemInfo: dict = {}\n",
    "    systemInfo['cpu'] = GetCpuInfo()\n",
    "    systemInfo['load'] = GetLoadAverage()\n",
    "    systemInfo['mem'] = GetMemInfo()\n",
    "    systemInfo['disk'] = GetDiskInfo()\n",
    "\n",
    "    return systemInfo\n",
    "\n",
    "\n",
    "\n",
    "def GetIoReadWrite() -> Dict[str, int]:\n",
    "    '''\n",
    "    获取系统IO读写\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    ioDisk = psutil.disk_io_counters()\n",
    "    ioTotal: dict = {}\n",
    "    ioTotal['write'] = GetIoWrite(ioDisk.write_bytes)\n",
    "    ioTotal['read'] = GetIoRead(ioDisk.read_bytes)\n",
    "    return ioTotal\n",
    "\n",
    "\n",
    "def GetIoWrite(ioWrite: int) -> int:\n",
    "    '''\n",
    "    获取IO写\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ioWrite : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    diskWrite: int = 0\n",
    "    oldWrite: int = cache.get('io_write')\n",
    "    if not oldWrite:\n",
    "        cache.set('io_write', ioWrite)\n",
    "        return diskWrite;\n",
    "\n",
    "    oldTime: float = cache.get('io_time')\n",
    "    newTime: float = time.time()\n",
    "    if not oldTime: oldTime = newTime\n",
    "    ioEnd: int = (ioWrite - oldWrite)\n",
    "    timeEnd: float = (time.time() - oldTime)\n",
    "    if ioEnd > 0:\n",
    "        if timeEnd < 1: timeEnd = 1\n",
    "        diskWrite = ioEnd / timeEnd\n",
    "    cache.set('io_write',ioWrite)\n",
    "    cache.set('io_time',newTime)\n",
    "    if diskWrite > 0: return int(diskWrite)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def GetIoRead(ioRead):\n",
    "    '''\n",
    "    读取IO读\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ioRead : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    diskRead: int = 0\n",
    "    oldRead: int = cache.get('io_read')\n",
    "    if not oldRead:\n",
    "        cache.set('io_read',ioRead)\n",
    "        return diskRead;\n",
    "    oldTime: float = cache.get('io_time')\n",
    "    newTime: float = time.time()\n",
    "    if not oldTime: oldTime = newTime\n",
    "    ioEnd: int = (ioRead - oldRead)\n",
    "    timeEnd: float = (time.time() - oldTime)\n",
    "    if ioEnd > 0:\n",
    "        if timeEnd < 1: timeEnd = 1;\n",
    "        diskRead = ioEnd / timeEnd;\n",
    "    cache.set('io_read', ioRead)\n",
    "    if diskRead > 0: return int(diskRead)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def GetRegValue(key: str, subkey: str, value: str) -> Any:\n",
    "    '''\n",
    "    获取系统注册表信息\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str\n",
    "        类型.\n",
    "    subkey : str\n",
    "        路径.\n",
    "    value : str\n",
    "        key.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value : Any\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    import winreg\n",
    "    key = getattr(winreg, key)\n",
    "    handle = winreg.OpenKey(key, subkey)\n",
    "    (value, type) = winreg.QueryValueEx(handle, value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def GetSystemVersion() -> str:\n",
    "    '''\n",
    "    获取操作系统版本（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        DESCRIPTIONm.\n",
    "\n",
    "    '''\n",
    "    if UNIX: return GetSystemVersionUnix()\n",
    "    return GetSystemVersionWindows()\n",
    "\n",
    "\n",
    "def GetSystemVersionWindows() -> str:\n",
    "    '''\n",
    "    获取操作系统版本（windows）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        import platform\n",
    "        bit: str = 'x86';\n",
    "        if 'PROGRAMFILES(X86)' in os.environ: bit = 'x64'\n",
    "\n",
    "        def get(key: str):\n",
    "            return GetRegValue(\n",
    "                \"HKEY_LOCAL_MACHINE\",\n",
    "                \"SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\",\n",
    "                key\n",
    "            )\n",
    "\n",
    "        osName = get('ProductName')\n",
    "        build = get('CurrentBuildNumber')\n",
    "\n",
    "        version: str = '{} (build {}) {} (Py{})'.format(\n",
    "            osName, build, bit, platform.python_version())\n",
    "        return version\n",
    "    except Exception as ex:\n",
    "        print('获取系统版本失败，错误：' + str(ex))\n",
    "        return '未知系统版本.'\n",
    "\n",
    "\n",
    "def GetSystemVersionUnix() -> str:\n",
    "    '''\n",
    "    获取系统版本（unix）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        系统版本.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        version: str = readFile('/etc/redhat-release')\n",
    "        if not version:\n",
    "            version = readFile(\n",
    "                '/etc/issue'\n",
    "            ).strip().split(\"\\n\")[0].replace('\\\\n','').replace('\\l','').strip()\n",
    "        else:\n",
    "            version = version.replace(\n",
    "                'release ',''\n",
    "            ).replace('Linux','').replace('(Core)','').strip()\n",
    "        v = sys.version_info\n",
    "        return version + '(Py {}.{}.{})'.format(v.major, v.minor, v.micro)\n",
    "    except Exception as err:\n",
    "        print('获取系统版本失败，错误：', err)\n",
    "        return '未知系统版本.'\n",
    "\n",
    "\n",
    "def GetBootTime() -> dict:\n",
    "    '''\n",
    "    获取当前系统启动时间\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    bootTime: float = psutil.boot_time()\n",
    "    return {\n",
    "        'timestamp': bootTime,\n",
    "        'runtime': time.time() - bootTime,\n",
    "        'datetime': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    }\n",
    "\n",
    "\n",
    "def GetCpuConstants() -> dict:\n",
    "    '''\n",
    "    获取CPU常量信息\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cpuConstants : CpuConstants\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    return cpuConstants.getDict\n",
    "\n",
    "\n",
    "def GetFullSystemData() -> dict:\n",
    "    '''\n",
    "    获取完全的系统信息\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    systemData: dict = {\n",
    "        **GetSystemInfo(),\n",
    "        'network': { **GetNetWork() },\n",
    "        'io': { **GetIoReadWrite() },\n",
    "        'boot': { **GetBootTime() },\n",
    "        'time': time.time()\n",
    "    }\n",
    "    return systemData\n",
    "\n",
    "cpuConstants = CpuConstants()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(GetFullSystemData())\n",
    "    print(GetCpuConstants())\n",
    "    print(GetSystemInfo())\n",
    "    print(GetNetWork())\n",
    "    print(GetIoReadWrite())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mac in c.Win32_NetworkAdapter():\n",
    "    print(\"mac地址\", mac.MACAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "**************************************************\n",
      "['电脑名称: DESKTOP-JHRQTGU', '使 用 者: DESKTOP-JHRQTGU\\\\Opti7080', 'MAC地址: cc:d9:ac:d8:57:06', '使用日期: 1.19.0', '主板型号: 41JYJ53', 'CPU型号: Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', '内存厂商: 80AD000080AD', '内存型号: HMA82GU6DJR8N-XN', '内存大小: 16.00GB', '内存厂商: 80AD000080AD', '内存型号: HMA82GU6DJR8N-XN', '内存大小: 16.00GB', '显卡名称: NVIDIA GeForce RTX 2070 SUPER', '显卡名称: Intel(R) UHD Graphics 630', 'IP地址: 10.176.130.235']\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import wmi\n",
    "import socket\n",
    "\n",
    "class information:\n",
    "    w = wmi.WMI() # 获取配置信息\n",
    "    list = []\n",
    "\n",
    "class INFO(information):\n",
    "    def __init__(self):\n",
    "        self.info()\n",
    "\n",
    "    # 获取配置信息\n",
    "    def info(self):\n",
    "    \n",
    "        for BIOSs in information.w.Win32_ComputerSystem():\n",
    "            information.list.append(\"电脑名称: %s\" % BIOSs.Caption)\n",
    "            information.list.append(\"使 用 者: %s\" % BIOSs.UserName)\n",
    "            \n",
    "        \n",
    "        address = hex(uuid.getnode())[2:]\n",
    "        address =  \":\".join([address[e:e+2] for e in range(0,11,2)])\n",
    "        information.list.append(\"MAC地址: %s\" % address)\n",
    "        \n",
    "        for BIOS in information.w.Win32_BIOS():\n",
    "            # information.list.append(\"使用日期: %s\" % BIOS.Description)\n",
    "            information.list.append(\"主板型号: %s\" % BIOS.SerialNumber)\n",
    "            \n",
    "        for processor in information.w.Win32_Processor():\n",
    "            information.list.append(\"CPU型号: %s\" % processor.Name.strip())\n",
    "            \n",
    "        for memModule in information.w.Win32_PhysicalMemory():\n",
    "            print('*'*50)\n",
    "            totalMemSize = int(memModule.Capacity)\n",
    "            information.list.append(\"内存厂商: %s\" % memModule.Manufacturer)\n",
    "            information.list.append(\"内存型号: %s\" % memModule.PartNumber.strip())\n",
    "            information.list.append(\"内存大小: %.2fGB\" % (totalMemSize / 1024**3))\n",
    "            \n",
    "        for disk in information.w.Win32_DiskDrive(InterfaceType=\"IDE\"):\n",
    "            \n",
    "            diskSize = int(disk.size)\n",
    "            information.list.append(\"磁盘名称: %s\" % disk.Caption)\n",
    "            information.list.append(\"磁盘大小: %.2fGB\" % (diskSize / 1024**3))\n",
    "            \n",
    "        for xk in information.w.Win32_VideoController():\n",
    "            information.list.append(\"显卡名称: %s\" % xk.name)\n",
    "\n",
    "\n",
    "        addr = socket.gethostbyname(socket.gethostname())\n",
    "        information.list.append(\"IP地址: %s\" % addr)\n",
    "\n",
    "        return '\\n'.join(information.list)\n",
    "\n",
    "infor = information()\n",
    "INFOs = INFO()\n",
    "print(information.list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑名称: DESKTOP-JHRQTGU\n",
      "使 用 者: DESKTOP-JHRQTGU\\Opti7080\n",
      "MAC地址: cc:d9:ac:d8:57:06\n",
      "使用日期: 1.19.0\n",
      "主板型号: 41JYJ53\n",
      "CPU型号: Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz\n",
      "内存厂商: 80AD000080AD\n",
      "内存型号: HMA82GU6DJR8N-XN\n",
      "内存大小: 16.00GB\n",
      "内存厂商: 80AD000080AD\n",
      "内存型号: HMA82GU6DJR8N-XN\n",
      "内存大小: 16.00GB\n",
      "显卡名称: NVIDIA GeForce RTX 2070 SUPER\n",
      "显卡名称: Intel(R) UHD Graphics 630\n",
      "IP地址: 10.176.130.235\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/BAAI--COIG to C:/Users/Opti7080/.cache/huggingface/datasets/BAAI___json/BAAI--COIG-d717e8bd6e2de3e7/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1033.84it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 282.25it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Failed to read file 'C:\\Users\\Opti7080\\.cache\\huggingface\\datasets\\downloads\\f333eaf7bddee100eed8b350de14d70471681ce0d62c310589a8464d46c13a29' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/textbox_q_context) changed from string to array in row 3\n",
      "                                                        \r"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:134\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 134\u001b[0m         dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39ma JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mkwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m     parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m     parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 2022)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:1858\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1857\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 1858\u001b[0m \u001b[39mfor\u001b[39;00m _, table \u001b[39min\u001b[39;00m generator:\n\u001b[0;32m   1859\u001b[0m     \u001b[39mif\u001b[39;00m max_shard_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m writer\u001b[39m.\u001b[39m_num_bytes \u001b[39m>\u001b[39m max_shard_size:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:137\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    136\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to read file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with error \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    138\u001b[0m \u001b[39m# If possible, parse the file as a list of json objects and exit the loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:113\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     pa_table \u001b[39m=\u001b[39m paj\u001b[39m.\u001b[39;49mread_json(\n\u001b[0;32m    114\u001b[0m         io\u001b[39m.\u001b[39;49mBytesIO(batch), read_options\u001b[39m=\u001b[39;49mpaj\u001b[39m.\u001b[39;49mReadOptions(block_size\u001b[39m=\u001b[39;49mblock_size)\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\pyarrow\\_json.pyx:258\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: JSON parse error: Column(/textbox_q_context) changed from string to array in row 3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mBAAI/COIG\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m#BAAI/COIG  FreedomIntelligence/phoenix-sft-data-v1 silk-road/Wizard-LM-Chinese-instruct-evol\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:985\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[0;32m    983\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split(split_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m    986\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    987\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    988\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    990\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m    992\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:1746\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1744\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1745\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1746\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1747\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1748\u001b[0m     ):\n\u001b[0;32m   1749\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   1750\u001b[0m             result \u001b[39m=\u001b[39m content\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:1891\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1890\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[1;32m-> 1891\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"BAAI/COIG\")\n",
    "#BAAI/COIG  FreedomIntelligence/phoenix-sft-data-v1 silk-road/Wizard-LM-Chinese-instruct-evol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'id'],\n",
       "    num_rows: 464510\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[190284] [Type: Instruction] [Lang: multi-lingual] [Dataset: Alpaca-gpt4-post-translation]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][190000]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset['train'][1]['output_zh']:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_list = pd.DataFrame()\n",
    "output_list['instruction'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1635,\n",
       " 'category': 'information_extraction',\n",
       " 'instruction_zh': '根据以下关于瑞典经济的文章，哪个经济部门占据了最大的产出？',\n",
       " 'context_zh': '瑞典是一个出口导向的混合经济体，拥有现代化的分销系统、优秀的内外部通讯和熟练的劳动力。木材、水力和铁矿石构成了一个经济体的资源基础，这个经济体严重依赖对外贸易。瑞典的工程部门占产出和出口的50%。电信、汽车工业和制药工业也非常重要。农业占GDP和就业的2%。军火工业具有高度先进的技术声誉。',\n",
       " 'response': 'According to this passage, the engineering sector accounts for the largest output, generating 50% of output and exports.',\n",
       " 'instruction': 'Based on the following passage regarding the economy of Sweden, what is the economic sector that accounts for the largest output?',\n",
       " 'context': \"Sweden is an export-oriented mixed economy featuring a modern distribution system, excellent internal and external communications, and a skilled labor force. Timber, hydropower and iron ore constitute the resource base of an economy heavily oriented toward foreign trade. Sweden's engineering sector accounts for 50% of output and exports. Telecommunications, the automotive industry and the pharmaceutical industries are also of great importance. Agriculture accounts for 2 percent of GDP and employment. The armaments industry has a technologically highly advanced reputation.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def has_chinese(text):\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fa5]')  # 匹配中文字符的正则表达式\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 示例用法\n",
    "sentence1 = \"shij!\"  # 包含中文字符\n",
    "print(has_chinese(sentence1))  # 输出: True\n",
    "\n",
    "sentence2 = \"Hello, World!\"  # 不包含中文字符\n",
    "print(has_chinese(sentence2))  # 输出: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.66k/1.66k [00:00<00:00, 1.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/silk-road--Wizard-LM-Chinese-instruct-evol to C:/Users/Opti7080/.cache/huggingface/datasets/silk-road___json/silk-road--Wizard-LM-Chinese-instruct-evol-e6be7ef9d3a16fba/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 243M/243M [00:25<00:00, 9.47MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:29<00:00, 29.57s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 83.97it/s]\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Opti7080/.cache/huggingface/datasets/silk-road___json/silk-road--Wizard-LM-Chinese-instruct-evol-e6be7ef9d3a16fba/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 44.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"silk-road/Wizard-LM-Chinese-instruct-evol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messageshistory = [\n",
    "                    {\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                    {\"role\":\"user\",\"content\":\"请给我介绍一下联想企业\"},\n",
    "                    ]\n",
    "response = openai.ChatCompletion.create(\n",
    "      engine=\"Kepler1\",\n",
    "      messages = messageshistory,\n",
    "      temperature=0.5,\n",
    "      max_tokens=300,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "print('AI bot:',response['choices'][0]['message']['content'])\n",
    "bot = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# print(response)\n",
    "# print(response['choices'][0]['message']['content'])\n",
    "\n",
    "while True :\n",
    "    response = openai.ChatCompletion.create(\n",
    "      engine=\"Kepler1\",\n",
    "      messages = messageshistory,\n",
    "      temperature=0.5,\n",
    "      max_tokens=800,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "    print('AI bot:',response['choices'][0]['message']['content'])  # print the reply of the bot.\n",
    "    bot = response['choices'][0]['message']['content']\n",
    "    a = 1\n",
    "\n",
    "    messageshistory.append({\"role\":\"assistant\",\"content\":bot}) # add bot output to history for chatbot to understand and respond\n",
    "    i = input('User: ')\n",
    "    messageshistory.append({\"role\":\"user\",\"content\":i}) # add user input to history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音量功能匹配为调节音量\n"
     ]
    }
   ],
   "source": [
    "opt5 = Operation5('请帮我降低音量，',model_sim,tokenizer_sim)\n",
    "opt5.judge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03121485,  0.00247283, -0.03576345, ..., -0.00866181,\n",
       "          0.02807012,  0.04373301],\n",
       "        [-0.02591462, -0.01915512, -0.04096009, ..., -0.02056012,\n",
       "          0.00361972,  0.01807105]], dtype=float32),\n",
       " array([372], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt5.corpus_embeddings,opt5.input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(opt5.corpus_embeddings@opt5.input_embeddings.T).argmax(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume.SetMute(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume.SetMute(1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'volume' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m volume\u001b[39m.\u001b[39mSetMasterVolumeLevel(\u001b[39m-\u001b[39m\u001b[39m65.25\u001b[39m \u001b[39m+\u001b[39m \u001b[39m8.26\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'volume' is not defined"
     ]
    }
   ],
   "source": [
    "volume.SetMasterVolumeLevel(-65.25 + 8.26, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from pycaw.pycaw import AudioUtilities, ISimpleAudioVolume\n",
    "\n",
    "# 创建一个Tkinter窗口\n",
    "window = tk.Tk()\n",
    "\n",
    "window.title(\"音量控制\")\n",
    "window.geometry(\"300x100\")\n",
    "\n",
    "# 获取默认的音频会话管理器\n",
    "sessions = AudioUtilities.GetAllSessions()\n",
    "for session in sessions:\n",
    "    volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "    if session.Process and session.Process.name() == \"python.exe\":\n",
    "        volume.SetMasterVolume(0.5, None)  # 初始化音量为50%\n",
    "\n",
    "# 函数：调节音量\n",
    "def set_volume(vol):\n",
    "    for session in sessions:\n",
    "        volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "        if session.Process and session.Process.name() == \"python.exe\":\n",
    "            volume.SetMasterVolume(vol / 100, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('ffmpeg -y -i data/voice1.wav -ac 1 -ar 16000 data/voice.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc:d9:ac:d8:57:06\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def get_mac_address():\n",
    "    mac=uuid.UUID(int = uuid.getnode()).hex[-12:]\n",
    "    return \":\".join([mac[e:e+2] for e in range(0,11,2)])\n",
    "\n",
    "print(get_mac_address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑名: DESKTOP-JHRQTGU.lenovo.com\n",
      "ip地址: 10.176.130.235\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "def get_computer_name_ip():\n",
    "    #获取本机电脑名\n",
    "    name = socket.getfqdn(socket.gethostname())\n",
    "    #获取本机ip\n",
    "    addr = socket.gethostbyname(socket.gethostname())\n",
    "    return name,addr\n",
    "\n",
    "myname,myaddr = get_computer_name_ip()\n",
    "print('电脑名:',myname)\n",
    "print('ip地址:',myaddr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "硬盘序列号 E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
      "CPU序列号 BFEBFBFF000A0655\n",
      "主板序列号 /41JYJ53/CNFCW0007K00LH/\n",
      "mac地址 None\n",
      "mac地址 CC:D9:AC:D8:57:02\n",
      "mac地址 A4:BB:6D:CE:5A:8C\n",
      "mac地址 CC:D9:AC:D8:57:06\n",
      "mac地址 CC:D9:AC:D8:57:03\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 BC:9B:20:52:41:53\n",
      "mac地址 BA:7F:20:52:41:53\n",
      "mac地址 BA:8A:20:52:41:53\n",
      "mac地址 CE:D9:AC:D8:57:02\n",
      "bios序列号 41JYJ53\n"
     ]
    }
   ],
   "source": [
    "import wmi\n",
    "\n",
    "c = wmi.WMI()\n",
    "\n",
    "# # 硬盘序列号\n",
    "for physical_disk in c.Win32_DiskDrive():\n",
    "    print(\"硬盘序列号\", physical_disk.SerialNumber)\n",
    "\n",
    "# CPU序列号\n",
    "for cpu in c.Win32_Processor():\n",
    "    print(\"CPU序列号\", cpu.ProcessorId.strip())\n",
    "\n",
    "# 主板序列号\n",
    "for board_id in c.Win32_BaseBoard():\n",
    "    print(\"主板序列号\", board_id.SerialNumber)\n",
    "\n",
    "# mac地址\n",
    "for mac in c.Win32_NetworkAdapter():\n",
    "    print(\"mac地址\", mac.MACAddress)\n",
    "\n",
    "# bios序列号\n",
    "for bios_id in c.Win32_BIOS():\n",
    "    print(\"bios序列号\", bios_id.SerialNumber.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m search_doc_faiss \n\u001b[0;32m      2\u001b[0m test \u001b[39m=\u001b[39m faiss_corpus(document_corpus\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/app.txt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ..utils import search_doc_faiss \n",
    "test = faiss_corpus(document_corpus='data/app.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise-aware Speech Separation with Contrastive Learning\n",
      "Zizheng Zhang1, Chen Chen2, Xiang Liu1, Yuchen Hu2, Eng Siong Chng2\n",
      "1School of Software and Microelectronics, Peking University, China 2School of Computer Science and Engineering, Nanyang Technological University, Singapore zzz128@stu.pku.edu.cn\n",
      "In this paper, we propose a noise-aware speech separa- tion method called NASS, which follows a typical encoder- separator-decoder pipeline [4]. Unlike previous works, NASS learns to predict the noise signal and leverage it to improve the speech quality of each speaker. Specifically, we conduct patch- wise contrastive learning [16, 17] on different representations: 1) We first sample hundreds of patches from the speaker’s rep- resentations. 2) The corresponding patches from ground-truth representations are viewed as positive examples, while 3) other patches from noise representation are all viewed as negative ex- amples. Reshaped by a two-layer MLP, the positive and nega- tive training examples are calculated with cosine similarities. By optimizing the cross-entropy loss, we minimize the mu- tual information between the representation of each speaker and noise representation, which significantly suppresses the noise information from the separated speech signals.\n",
      "Recently, speech separation (SS) task has achieved remarkable progress driven by deep learning technique. However, it is still challenging to separate target signals from noisy mixture, as neural model is vulnerable to assign background noise to each In this paper, we propose a noise-aware SS method speaker. called NASS, which aims to improve the speech quality of sep- arated signals in noisy conditions. Specifically, NASS views background noise as an independent speaker and predicts it with other speakers in a mask-based manner. Then we conduct patch-wise contrastive learning on feature level to minimize the mutual information between the predicted noise-speaker and other speakers, which suppresses the noise information in sep- arated signals. The experimental results show that NASS effec- tively improves the noise-robustness for different mask-based separation backbones with less than 0.1M parameter increase. Furthermore, SI-SNRi results demonstrate that NASS achieves state-of-the-art performance on WHAM! dataset. Index Terms: noisy speech separation, contrastive learning\n",
      "To evaluate the effectiveness of NASS, we conduct in- tensive experiments on noisy WHAM! [18] and LibriMix [9] datasets, and select three milestone works Conv-TasNet [4], DPRNN [5], and Sepformer [6] as the separator’s backbone. The experimental results show that NASS can effectively im- prove the noise-robustness for all these separation models with less than 0.1M parameter increase. Furthermore, NASS achieves the state-of-the-art (SOTA) performance on WHAM! in terms of SI-SNRi [19] and SDRi [20].\n",
      "1. Introduction\n",
      "Speech separation (SS) aims to separate speech signals from the overlapping speech mixture [1], which can serve as a pre- processor for downstream speech applications [2, 3]. Recently, deep learning-based methods have developed various neural networks for SS [4–7], and achieved remarkable performances on public datasets [8, 9] mixed by clean speech. However, it is still challenging to separate target speech from noisy mixture, e.g., noisy speech separation, since noise signal usually has a wide distribution on frequency domain to interfere with the hu- man voice.    \n",
      "2. NASS Method\n",
      "We now introduce our proposed NASS method, as shown in Figure 1, which consists of mask-based architecture and the patch-wise contrastive learning strategy.\n",
      "2.1. Mask-based Architecture\n",
      "For noisy SS, the mainstream mask-based method [4–7] is vulnerable to assign background noise to the target speaker [10]. We also validate this phenomenon with a related experiment. One intuitive solution is utilizing the speech enhancement (SE) [11] technique as a pre-processor to remove noise information from the mixture in a multi-task learning manner [12, 13]. De- spite the slight improvement, this method may lead to an over- suppression problem [14] — SE module would inevitably re- move some helpful information when it attempts to denoise, thus resulting in a sub-optimal performance for SS.\n",
      "In this work, we follow the encoder-separator-decoder pipeline, where the mask of each speaker is predicted as shown in Fig- ure 1. Since NASS is theoretically applicable to any mask-based method, we select Sepformer as an example in this part due to its remarkable performance in SS.\n",
      "2.1.1. Encoder and Decoder\n",
      "The encoder takes the input noisy mixture xn ∈ R1×T in time domain and then generates a STFT-like [22] representation hxn ∈ RN ×L, where T is the signal length, N is the number of filters and L is the number of vectors:\n",
      "To alleviate the influence of noise, our basic idea is to view background noise as an independent speaker, which can be si- multaneously predicted along with other speakers. In addition to avoiding the over-suppression problem, the estimated noise signal can benefit the separated speech from a mutual informa- tion [15] perspective: we aim to minimize the mutual informa- tion between predicted noise and separated speech, which can prevent the noise from existing in the separated signal.\n",
      "hxn =ReLU(Conv1d(xn))\n",
      "The decoder acts as an inverse operation of the encoder, which takes all predicted representations hk ∈ RN ×L and re-\n",
      "\n",
      "\n",
      "Patch-wiseContrasiveLearning\n",
      "Figure 1: The overall pipeline (2-speaker version) of NASS. xn and ˆn denotes the input noisy mixture and predicted noise. ˆs1 and ˆs2 are the separated speech signals while s1 and s2 are the ground-truth. hˆs1 , hˆs2 and hˆn in dashed boxes are trainable predicted representa- tions, while hˆs1 and hˆs2 in solid boxes are the ground-truth. “+” denotes the mutual information between separated and ground-truth speech is maximized while “-” denotes the mutual information between separated speech and noise is minimized. Utterance-level permutation-invariant training (uPIT) [21] is employed to determine the order of speakers for corresponding comparisons.\n",
      "Patch Sampler\n",
      "negativespositive\n",
      "Classification\n",
      "Processing Block\n",
      "Inter-Transformer\n",
      "Overlap-add\n",
      "LayerNorm + Linear + Chunking\n",
      "Intra-Transformer\n",
      "Repeat times\n",
      "PReLU + Linear\n",
      "Figure 2: The structure of masking network in Sepformer. hxn is the encoded representation of noisy input and mk is the gen- erated trainable mask. “Processing Block” is the main com- ponent, where Sepformer uses the dual path (intra-Transformer and inter-Transformer) [6] for long sequence modeling.\n",
      "Figure 3: The diagram of patch-wise contrastive learning. The query example ri p and negative examples ri n are sampled from the predicted speech representation hˆst , ground-truth speech representation hst and predicted noise representation hˆn, respectively. The classification is based on the results of their cosine similarity.\n",
      "q, positive example ri\n",
      "constructs the separated signals ˆyk ∈ R1×T in time domain:\n",
      "ˆyk = Conv1dTranspose(hk)\n",
      "As mentioned before, we count noise as an additional speaker, which can make use of the noise information within the existing framework. The noise-speaker has its own super- vision and prediction like human speaker. From Equation 2, 3 and 4, we have the predicted noise signal ˆn ∈ R1×T , the noise mask mn ∈ RN ×L and the predicted noise represen- tation hˆn ∈ RN ×L, respectively. Thus far we have mk ∈ {m1, m2, . . . , mC , mn}, hk ∈ {hˆs1 , hˆs2 , . . . , hˆsC , hˆn}, etc., for a total of C + 1 sources.\n",
      "In our work, additionally, the ground-truth speech signal st ∈ R1×T is encoded as hst ∈ RN ×L, which is used to- gether with hk in the subsequent contrastive learning, where t ∈ {1, 2, . . . , C} and C is the number of human speakers.\n",
      "2.1.2. Masking Network and Noise-speaker\n",
      "The masking network takes hxn and learns a mask mk ∈ RN ×L for each of G sources, then yielding hk:\n",
      "2.2. Patch-wise Contrastive Learning\n",
      "mk = MaskingNet(hxn )\n",
      "Based on the noise-speaker, we can further denoise the sepa- rated speech utilizing predicted noise. As shown in Figure 1, patch-wise contrastive learning is conducted on hst and hk, where details can be found in Figure 3.\n",
      "hk = mk · hxn\n",
      "As shown in Figure 2. First hxn is processed by layer nor- malization [23] and a linear layer, then chunked on the time axis with 50% overlap, resulting in an output h′ ∈ RN ×K×S, where K is the chunk size and S is the number of chunks. Next h′ is sent into the processing block, which learns the local and global information by permuting. Then the output of processing block h′′ ∈ RN ×K×S is processed by a PReLU [24] activation and a linear layer, denoted as h′′′ ∈ R(N ×G)×K×S. The overlap-add operation, acting as the inverse of chunking, is employed to ob- tain h′′′′ ∈ R(N ×G)×L from h′′′. Finally h′′′′ goes through a two-layer MLP and a ReLU [25] activation, then generating all mk at once.\n",
      "2.2.1. Patch Sampler\n",
      "We performed 256 comparisons in the contrastive learning. In each comparison, we first randomly sample a small patch on the predicted speech representation hˆst , which serves as a query example ri q. Then the patch of corresponding position on its ground-truth representation hst is sampled, which serves as a positive example ri p. Finally other M patches randomly sam- pled on predicted noise representation hˆn serve as negative ex-\n",
      "\n",
      "\n",
      "Table 1: The ablation study of noise-speaker on LibriMix. “NS” denotes the setting option of noise-speaker. “∗” denotes the baseline results is self-reproduced.\n",
      "amples rj n. The whole process is implemented by patch sam- pler, which learns to project query, positive and negative patches into examples in a shared 3-D embedding unit space, which pre- vents the space from collapsing or expanding:\n",
      "SI-SNRi (dB)\n",
      "n = PatchSampler(hst , hˆst , hˆn)\n",
      "ConvTasNet∗\n",
      "n ∈ RP ×P ×Q and i, j ∈ {1, 2, . . . , M }. P where ri denotes the patch size and Q denotes the features of MLP in patch sampler. In this work, M , P and Q is set to 256, 1 and 256, respectively.\n",
      "2.2.2. Contrastive Loss\n",
      "From the perspective of mutual information, query examples should be similar to corresponding positive examples but dis- similar to all negative examples, which provides an M + 1 clas- sification problem. By calculating their cosine similarities and optimizing the cross-entropy loss, the mutual information be- tween predicted speech and noise would be minimized, thus suppressing the noise from separated speech. The contrastive loss is conducted from each of C human speakers, which can be formulated as:\n",
      "Table 2: The ablation study of patch-wise contrastive learning on Libri2Mix with Sepformer. “S−→N” denotes LS−→N P CL while “N−→S” denotes LN−→S P CL . “λ” indicates the balancing param- eter. “M ” indicates the number of negative patches in each comparison.\n",
      "λ M SI-SNRi (dB)\n",
      "× ✓ × ✓ ✓ ✓ ✓ ✓\n",
      "× ✓ ✓ × × × × ×\n",
      "1 1 1 1 2 2 3\n",
      "256 256 256 1 256 512 256\n",
      "13.3 13.4 13.5 13.5 13.4 13.6 13.5 13.4\n",
      "13.8 14.0 14.0 14.1 14.0 14.1 14.1 14.0\n",
      "25.9 25.9 25.9 25.9 25.9 25.9 25.9 25.9\n",
      "eri q ·ri p/τ p/τ + (cid:80)M j=1 eri\n",
      "where τ denotes the temperature parameter [26], and is set to 0.07 in this work.\n",
      "38 and -30 dB of LUFS. With 331 speakers, Libri2Mix has 80 hours of speech (58h, 11h, 11h for training, validation, test, respectively) and Libri3Mix has 62 hours (40h, 11h, 11h for training, validation, test, respectively).\n",
      "2.3. Training Objective\n",
      "The main separation loss Lsi−snr is to maximize SI-SNR be- tween separated signals ˆyk and ground-truth signals yk for G sources:\n",
      "3.2. Experimental Setup\n",
      "∥ ˆyT k yk ∥yk∥2 yk∥ ∥ ˆyT k yk ∥yk∥2 − ˆyk∥\n",
      "To ensure reproducibility, we conduct the experiments on SpeechBrain [29], an open-source AI toolkit for SS tasks. The network configurations of existing methods are the same as WSJ0-Mix recipes on SpeechBrain, which follows the prior works where more details can be found.\n",
      "Thus far, the total loss of proposed NASS method is formu-\n",
      "We train 200 epochs for all methods on NVIDIA V100 GPUs, using Adam optimizer [30] with initial learning rate of 1.5 × 10−4 and automatic mixed precision [31]. After train- ing 85 epochs (5 epochs for Conv-TasNet), the learning rate is halved if with no improvement of validation for 3 epochs. Speed perturbation [32] is applied for data augmentation. There’s no dynamic mixing [33] in our experiments. The batch size and number of workers are set to 1 and 4. The training signal length is 4 seconds long and loss threshold is set to -30. Gradient clip- ping is applied to limit the L2 norm of gradients to 5. All the hyperparameters are adjusted on the validation set.\n",
      "LT otal = Lsi−snr + λLP CL\n",
      "where λ is the parameter to balance SS loss Lsi−snr and con- trastive loss LP CL, which is set to 2 or 3 in this work.\n",
      "The model is trained with uPIT. It is worthy noted that LP CL utilizes the permutation result of Lsi−snr, which reduces double-counting and ensures the correct comparisons.\n",
      "3. Experiments\n",
      "3.1. Datasets\n",
      "We evaluate NASS and existing methods on two common noisy datasets: WHAM! and LibriMix. WHAM! is a noisy version of WSJ0-2Mix [8], which is added noise samples recorded in coffee shops, restaurants and bars. SNR between the loudest speaker and noise varies from -6 to +3 dB. WHAM! follows the same structure as WSJ0-2Mix, which has 119 speakers and 43 hours of speech (30h, 8h, 5h for training, validation, test, respectively). LibriMix contains Libri2Mix and Libri3Mix for noisy multi-speaker SS tasks. In our chosen version of Lib- riMix, the clean mixture is selected from LibriSpeech train- 100 [27] and mixed between -25 and -33 dB of LUFS [28]. Noise samples from WHAM! are added to the mixture between\n",
      "3.3. Metrics and Baselines\n",
      "We use the results of SI-SNRi and SDRi in the test set to evalu- ate all experiments, which measure the degree of improvement in clarity and fidelity of these separated audios. For all metrics, higher score indicates better performance.\n",
      "To assess the effectiveness of proposed NASS method, we set three baselines from mask-based methods for comparisons: • ConvTasNet [4]: A fully-convolutional network without dual\n",
      "path but good performance in clean SS tasks.\n",
      "DPRNN [5]: An RNN-based dual-path model with relatively\n",
      "\n",
      "\n",
      "(b)(c)(e)(g)(f)(d)Speaker 1Speaker 2\n",
      "Table 3: The competitive results of proposed NASS method on LibriMix. “∗” denotes the baseline results is self-reproduced.\n",
      "SI-SNRi (dB)\n",
      "ConvTasNet∗\n",
      "ConvTasNet (NASS)\n",
      "DPRNN (NASS)\n",
      "Sepformer (NASS)\n",
      "Table 4: The competitive results of proposed NASS method on WHAM!. The baseline results are reported in the original paper [7], where all can be found.\n",
      "Figure 4: The comparison results of spectrum on Libri2mix with Sepformer. Subplot (a) denotes the mixture; (b), (d), (f) denotes the separated result of speaker 1 in Sepformer baseline, noise- speaker, patch-wise contrastive learning, respectively; (c), (e), (g) denotes the same as (b), (d), (f) in speaker 2, respectively; (h), (i) denotes the result of predicted noise in noise-speaker, patch-wise contrastive learning, respectively.\n",
      "SI-SNRi (dB)\n",
      "DPRNN Sepformer TDANet\n",
      "13.7 14.4 15.2\n",
      "14.1 15.0 15.4\n",
      "2.7 26.0 2.3\n",
      "DPRNN (NASS) Sepformer (NASS)\n",
      "large receptive field for long sequence modeling.\n",
      "datasets. It’s worth pointing out that NASS can make the pre- vious DPRNN and Sepformer surpass the current SOTA model TDANet in WHAM!, within 0.1M additional parameters.\n",
      "Sepformer [6]: A Transformer-based dual-path network with relatively large size but excellent performance, allowing par- allel computations.\n",
      "In addition, to further illustrate the effectiveness of NASS, Figure 4 visualizes the comparison results of spectrum for 2- speaker SS. We can see some spectrum of noise in (b), (c) from the Sepformer baseline. While from (d), (e) with noise-speaker, and (f), (g) with additional patch-wise contrastive learning, we can see the spectrum of noise gradually fades away in a degree, thus separating the two sources better. Besides, NASS yields the prediction of noise, which can be used in other speech tasks. In (h), (i), we can also see an improvement from the patch-wise contrastive learning to noise-speaker.\n",
      "4.1. Effect of Noise-speaker\n",
      "The effect of noise-speaker is shown in Table 1. Note that the all following test results of SI-SNR include human speakers only. Compared to the baselines, benefit from noise supervision, the two models with noise-speaker perform better in the separation of either 2 or 3 speakers, especially in the case of 3 speakers, and within 0.1M additional parameters.\n",
      "5. Conclusions\n",
      "4.2. Effect of Patch-wise Contrastive Learning\n",
      "In this paper, we propose NASS method to improve the noise- robustness of previous SS works. Specifically, noise-speaker is set up to make use of the noise supervision, and the prediction of noise can be used by other speech tasks. Patch-wise con- trastive learning is conducted on predicted speech and noise rep- resentations, which minimize the mutual information beetween them, thus separating to each other in detail. Experimental re- sults show that NASS can significantly suppress the noise from separated speech, with less than 0.1M additional parameters and achieves the current SOTA on WHAM! dataset.\n",
      "Based on noise-speaker, we study the effect of patch-wise con- trastive learning in Table 2. As mentioned in Equation 6, the query example ri q and negative examples rj n come from speech and noise, respectively, denoting the loss as LS−→N P CL . However, the query example can be sampled from noise representation and compared to the negative examples on speech representa- tions, denoting the loss as LN−→S P CL . The results show that both the choices work but conflict to each other in a degree, and the best is with LS−→N P CL only. We also conduct the ablation study from the balancing parameter λ and the number of negative patches M in each comparison. The results show that the best settings are as described previously.\n",
      "4.3. Benchmark against Competitive Methods\n",
      "Table 3 and Table 4 show the comparison results on noisy Lib- riMix and WHAM!, respectively. Not only self-reproduced but other-reported results indicate NASS \n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"第1页\n",
    "Noise-aware Speech Separation with Contrastive Learning\n",
    "Zizheng Zhang1, Chen Chen2, Xiang Liu1, Yuchen Hu2, Eng Siong Chng2\n",
    "1School of Software and Microelectronics, Peking University, China 2School of Computer Science and Engineering, Nanyang Technological University, Singapore zzz128@stu.pku.edu.cn\n",
    "In this paper, we propose a noise-aware speech separa- tion method called NASS, which follows a typical encoder- separator-decoder pipeline [4]. Unlike previous works, NASS learns to predict the noise signal and leverage it to improve the speech quality of each speaker. Specifically, we conduct patch- wise contrastive learning [16, 17] on different representations: 1) We first sample hundreds of patches from the speaker’s rep- resentations. 2) The corresponding patches from ground-truth representations are viewed as positive examples, while 3) other patches from noise representation are all viewed as negative ex- amples. Reshaped by a two-layer MLP, the positive and nega- tive training examples are calculated with cosine similarities. By optimizing the cross-entropy loss, we minimize the mu- tual information between the representation of each speaker and noise representation, which significantly suppresses the noise information from the separated speech signals.\n",
    "Recently, speech separation (SS) task has achieved remarkable progress driven by deep learning technique. However, it is still challenging to separate target signals from noisy mixture, as neural model is vulnerable to assign background noise to each In this paper, we propose a noise-aware SS method speaker. called NASS, which aims to improve the speech quality of sep- arated signals in noisy conditions. Specifically, NASS views background noise as an independent speaker and predicts it with other speakers in a mask-based manner. Then we conduct patch-wise contrastive learning on feature level to minimize the mutual information between the predicted noise-speaker and other speakers, which suppresses the noise information in sep- arated signals. The experimental results show that NASS effec- tively improves the noise-robustness for different mask-based separation backbones with less than 0.1M parameter increase. Furthermore, SI-SNRi results demonstrate that NASS achieves state-of-the-art performance on WHAM! dataset. Index Terms: noisy speech separation, contrastive learning\n",
    "To evaluate the effectiveness of NASS, we conduct in- tensive experiments on noisy WHAM! [18] and LibriMix [9] datasets, and select three milestone works Conv-TasNet [4], DPRNN [5], and Sepformer [6] as the separator’s backbone. The experimental results show that NASS can effectively im- prove the noise-robustness for all these separation models with less than 0.1M parameter increase. Furthermore, NASS achieves the state-of-the-art (SOTA) performance on WHAM! in terms of SI-SNRi [19] and SDRi [20].\n",
    "1. Introduction\n",
    "Speech separation (SS) aims to separate speech signals from the overlapping speech mixture [1], which can serve as a pre- processor for downstream speech applications [2, 3]. Recently, deep learning-based methods have developed various neural networks for SS [4–7], and achieved remarkable performances on public datasets [8, 9] mixed by clean speech. However, it is still challenging to separate target speech from noisy mixture, e.g., noisy speech separation, since noise signal usually has a wide distribution on frequency domain to interfere with the hu- man voice.    \n",
    "2. NASS Method\n",
    "We now introduce our proposed NASS method, as shown in Figure 1, which consists of mask-based architecture and the patch-wise contrastive learning strategy.\n",
    "2.1. Mask-based Architecture\n",
    "For noisy SS, the mainstream mask-based method [4–7] is vulnerable to assign background noise to the target speaker [10]. We also validate this phenomenon with a related experiment. One intuitive solution is utilizing the speech enhancement (SE) [11] technique as a pre-processor to remove noise information from the mixture in a multi-task learning manner [12, 13]. De- spite the slight improvement, this method may lead to an over- suppression problem [14] — SE module would inevitably re- move some helpful information when it attempts to denoise, thus resulting in a sub-optimal performance for SS.\n",
    "In this work, we follow the encoder-separator-decoder pipeline, where the mask of each speaker is predicted as shown in Fig- ure 1. Since NASS is theoretically applicable to any mask-based method, we select Sepformer as an example in this part due to its remarkable performance in SS.\n",
    "2.1.1. Encoder and Decoder\n",
    "The encoder takes the input noisy mixture xn ∈ R1×T in time domain and then generates a STFT-like [22] representation hxn ∈ RN ×L, where T is the signal length, N is the number of filters and L is the number of vectors:\n",
    "To alleviate the influence of noise, our basic idea is to view background noise as an independent speaker, which can be si- multaneously predicted along with other speakers. In addition to avoiding the over-suppression problem, the estimated noise signal can benefit the separated speech from a mutual informa- tion [15] perspective: we aim to minimize the mutual informa- tion between predicted noise and separated speech, which can prevent the noise from existing in the separated signal.\n",
    "hxn =ReLU(Conv1d(xn))\n",
    "The decoder acts as an inverse operation of the encoder, which takes all predicted representations hk ∈ RN ×L and re-\n",
    "\n",
    "第2页\n",
    "Patch-wiseContrasiveLearning\n",
    "Figure 1: The overall pipeline (2-speaker version) of NASS. xn and ˆn denotes the input noisy mixture and predicted noise. ˆs1 and ˆs2 are the separated speech signals while s1 and s2 are the ground-truth. hˆs1 , hˆs2 and hˆn in dashed boxes are trainable predicted representa- tions, while hˆs1 and hˆs2 in solid boxes are the ground-truth. “+” denotes the mutual information between separated and ground-truth speech is maximized while “-” denotes the mutual information between separated speech and noise is minimized. Utterance-level permutation-invariant training (uPIT) [21] is employed to determine the order of speakers for corresponding comparisons.\n",
    "Patch Sampler\n",
    "negativespositive\n",
    "Classification\n",
    "Processing Block\n",
    "Inter-Transformer\n",
    "Overlap-add\n",
    "LayerNorm + Linear + Chunking\n",
    "Intra-Transformer\n",
    "Repeat times\n",
    "PReLU + Linear\n",
    "Figure 2: The structure of masking network in Sepformer. hxn is the encoded representation of noisy input and mk is the gen- erated trainable mask. “Processing Block” is the main com- ponent, where Sepformer uses the dual path (intra-Transformer and inter-Transformer) [6] for long sequence modeling.\n",
    "Figure 3: The diagram of patch-wise contrastive learning. The query example ri p and negative examples ri n are sampled from the predicted speech representation hˆst , ground-truth speech representation hst and predicted noise representation hˆn, respectively. The classification is based on the results of their cosine similarity.\n",
    "q, positive example ri\n",
    "constructs the separated signals ˆyk ∈ R1×T in time domain:\n",
    "ˆyk = Conv1dTranspose(hk)\n",
    "As mentioned before, we count noise as an additional speaker, which can make use of the noise information within the existing framework. The noise-speaker has its own super- vision and prediction like human speaker. From Equation 2, 3 and 4, we have the predicted noise signal ˆn ∈ R1×T , the noise mask mn ∈ RN ×L and the predicted noise represen- tation hˆn ∈ RN ×L, respectively. Thus far we have mk ∈ {m1, m2, . . . , mC , mn}, hk ∈ {hˆs1 , hˆs2 , . . . , hˆsC , hˆn}, etc., for a total of C + 1 sources.\n",
    "In our work, additionally, the ground-truth speech signal st ∈ R1×T is encoded as hst ∈ RN ×L, which is used to- gether with hk in the subsequent contrastive learning, where t ∈ {1, 2, . . . , C} and C is the number of human speakers.\n",
    "2.1.2. Masking Network and Noise-speaker\n",
    "The masking network takes hxn and learns a mask mk ∈ RN ×L for each of G sources, then yielding hk:\n",
    "2.2. Patch-wise Contrastive Learning\n",
    "mk = MaskingNet(hxn )\n",
    "Based on the noise-speaker, we can further denoise the sepa- rated speech utilizing predicted noise. As shown in Figure 1, patch-wise contrastive learning is conducted on hst and hk, where details can be found in Figure 3.\n",
    "hk = mk · hxn\n",
    "As shown in Figure 2. First hxn is processed by layer nor- malization [23] and a linear layer, then chunked on the time axis with 50% overlap, resulting in an output h′ ∈ RN ×K×S, where K is the chunk size and S is the number of chunks. Next h′ is sent into the processing block, which learns the local and global information by permuting. Then the output of processing block h′′ ∈ RN ×K×S is processed by a PReLU [24] activation and a linear layer, denoted as h′′′ ∈ R(N ×G)×K×S. The overlap-add operation, acting as the inverse of chunking, is employed to ob- tain h′′′′ ∈ R(N ×G)×L from h′′′. Finally h′′′′ goes through a two-layer MLP and a ReLU [25] activation, then generating all mk at once.\n",
    "2.2.1. Patch Sampler\n",
    "We performed 256 comparisons in the contrastive learning. In each comparison, we first randomly sample a small patch on the predicted speech representation hˆst , which serves as a query example ri q. Then the patch of corresponding position on its ground-truth representation hst is sampled, which serves as a positive example ri p. Finally other M patches randomly sam- pled on predicted noise representation hˆn serve as negative ex-\n",
    "\n",
    "第3页\n",
    "Table 1: The ablation study of noise-speaker on LibriMix. “NS” denotes the setting option of noise-speaker. “∗” denotes the baseline results is self-reproduced.\n",
    "amples rj n. The whole process is implemented by patch sam- pler, which learns to project query, positive and negative patches into examples in a shared 3-D embedding unit space, which pre- vents the space from collapsing or expanding:\n",
    "SI-SNRi (dB)\n",
    "n = PatchSampler(hst , hˆst , hˆn)\n",
    "ConvTasNet∗\n",
    "n ∈ RP ×P ×Q and i, j ∈ {1, 2, . . . , M }. P where ri denotes the patch size and Q denotes the features of MLP in patch sampler. In this work, M , P and Q is set to 256, 1 and 256, respectively.\n",
    "2.2.2. Contrastive Loss\n",
    "From the perspective of mutual information, query examples should be similar to corresponding positive examples but dis- similar to all negative examples, which provides an M + 1 clas- sification problem. By calculating their cosine similarities and optimizing the cross-entropy loss, the mutual information be- tween predicted speech and noise would be minimized, thus suppressing the noise from separated speech. The contrastive loss is conducted from each of C human speakers, which can be formulated as:\n",
    "Table 2: The ablation study of patch-wise contrastive learning on Libri2Mix with Sepformer. “S−→N” denotes LS−→N P CL while “N−→S” denotes LN−→S P CL . “λ” indicates the balancing param- eter. “M ” indicates the number of negative patches in each comparison.\n",
    "λ M SI-SNRi (dB)\n",
    "× ✓ × ✓ ✓ ✓ ✓ ✓\n",
    "× ✓ ✓ × × × × ×\n",
    "1 1 1 1 2 2 3\n",
    "256 256 256 1 256 512 256\n",
    "13.3 13.4 13.5 13.5 13.4 13.6 13.5 13.4\n",
    "13.8 14.0 14.0 14.1 14.0 14.1 14.1 14.0\n",
    "25.9 25.9 25.9 25.9 25.9 25.9 25.9 25.9\n",
    "eri q ·ri p/τ p/τ + (cid:80)M j=1 eri\n",
    "where τ denotes the temperature parameter [26], and is set to 0.07 in this work.\n",
    "38 and -30 dB of LUFS. With 331 speakers, Libri2Mix has 80 hours of speech (58h, 11h, 11h for training, validation, test, respectively) and Libri3Mix has 62 hours (40h, 11h, 11h for training, validation, test, respectively).\n",
    "2.3. Training Objective\n",
    "The main separation loss Lsi−snr is to maximize SI-SNR be- tween separated signals ˆyk and ground-truth signals yk for G sources:\n",
    "3.2. Experimental Setup\n",
    "∥ ˆyT k yk ∥yk∥2 yk∥ ∥ ˆyT k yk ∥yk∥2 − ˆyk∥\n",
    "To ensure reproducibility, we conduct the experiments on SpeechBrain [29], an open-source AI toolkit for SS tasks. The network configurations of existing methods are the same as WSJ0-Mix recipes on SpeechBrain, which follows the prior works where more details can be found.\n",
    "Thus far, the total loss of proposed NASS method is formu-\n",
    "We train 200 epochs for all methods on NVIDIA V100 GPUs, using Adam optimizer [30] with initial learning rate of 1.5 × 10−4 and automatic mixed precision [31]. After train- ing 85 epochs (5 epochs for Conv-TasNet), the learning rate is halved if with no improvement of validation for 3 epochs. Speed perturbation [32] is applied for data augmentation. There’s no dynamic mixing [33] in our experiments. The batch size and number of workers are set to 1 and 4. The training signal length is 4 seconds long and loss threshold is set to -30. Gradient clip- ping is applied to limit the L2 norm of gradients to 5. All the hyperparameters are adjusted on the validation set.\n",
    "LT otal = Lsi−snr + λLP CL\n",
    "where λ is the parameter to balance SS loss Lsi−snr and con- trastive loss LP CL, which is set to 2 or 3 in this work.\n",
    "The model is trained with uPIT. It is worthy noted that LP CL utilizes the permutation result of Lsi−snr, which reduces double-counting and ensures the correct comparisons.\n",
    "3. Experiments\n",
    "3.1. Datasets\n",
    "We evaluate NASS and existing methods on two common noisy datasets: WHAM! and LibriMix. WHAM! is a noisy version of WSJ0-2Mix [8], which is added noise samples recorded in coffee shops, restaurants and bars. SNR between the loudest speaker and noise varies from -6 to +3 dB. WHAM! follows the same structure as WSJ0-2Mix, which has 119 speakers and 43 hours of speech (30h, 8h, 5h for training, validation, test, respectively). LibriMix contains Libri2Mix and Libri3Mix for noisy multi-speaker SS tasks. In our chosen version of Lib- riMix, the clean mixture is selected from LibriSpeech train- 100 [27] and mixed between -25 and -33 dB of LUFS [28]. Noise samples from WHAM! are added to the mixture between\n",
    "3.3. Metrics and Baselines\n",
    "We use the results of SI-SNRi and SDRi in the test set to evalu- ate all experiments, which measure the degree of improvement in clarity and fidelity of these separated audios. For all metrics, higher score indicates better performance.\n",
    "To assess the effectiveness of proposed NASS method, we set three baselines from mask-based methods for comparisons: • ConvTasNet [4]: A fully-convolutional network without dual\n",
    "path but good performance in clean SS tasks.\n",
    "DPRNN [5]: An RNN-based dual-path model with relatively\n",
    "\n",
    "第4页\n",
    "(b)(c)(e)(g)(f)(d)Speaker 1Speaker 2\n",
    "Table 3: The competitive results of proposed NASS method on LibriMix. “∗” denotes the baseline results is self-reproduced.\n",
    "SI-SNRi (dB)\n",
    "ConvTasNet∗\n",
    "ConvTasNet (NASS)\n",
    "DPRNN (NASS)\n",
    "Sepformer (NASS)\n",
    "Table 4: The competitive results of proposed NASS method on WHAM!. The baseline results are reported in the original paper [7], where all can be found.\n",
    "Figure 4: The comparison results of spectrum on Libri2mix with Sepformer. Subplot (a) denotes the mixture; (b), (d), (f) denotes the separated result of speaker 1 in Sepformer baseline, noise- speaker, patch-wise contrastive learning, respectively; (c), (e), (g) denotes the same as (b), (d), (f) in speaker 2, respectively; (h), (i) denotes the result of predicted noise in noise-speaker, patch-wise contrastive learning, respectively.\n",
    "SI-SNRi (dB)\n",
    "DPRNN Sepformer TDANet\n",
    "13.7 14.4 15.2\n",
    "14.1 15.0 15.4\n",
    "2.7 26.0 2.3\n",
    "DPRNN (NASS) Sepformer (NASS)\n",
    "large receptive field for long sequence modeling.\n",
    "datasets. It’s worth pointing out that NASS can make the pre- vious DPRNN and Sepformer surpass the current SOTA model TDANet in WHAM!, within 0.1M additional parameters.\n",
    "Sepformer [6]: A Transformer-based dual-path network with relatively large size but excellent performance, allowing par- allel computations.\n",
    "In addition, to further illustrate the effectiveness of NASS, Figure 4 visualizes the comparison results of spectrum for 2- speaker SS. We can see some spectrum of noise in (b), (c) from the Sepformer baseline. While from (d), (e) with noise-speaker, and (f), (g) with additional patch-wise contrastive learning, we can see the spectrum of noise gradually fades away in a degree, thus separating the two sources better. Besides, NASS yields the prediction of noise, which can be used in other speech tasks. In (h), (i), we can also see an improvement from the patch-wise contrastive learning to noise-speaker.\n",
    "4.1. Effect of Noise-speaker\n",
    "The effect of noise-speaker is shown in Table 1. Note that the all following test results of SI-SNR include human speakers only. Compared to the baselines, benefit from noise supervision, the two models with noise-speaker perform better in the separation of either 2 or 3 speakers, especially in the case of 3 speakers, and within 0.1M additional parameters.\n",
    "5. Conclusions\n",
    "4.2. Effect of Patch-wise Contrastive Learning\n",
    "In this paper, we propose NASS method to improve the noise- robustness of previous SS works. Specifically, noise-speaker is set up to make use of the noise supervision, and the prediction of noise can be used by other speech tasks. Patch-wise con- trastive learning is conducted on predicted speech and noise rep- resentations, which minimize the mutual information beetween them, thus separating to each other in detail. Experimental re- sults show that NASS can significantly suppress the noise from separated speech, with less than 0.1M additional parameters and achieves the current SOTA on WHAM! dataset.\n",
    "Based on noise-speaker, we study the effect of patch-wise con- trastive learning in Table 2. As mentioned in Equation 6, the query example ri q and negative examples rj n come from speech and noise, respectively, denoting the loss as LS−→N P CL . However, the query example can be sampled from noise representation and compared to the negative examples on speech representa- tions, denoting the loss as LN−→S P CL . The results show that both the choices work but conflict to each other in a degree, and the best is with LS−→N P CL only. We also conduct the ablation study from the balancing parameter λ and the number of negative patches M in each comparison. The results show that the best settings are as described previously.\n",
    "4.3. Benchmark against Competitive Methods\n",
    "Table 3 and Table 4 show the comparison results on noisy Lib- riMix and WHAM!, respectively. Not only self-reproduced but other-reported results indicate NASS \"\"\"\n",
    "\n",
    "\n",
    "pattern = r'第(\\d+)页\\n'\n",
    "import re\n",
    "doc = []\n",
    "for item in re.split(pattern,context):\n",
    "    if len(item)>1:\n",
    "        print(item)\n",
    "        doc.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['Noise-aware Speech Separation with Contrastive Learning','In this paper, we propose NASS method to imp','The decoder acts as an inverse operation of the encoder, w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noise-aware Speech Separation with Contrastive Learning',\n",
       " 'Zizheng Zhang1, Chen Chen2, Xiang Liu1, Yuchen Hu2, Eng Siong Chng2',\n",
       " '1School of Software and Microelectronics, Peking University, China 2School of Computer Science and Engineering, Nanyang Technological University, Singapore zzz128@stu.pku.edu.cn',\n",
       " 'In this paper, we propose a noise-aware speech separa- tion method called NASS, which follows a typical encoder- separator-decoder pipeline [4]. Unlike previous works, NASS learns to predict the noise signal and leverage it to improve the speech quality of each speaker. Specifically, we conduct patch- wise contrastive learning [16, 17] on different representations: 1) We first sample hundreds of patches from the speaker’s rep- resentations. 2) The corresponding patches from ground-truth representations are viewed as positive examples, while 3) other patches from noise representation are all viewed as negative ex- amples. Reshaped by a two-layer MLP, the positive and nega- tive training examples are calculated with cosine similarities. By optimizing the cross-entropy loss, we minimize the mu- tual information between the representation of each speaker and noise representation, which significantly suppresses the noise information from the separated speech signals.',\n",
       " 'Recently, speech separation (SS) task has achieved remarkable progress driven by deep learning technique. However, it is still challenging to separate target signals from noisy mixture, as neural model is vulnerable to assign background noise to each In this paper, we propose a noise-aware SS method speaker. called NASS, which aims to improve the speech quality of sep- arated signals in noisy conditions. Specifically, NASS views background noise as an independent speaker and predicts it with other speakers in a mask-based manner. Then we conduct patch-wise contrastive learning on feature level to minimize the mutual information between the predicted noise-speaker and other speakers, which suppresses the noise information in sep- arated signals. The experimental results show that NASS effec- tively improves the noise-robustness for different mask-based separation backbones with less than 0.1M parameter increase. Furthermore, SI-SNRi results demonstrate that NASS achieves state-of-the-art performance on WHAM! dataset. Index Terms: noisy speech separation, contrastive learning',\n",
       " 'To evaluate the effectiveness of NASS, we conduct in- tensive experiments on noisy WHAM! [18] and LibriMix [9] datasets, and select three milestone works Conv-TasNet [4], DPRNN [5], and Sepformer [6] as the separator’s backbone. The experimental results show that NASS can effectively im- prove the noise-robustness for all these separation models with less than 0.1M parameter increase. Furthermore, NASS achieves the state-of-the-art (SOTA) performance on WHAM! in terms of SI-SNRi [19] and SDRi [20].',\n",
       " '1. Introduction',\n",
       " 'Speech separation (SS) aims to separate speech signals from the overlapping speech mixture [1], which can serve as a pre- processor for downstream speech applications [2, 3]. Recently, deep learning-based methods have developed various neural networks for SS [4–7], and achieved remarkable performances on public datasets [8, 9] mixed by clean speech. However, it is still challenging to separate target speech from noisy mixture, e.g., noisy speech separation, since noise signal usually has a wide distribution on frequency domain to interfere with the hu- man voice.    ',\n",
       " '2. NASS Method',\n",
       " 'We now introduce our proposed NASS method, as shown in Figure 1, which consists of mask-based architecture and the patch-wise contrastive learning strategy.',\n",
       " '2.1. Mask-based Architecture',\n",
       " 'For noisy SS, the mainstream mask-based method [4–7] is vulnerable to assign background noise to the target speaker [10]. We also validate this phenomenon with a related experiment. One intuitive solution is utilizing the speech enhancement (SE) [11] technique as a pre-processor to remove noise information from the mixture in a multi-task learning manner [12, 13]. De- spite the slight improvement, this method may lead to an over- suppression problem [14] — SE module would inevitably re- move some helpful information when it attempts to denoise, thus resulting in a sub-optimal performance for SS.',\n",
       " 'In this work, we follow the encoder-separator-decoder pipeline, where the mask of each speaker is predicted as shown in Fig- ure 1. Since NASS is theoretically applicable to any mask-based method, we select Sepformer as an example in this part due to its remarkable performance in SS.',\n",
       " '2.1.1. Encoder and Decoder',\n",
       " 'The encoder takes the input noisy mixture xn ∈ R1×T in time domain and then generates a STFT-like [22] representation hxn ∈ RN ×L, where T is the signal length, N is the number of filters and L is the number of vectors:',\n",
       " 'To alleviate the influence of noise, our basic idea is to view background noise as an independent speaker, which can be si- multaneously predicted along with other speakers. In addition to avoiding the over-suppression problem, the estimated noise signal can benefit the separated speech from a mutual informa- tion [15] perspective: we aim to minimize the mutual informa- tion between predicted noise and separated speech, which can prevent the noise from existing in the separated signal.',\n",
       " 'hxn =ReLU(Conv1d(xn))',\n",
       " 'The decoder acts as an inverse operation of the encoder, which takes all predicted representations hk ∈ RN ×L and re-',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('D:/conda/envs/QA/bin')}\n",
      "  warn(msg)\n",
      "d:\\conda\\envs\\QA\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: D:\\conda\\envs\\QA did not contain ['cudart64_110.dll', 'cudart64_120.dll', 'cudart64_12.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "You are using a model of type chatglm to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:24<00:00,  3.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(65024, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear8bitLt(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): CoreAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear8bitLt(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.append(\"..\")\n",
    "from utils.search_doc_faiss import faiss_corpus\n",
    "import argparse\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# def get_parser():\n",
    "#     parser = argparse.ArgumentParser('Matching Task')\n",
    "#     parser.add_argument('--document-corpus', default='../data/app.txt')\n",
    "#     parser.add_argument('--new-embed', default=False,type=bool)\n",
    "#     parser.add_argument('--document-embed', default='./data/document_embed.npy')\n",
    "#     parser.add_argument('--k', default=1)\n",
    "#     parser.add_argument('--device', default=\"cuda\")\n",
    "#     parser.add_argument('--model-name', default=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "#     parser.add_argument('--index_location', default='../data/pdf_search.index')\n",
    "#     parser.add_argument('--search', default=1)\n",
    "#     args = parser.parse_args([])      \n",
    "#     return args\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--document-corpus', default='./data/document_corpus.txt')\n",
    "    parser.add_argument('--k', default=1)\n",
    "    parser.add_argument('--device', default=\"cuda\")\n",
    "    parser.add_argument('--index_location', default='./data/test.index')\n",
    "    parser.add_argument('--search', default=1)\n",
    "    # parser.add_argument('--model-name', default=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    parser.add_argument('--work-dir',default=r\"./tzh_model/lenomate_hi\")\n",
    "    parser.add_argument('--model-dir',default=r\"C:\\Users\\89721\\Desktop\\model_chatglm2\")\n",
    "    # parser.add_argument('--simmodel-dir',default=r\"C:\\Users\\89721\\Desktop\\models--GanymedeNil--text2vec-large-chinese\")\n",
    "    parser.add_argument('--simmodel-dir',default=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    \n",
    "    parser.add_argument('--temperature',default=0.95)\n",
    "    # parser.add_argument('--model-dir',default=r\"/home/tzh/model_dir_cache/models--THUDM--chatglm-6b\")\n",
    "    config = parser.parse_args([])      \n",
    "    return config\n",
    "args = get_parser()\n",
    "model = AutoModel.from_pretrained(args.model_dir, load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_dir,trust_remote_code=True)\n",
    "model_sim =  AutoModel.from_pretrained(args.simmodel_dir).to('cuda')\n",
    "tokenizer_sim = AutoTokenizer.from_pretrained(args.simmodel_dir)\n",
    "# model = PeftModel.from_pretrained(model, args.work_dir)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operation.read_file import read_file\n",
    "file = read_file(r\"C:\\Users\\89721\\Downloads\\ThinkCentre+FY2021+Alder+Lake+TDT+MRD-V1.32+20210423.pptx\")\n",
    "cont = file.fit(trucation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 17 09:57:06 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.30                 Driver Version: 531.30       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "| 34%   32C    P8               10W / 170W|  11906MiB / 12288MiB |      6%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1368    C+G   ...)\\NetEase\\CloudMusic\\cloudmusic.exe    N/A      |\n",
      "|    0   N/A  N/A      7152    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7596    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A      8740    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10436    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10460    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11336    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     11624    C+G   ...on\\114.0.1823.79\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     12072    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18648    C+G   ...on\\114.0.1823.82\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     19992    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     20672    C+G   ...5\\extracted\\runtime\\WeChatAppEx.exe    N/A      |\n",
      "|    0   N/A  N/A     20908      C   D:\\conda\\envs\\QA\\python.exe               N/A      |\n",
      "|    0   N/A  N/A     21368    C+G   ...4309\\office6\\promecefpluginhost.exe    N/A      |\n",
      "|    0   N/A  N/A     21816    C+G   ...vo\\Ready For Assistant\\ReadyFor.exe    N/A      |\n",
      "|    0   N/A  N/A     22112    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下PPT的内容，简洁和专业的来回答用户关于此PPT的问题。\n",
      "## PPT内容:\n",
      "第1页：\n",
      " ThinkCentre Tower FY2122 Intel Alder Lake MRD V1.3 \n",
      "第2页：\n",
      "Contents Revision History Executive Summary Marketing Strategy Product Vision Customer Feedback Market Overview Roadmap Product Overview Product Launch Plan Country / Region Scope Volume Call Marketing Requirement Hardware Definition Configuration Requirement Sample Requirement Software Definition BIOS Requirement OS / App Requirement Building Block Accessory Service Package Regulation  \n",
      "第3页：\n",
      "Revision History  | Date | Version | What’s Changed |\n",
      "| --- | --- | --- |\n",
      "| Date | Version | What’s Changed |\n",
      "| 2020-08-03 | 0.1 | Preliminary release |\n",
      "| 2020-09-10 | 0.2 | Spec update |\n",
      "|  |  | Add Anti Virus/Bacterium bezel |\n",
      "| 2020-09-17 | 0.3 | Add OS scope |\n",
      "|  |  | Update country scope |\n",
      "|  |  | Update M757t spec |\n",
      "|  | 0.4 | Add self-healing feature(HDD backup) for KT, P19 |\n",
      "|  |  | Clarify 17L EOU request, p16 |\n",
      "|  |  | Update New BIOS request for FY22 ADL generation, P21 |\n",
      "|  | 0.5 | Correct OS scope |\n",
      "| 2020-12-10 | 0.6 | HW definition updating\n",
      "M80t move to DDR5, F USB move to 1*10G TypeC\n",
      "M90t F USB move to 2*10G TypeA+2*5G TypeA\n",
      "SBB table added in |\n",
      "| 2020-12-15 | 0.7 | Commodity update (Thunderbolt, HDMI2.1 dongle, Smart Fan, HDD Bracket)\n",
      "M70t Q670 PSYG, SS target CQ1 2022\n",
      "NationZ need have 公安部销售许可\n",
      "Removing Secure Wipe from KT M9/M7\n",
      "TFX 230W PSU\n",
      "SW Plan updating |\n",
      "|  |  |  || Date | Version | What’s Changed |\n",
      "| --- | --- | --- |\n",
      "| Date | Version | What’s Changed |\n",
      "| 2021-01-26 | 0.8 | Removing Kensington lock\n",
      "Updating DP to HDMI dongle\n",
      "Updating the format of certification slides\n",
      "M70t gen3 would not support PCIE 4x addon cards\n",
      "TFX230W  TFX260W PSU\n",
      "Anti-Dust Certification requires\n",
      "OEM-MT reserves for M70t |\n",
      "| 2021-03-15 | 0.9 | Roadmap\n",
      "Removing Smart Fan\n",
      "Updating GFX scope\n",
      "Volume for M750t & M757t\n",
      "RAID 0,1 on M90t & M80t & M70t gen3 |\n",
      "| 2021-03-19 | 1.0 | BC version for M750t & M757t\n",
      "Correcting typo of 2.5G Lan for M757t |\n",
      "| 2021-04-01 | 1.1 | Renaming M757t to M800t.\n",
      "Up to RTX3070Ti level GFX. |\n",
      "| 2021-04-12 | 1.2 | SBB WIFI description clarified |\n",
      "| 2021-04-23 | 1.3 | Removing M995t\n",
      "TFX180W PSU added in\n",
      "Euterpe wireless KB/MS removed\n",
      "NAS card, Collaboration card added in\n",
      "GFX scope updated basing on current situation.\n",
      "Volume updated |\n",
      "|  |  |  |\n",
      "|  |  |  |\n",
      "第4页：\n",
      "Marketing Strategy  \n",
      "第5页：\n",
      " Product Launch Plan  | Product | M90t Gen 3 | M80t Gen 3 | M70t Gen 3 | M950t | M800t | M750t |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Product | M90t Gen 3 | M80t Gen 3 | M70t Gen 3 | M950t | M800t | M750t |\n",
      "| Form Factor | TDT 17L | TDT 13.6L | TDT 13.6L | TDT 17L | TDT 17L | TDT 17L |\n",
      "| Segment | Premium Commercial | Mainstream Commercial | Mainstream Commercial | Mainstream Commercial | Mainstream Commercial | Mainstream Commercial |\n",
      "| Target Market | Education\u000bGovernment\u000bLarge Enterprise | Education\u000bGovernment\u000bLarge Enterprise | Education\u000bGovernment\u000bLarge Enterprise | Education\u000bGovernment\u000bLE | Education\u000bGovernment\u000bLE | Education\u000bGovernment\u000bLE |\n",
      "| GEO Availability | WW\n",
      "Global Account | WW\n",
      "Global Account | WW\n",
      "Global Account | PRC | PRC | PRC |\n",
      "| Ship Support | Mar 2022 | Jan 2022 | Jan 2022 | Jan 2022 | Jan 2022 | Nov 2021 |\n",
      "| MFG Plan | LIPC/MTY/HGY/\n",
      "Pondi | LIPC/MTY/HGY/\n",
      "Pondi/ | LIPC/MTY/HGY/\n",
      "Pondi | HYP/CDP | HYP/CDP | HYP/CDP |\n",
      "| Life Cycle | 15 months | 15 months | 15 months | 15 months | 15 months | 15 months |\n",
      "| Operation | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB |\n",
      "| Volume | 39K | 85K | 197K | 140K | 100K | 275K |\n",
      "第6页：\n",
      " Country / Region Scope  | GEO | Scope | Sub-GEO | Detail Country/Region list |\n",
      "| --- | --- | --- | --- |\n",
      "| GEO | Scope | Sub-GEO | Detail Country/Region list |\n",
      "| AP | V | INDIA |  |\n",
      "|  | V | ANZ |  |\n",
      "|  | V | CAP |  |\n",
      "|  | V | JPN |  |\n",
      "| PRC | V | PRC |  |\n",
      "|  | V | China |  |\n",
      "| EMEA | V | Central |  |\n",
      "|  | V | North |  |\n",
      "|  | V | South |  |\n",
      "|  | V | UKI |  |\n",
      "|  | V | EET |  |\n",
      "|  | V | RUCIS |  |\n",
      "|  | V | MEA |  |\n",
      "| Americas | V | Brazil |  |\n",
      "|  | V | LAS |  |\n",
      "|  | V | NA |  |\n",
      "第7页：\n",
      "Marketing Requirement  \n",
      "第8页：\n",
      " ThinkCentre Portfolio  Keep M9/M8/M7 serials to enhance product competitiveness  13.6L ADL 65W, vPro Security 4 DIMM DDR5 2 x M.2 Slot for SSD 4 independent Display  17L ADL 125W/ 65w, 10C i9 CPU 4 DIMM DDR5 2 x M.2 Slot for SSD 4 independent Display Modern Standby 13.6L ADL 65W 4 DIMM DDR4 M70 t/s Mainstream  vPro vPro Q670 Q670 Q670 M80 t/s Mainstream, Vpro M90 t/s Premium, Vpro \n",
      "第9页：\n",
      " Configuration Requirement Energy Star White Paper, Web release.  | For all commercial TDT | CPU | PSU | Storage | Memory | ODD | WIFI | OS |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| For all commercial TDT | CPU | PSU | Storage | Memory | ODD | WIFI | OS |\n",
      "| Typical configuration | Intel I5 / AMD Althon220GE/Ryzen5; 在此要求之下，如存在多颗CPU，按照能耗测试结果，使用能耗最低CPU | 通过测试筛选，选出来功耗最低的搭配测试 | 256GB SSD | 8G | TW/SFF加; \n",
      "Tiny不加 | TW/SFF不加; \n",
      "Tiny加 | New Win\n",
      "Win10 Pro |\n",
      "| Best configuration | 按照能耗测试结果，使用能耗最低CPU | 通过测试筛选，选出来功耗最低的搭配测试 | 256G PCIE-SSD (to support C8 enable) | 8G | TW/SFF加; \n",
      "Tiny不加 | TW/SFF不加; \n",
      "Tiny加 | New Win\n",
      "Win10 Pro |\n",
      "| Remarks | card reader 小众部件，不加 |  |  |  |  |  |  |\n",
      "第10页：\n",
      " Sample Requirement  SIT samples reservation for GEO. | Description | Units | Remark |\n",
      "| --- | --- | --- |\n",
      "| Description | Units | Remark |\n",
      "| M90t | 10 | W/package |\n",
      "| M80t | 10 | W/package |\n",
      "| M70t | 10 | W/package |\n",
      "| M950t | 2 | W/package |\n",
      "| M800t, M750t | 2,2 | W/package |\n",
      "第11页：\n",
      " Self-Healing Request   NIST requests protection, detection, and recovery. Backup BIOS in either SPI ROM or storage is acceptable.   In 2022, the available self healing is Intel Boot Guard based RoT (Level 2). It is introduced in M9/M8/M7 as key upselling feature   Backup in storage is the most efficient BIOS self healing solution for PRC Product. No cost or hardware change required         2018 Lenovo Internal. All rights reserved.  2022 Alder Lake  \n",
      "第12页：\n",
      " BIOS Requirement | Feature Set | Feature Details |\n",
      "| --- | --- |\n",
      "| Feature Set | Feature Details |\n",
      "| Common Image/BIOS | Yes, across all the platforms |\n",
      "| Language | Multi-language (including English, French, Chinese, Russian) |\n",
      "| Traditional BIOS Features | Refer to “Traditional BIOS features” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Management Features | Refer to “BIOS Management Features” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Security Features | Refer to “BIOS Security Features” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Application support | Refer to “Application Support” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Family/Series Features | Refer to “M Series Feature” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Processors/Chipset Features | Refer to “Intel CPU & Chipset Features” of “Lenovo Desktop BIOS Common Features List” |Notice:  #1. “Lenovo Desktop BIOS Common Features List” and “Lenovo Desktop BIOS New feature List” are the key and official documentation maintained together by Portfolio MKT BIOS PM and Dev BIOS Team, and rolling update #2. Explanation of feature numbers \tTaking “2.1” in Management for example: \t“2” is tab sequence number in “BIOS Feature List” document, referring to “BIOS Management Features” tab \t“1” is feature number in “Management BIOS Features” tab, referring to “DASH 1.0” Required Optimized on Security for Virtualization  \n",
      "第13页：\n",
      " BIOS & Security Requirement – ThinkCentre FY2122  | New Hardware & BIOS Requirements |\n",
      "| --- |\n",
      "| New Hardware & BIOS Requirements |\n",
      "| Certificate Based Bios Authentication & Management(similar to DFCI & Sure Admin)\n",
      "Self-Healing Platform(FW Resilience 2.0)\n",
      "All FW to be capsule update capable\n",
      "Enhanced EC Capabilities\n",
      "NationZ TPM for PRC\n",
      "BIOS Modification and Event Log\n",
      "Secure Cloud Boot(Https boot)\n",
      "System Deployment Mode(AMD & Intel)\n",
      "Enhance Tamper Protection\n",
      "High performance mode\n",
      "E-privacy screen control\n",
      "Tile HW integration\n",
      "Bluetooth Beaconing Asset Tag\n",
      "FIDO 2.0 supervised bios access\n",
      "UI Improvement - No DOS / Text screens || 2021 Carry Forward Requirements |\n",
      "| --- |\n",
      "| 2021 Carry Forward Requirements |\n",
      "| All Legacy features from 2020/2021\n",
      "ASCII password support\n",
      "Subscription Certificate Storage (AIA)\n",
      "Odometer \n",
      "Secure Boot PEK/KEK key customization\n",
      "Trusted Device Setup support\n",
      "Transparent Supply Chain support \n",
      "Secure core PC/L3 security requirements || Intel vPro Brand Requirements |\n",
      "| --- |\n",
      "| Intel vPro Brand Requirements |\n",
      "| All vPro Brand Requirements including\n",
      "Total memory Encryption*\n",
      "Active Management Technology*\n",
      "Wifi 6E*\n",
      "Trust Execution Technology\n",
      "System Security Report\n",
      "Secure Boot\n",
      "TPM 2.0\n",
      "Modern standby(NB) |*. Indicates implementation update from TGL  \n",
      "第14页：\n",
      " Secured Core PC Requirement For all ThinkCentre M9 models, claim Secured Core PC  \n",
      "第15页：\n",
      "Service MT required         Reference warranty term (not finalized)  |  | R model | ES - R model | ES - T model | Non- ES - R model | Non- ES - T model | OEM-MT |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "|  | R model | ES - R model | ES - T model | Non- ES - R model | Non- ES - T model | OEM-MT |\n",
      "| M90t |  | V | V | V | V |  |\n",
      "| M80t |  | V | V | V | V |  |\n",
      "| M70 t |  | V | V | V | V | V |\n",
      "| M950t, M800t, M750t | V |  |  |  |  |  |\n",
      "第16页：\n",
      " Updated WW Standardized Warranty Terms  | Product Detail | Base Warranty Term\n",
      "(To-Be) | Sales Configurator Default |\n",
      "| --- | --- | --- |\n",
      "| Product Detail | Base Warranty Term\n",
      "(To-Be) | Sales Configurator Default |\n",
      "| ThinkCentre M9xx | Up to 3yr On-site for ALL MTs\n",
      "1 Year On-site option for all MTs | Base Warranty |\n",
      "| ThinkCentre M8xx M7xx | Up to 3yr On-site for ALL MTs\n",
      "1 Year On-site option for all MTs\n",
      "1 Year Courier/Carryin for all MTs  (for EMEA only) | Base Warranty |\n",
      "| ThinkCentre Nano | Up to 3yr On-site for ALL MTs\n",
      "1 Year On-site option for all MTs\n",
      "1 Year Courier/Carryin for all MTs | Base Warranty |\n",
      "| All other ThinkCentre M6xx | Up to 1yr On-site for ALL MTs | Base Warranty |\n",
      "| PRC | 3年尊享 |  |To be updated before BC. \n",
      "第17页：\n",
      "Regulation  | Countries | Item | Mark |\n",
      "| --- | --- | --- |\n",
      "| Countries | Item | Mark |\n",
      "| Germany (Voluntary) | GS |  |\n",
      "| Brazil (Voluntary) | Inmetro | GEO own |\n",
      "| North America Governmental Purchase | EnergyStar | Must have ES config. |\n",
      "| North America Governmental Purchase Program | EPEAT | Must have EPEAT Gold config. |\n",
      "| China Governmental Purchase Program | CECP/CELP | Per ChinaGeo requirement, follow governmental purchase program\u000bfourth per year |\n",
      "| Taiwan Unique requirement | Taiwan GreenMark | GEO own |\n",
      "| Lenovo Requirement | Low Halogen | HDD/SSD/ODD/Chassis/Speaker/Camera/Panel/Memory/AIO GPU/Touch shall meet Low halogen requirement |\n",
      "| Lenovo Requirement | Product Carbon Footprint | All WW-selling product need finish PCF calculation before SS |\n",
      "| Commercial Business opportunity | TCO | All Commercial TDT (ROW models) |\n",
      "| Lenovo Requirement & EPEAT Requirement | Commercial M-series TDT | use 85% PCC in all chassis plastics including HDD tray; \n",
      "use 30% PCC in fan frame;\n",
      "use 85% PCC in fan duct; use 30% PCC in PSU fan; |\n",
      "| Commercial Business opportunity | TUV Ultra Low Noise Certificate\n",
      "TUV Low noise Certificate | All WW-selling Commercial TDT\n",
      "TUV Low Noise Certificate with two levels: ‘Ultra Low Noise’ and ‘Low Noise’, depends on configuration |\n",
      "| Commercial Business opportunity | Anti-Dust Certification | All TDT with Dust Shield supporting. |MKT requires certification items in below table. Mandatory certification list sharing from Cer team is attached for aware. \n",
      "第18页：\n",
      " Software Offering Scope  | Category | Content |  | ROW | PRC | Comments |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| Category | Content |  | ROW | PRC | Comments |\n",
      "| OS | Windows 10 64-bit & Windows October 2021 release Preload and Drivers |  |  |  |  |\n",
      "|  | Windows October 2021 release Pro with Downgrade Facilitation |  |  |  | CPPP reserved for PRC on OEM & Base |\n",
      "|  | Windows October 2021 release Pro |  |  |  | PRC: CS & EN |\n",
      "|  | Windows October 2021 release Home |  |  |  | PRC KT EN only |\n",
      "|  | Windows October 2021 release Home Single Language |  |  |  |  |\n",
      "|  | Windows October 2021 release Home Chinese Language Edition |  |  |  | PM confirm whether ADL project request this SKU on ROW |\n",
      "|  | Windows 10 CMIT Government Edition (GSKU) |  |  |  | V-next, CMIT RTM release TBD. |\n",
      "|  | Windows 10 IoT LTSC |  |  | N/A | Pure OS  Special bid only |\n",
      "| Service | Drivers on web, WU, SCCM and Lenovo Vantage System update function |  |  |  | Web release only for LTSC |\n",
      "|  | Recovery – USB Key/ digital download for Win10 |  |  |  | 3 waves for RoW，W10，Only W10 CS+OKR for PRC |\n",
      "| Preloaded Apps | Lenovo Apps | Commercial Vantage |  |  | As optional in KT |\n",
      "|  |  | ICE |  |  | Merge with Commercial Vantage, Under Evaluation |\n",
      "|  | 3rd party Apps | MS Office Trial (except Japan) |  |  | Jumpstart requirement, Not for JP |\n",
      "|  |  | MS Office JP and Premium JP - royalty $ |  |  | JP only |\n",
      "|  |  | PowerDVD/Power2GO – UWP |  |  |  |\n",
      "|  |  | My Office |  |  | Russia only to meet Russian Law |\n",
      "|  |  | Kaspersky |  |  | Russia only to meet Russian Law |\n",
      "|  |  | Yandex Browser |  |  | Russia only to meet Russian Law |\n",
      "|  | Special Bid Only | Enterprise Ready Enablement |  |  |  |\n",
      "|  |  | Shape the Future |  |  |  |\n",
      "|  |  | LAA |  |  | M9 only, Support TDS Feature |\n",
      "|  |  | L3 Security Support |  |  |  |\n",
      "|  | Optional | EDU |  |  | Web release，flash license into BIOS |\n",
      "|  |  | UEFI OKR |  |  | Preload pre-install, flash license into BIOS |\n",
      "|  |  | Lenovo Remote Management Pro and Basic |  |  |  |\n",
      "| Cloud Apps | NA | Mcafee/Norton/Adobe CTO APPs delivered by Modern preload 1.3 |  | N/A |  |\n",
      "| Other OS | Ubuntu Preloads and Certification - Eng., |  |  |  | Tiny KT follow Tiny ROW Ubuntu preload |\n",
      "|  | Redhat certification |  |  |  |  |\n",
      "|  | NO OS |  |  |  |  |\n",
      "|  | VM DOS |  |  |  |  |\n",
      "第19页：\n",
      " Additional Software Requirement  Note N-2 driver support is a regular requirement from 2019 onwards RAID 0/1 supported when dual HDD or SSD (Spb only) -- Depends on platform capability All OS must support ITC customization |  |  | RoW | PRC |\n",
      "| --- | --- | --- | --- |\n",
      "|  |  | RoW | PRC |\n",
      "| Other OS | Ubuntu Preloads and Certification - Eng., English for EM, CS |  | N/A |\n",
      "|  | Redhat Certification |  | N/A |\n",
      "| OS requirements | Systems can ship with no OS |  | N/A |\n",
      "|  | Windows 10 Enterprise Edition & Windows October 2021  and Windows 10 Enterprise LTSC (RS5 OR ABOVE) are supported by the Win10 drivers |  | N/A |\n",
      "|  | System BCT testing for Win 10 OS refreshes & Windows October 2021 required for 2 years from the last preload OS refresh. |  |  |\n",
      "|  | Linux LVFS support: Deliver all BIOS, EC, ME updates via LVFS (Linux distribution includes Ubuntu, RedHat) |  | N/A |\n",
      "| Driver requirements | SCCM – refresh by preload update cadence |  |  |\n",
      "|  | Driver BCT testing required for 2 years from last sw update (CSUP.txt) |  |  |\n",
      "|  | Driver must be posted on Windows update by ship support. Driver updates and BIOS updates must be submitted to be posted on windows update 14 days after posting to first party web site. |  |  |\n",
      "|  | All drivers must be DCH compliant |  |  |\n",
      "|  | All driver must support current OS with 2 previous releases (release single driver and  SCCM package on webpage) (Pro-Enterprise) |  |  |\n",
      "|  | HW OD will require Linux Vendor Firmware Service (LVFS) Support for BIOS/EC/ME |  | N/A |\n",
      "|  | All SBBs should have drivers compliant to OS scope |  |  |\n",
      "|  | HSA driver package |  |  |\n",
      "第21页：\n",
      " RTX3070Ti Support Estimation in 17L Chassis Bigger size GFX like RTX3070Ti can be assembled in 17L FF via mechanical tooling EC. U Cage: Open a window in front side. GFX bracket: EMI shielding, GFX holder and make up for strength of chassis. Around $0.7 cost adder. Front Bezel: To make room for GFX.  \n",
      "\n",
      "## 问题:\n",
      "请帮我总结一下这个产品文档的内容\n",
      "## 回答：\n",
      "这个产品文档是关于 ThinkCentre Tower FY2122 Intel Alder Lake MRD V1.3的介绍，其中包括以下内容：\n",
      "\n",
      "1. 产品概述：介绍产品的功能、型号、定位、目标市场、产品规格等。\n",
      "2. 硬件要求：包括处理器、主板、内存、显卡、存储设备、操作系统等硬件要求。\n",
      "3. 软件要求：包括操作系统版本、驱动程序、安全软件等软件要求。\n",
      "4. 产品特点：介绍产品特点，如高性能、多功能、易用性等。\n",
      "5. 应用场景：介绍产品的适用场景，如企业、学校、家庭等。\n",
      "6. 市场定位：介绍产品的市场定位，如高端、中端、入门等。\n",
      "7. 产品规格：介绍产品的具体规格，包括尺寸、重量、接口等。\n",
      "8. 外观设计：介绍产品的外观设计，包括外观特点、颜色等。\n",
      "9. 产品手册：提供产品手册的下载链接，包括产品说明、操作指南、用户手册等。\n",
      "\n",
      "这个产品文档是一份详细的产品说明书，旨在帮助用户更好地了解和应用该产品。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "type_doc = 'PPT'\n",
    "query = '请帮我总结一下这个产品文档的内容'\n",
    "prompt_chat = f\"\"\"基于以下{type_doc}的内容，简洁和专业的来回答用户关于此{type_doc}的问题。\n",
    "## {type_doc}内容:\n",
    "{cont}\n",
    "## 问题:\n",
    "{query}\n",
    "## 回答：\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(prompt_chat, return_tensors='pt').to('cuda')\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=6000,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria_ppt])\n",
    "    )\n",
    "print(tokenizer.decode(out[0]))\n",
    "answer = tokenizer.decode(out[0]).split('## 回答：')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这个产品文档是关于 ThinkCentre Tower FY2122 Intel Alder Lake MRD V1.3的介绍，其中包括以下内容：\n",
      "\n",
      "1. 产品概述：介绍产品的功能、型号、定位、目标市场、产品规格等。\n",
      "2. 硬件要求：包括处理器、主板、内存、显卡、存储设备、操作系统等硬件要求。\n",
      "3. 软件要求：包括操作系统版本、驱动程序、安全软件等软件要求。\n",
      "4. 产品特点：介绍产品特点，如高性能、多功能、易用性等。\n",
      "5. 应用场景：介绍产品的适用场景，如企业、学校、家庭等。\n",
      "6. 市场定位：介绍产品的市场定位，如高端、中端、入门等。\n",
      "7. 产品规格：介绍产品的具体规格，包括尺寸、重量、接口等。\n",
      "8. 外观设计：介绍产品的外观设计，包括外观特点、颜色等。\n",
      "9. 产品手册：提供产品手册的下载链接，包括产品说明、操作指南、用户手册等。\n",
      "\n",
      "这个产品文档是一份详细的产品说明书，旨在帮助用户更好地了解和应用该产品。\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_ids\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operation.operation_server as operation\n",
    "opt = eval(f\"operation.Operation3\")('帮我打开任务管理器')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\QA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m opt\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[1;32md:\\LenoMate\\operation\\operation_server.py:121\u001b[0m, in \u001b[0;36mOperation3.fit\u001b[1;34m(self, model, tokenizer)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m,model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,tokenizer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    120\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch_doc_faiss\u001b[39;00m \u001b[39mimport\u001b[39;00m faiss_corpus\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmap\u001b[39;00m \u001b[39mimport\u001b[39;00m name_exe_map\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtool \u001b[39m=\u001b[39m search_tool()\n\u001b[0;32m    123\u001b[0m     args_app \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_parser() \n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这个市场需求文档是关于 ThinkCentre Tower FY2122 的市场推广和产品定位的。它包括以下内容：\n",
      "\n",
      "1. 产品概述：描述了产品的特性、功能、目标市场和竞争对手等信息。\n",
      "2. 市场定位：介绍了市场定位策略，包括目标客户、市场细分、市场定位和品牌策略等。\n",
      "3. 产品定位：描述了产品的定位，包括产品特点、品牌价值和市场定位等。\n",
      "4. 市场推广策略：介绍了市场推广策略，包括市场推广目标、市场推广方式和市场推广内容等。\n",
      "5. 产品包装和品牌形象：描述了产品的包装要求和品牌形象要求等。\n",
      "6. 供应链管理：介绍了供应链管理策略，包括供应商、分销渠道和物流等。\n",
      "7. 市场准入和合规要求：介绍了市场准入和合规要求，包括相关法规、标准和要求等。\n",
      "8. 产品认证和质量保证：介绍了产品认证和质量保证要求，包括认证机构和认证标准等。\n",
      "9. 售后服务和支持：介绍了售后服务和支持要求，包括保修、维修和升级等。\n",
      "10. 其他信息：包括销售渠道、市场调研和竞争对手等。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "type_doc = 'PPT'\n",
    "query = '请帮我详细介绍一下产品的功能、型号、定位、目标市场、产品规格等'\n",
    "prompt_chat = f\"\"\"基于以下{type_doc}的内容，专业的来回答用户关于此{type_doc}的问题。\n",
    "## {type_doc}内容:\n",
    "{cont}\n",
    "## 问题:\n",
    "{query}\n",
    "## 回答：\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(prompt_chat, return_tensors='pt').to('cuda')\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=6000,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria_ppt])\n",
    "    )\n",
    "# print(tokenizer.decode(out[0]))\n",
    "answer = tokenizer.decode(out[0]).split('## 回答：')[1]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "根据PPT内容，我们可以得出以下信息：\n",
      "\n",
      "产品名称：ThinkCentre Tower FY2122\n",
      "\n",
      "产品型号：M90t Gen 3\n",
      "\n",
      "产品定位：\n",
      "\n",
      "* 适用于企业、教育、政府机构等大型用户\n",
      "* 用于数据中心、云计算等高性能环境\n",
      "\n",
      "目标市场：\n",
      "\n",
      "* 大型企业、跨国公司\n",
      "* 政府机构、教育机构\n",
      "\n",
      "产品规格：\n",
      "\n",
      "* 17.3英寸全高清IPS显示屏\n",
      "* Intel Core i7/i9处理器\n",
      "* 8GB/16GB DDR4内存\n",
      "* 256GB/512GB NVMe固态硬盘\n",
      "* 1TB/2TB HDD\n",
      "* 100W/120W独立显卡\n",
      "* ThinkPad专利键盘\n",
      "* 802.11ac WIFI\n",
      "* USB 3.1 Gen 2\n",
      "* HDMI 2.0\n",
      "* RJ45网口\n",
      "* SD卡读卡器\n",
      "* 音频接口\n",
      "* 摄像头\n",
      "\n",
      "根据PPT内容，我们可以看出该产品是一款适用于企业、教育、政府机构等大型用户的高性能数据中心产品，具有强大的计算能力、高可靠性、高扩展性等特点，适用于多种应用场景，如云计算、大数据处理、人工智能等。同时，该产品还具有很多创新性的设计，如可拆卸式硬盘、支持多种扩展卡\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "type_doc = 'PPT'\n",
    "query = '请帮我详细介绍一下产品的功能、型号、定位、目标市场、产品规格等'\n",
    "prompt_chat = f\"\"\"基于以下{type_doc}的内容，专业的来回答用户关于此{type_doc}的问题。\n",
    "## {type_doc}内容:\n",
    "{cont}\n",
    "## 问题:\n",
    "{query}\n",
    "## 回答：\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(prompt_chat, return_tensors='pt').to('cuda')\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=6000,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria_ppt])\n",
    "    )\n",
    "# print(tokenizer.decode(out[0]))\n",
    "answer = tokenizer.decode(out[0]).split('## 回答：')[1]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这个产品是一个由Lenovo公司生产的服务器，型号为M90t。它具有高性能的硬件和软件功能，以满足企业用户的需求。以下是这个产品的一些主要特点：\n",
      "\n",
      "* 强大的硬件：M90t配备了高性能的Intel处理器和NVIDIA显卡，可提供强大的计算能力。\n",
      "* 灵活的软件：M90t支持多种操作系统，包括Windows Server 2016、Red Hat Enterprise Linux 7和Debian Linux 11等。\n",
      "* 安全性：M90t具有多重安全保护，包括硬件保护、软件保护和身份验证。\n",
      "* 可靠性：M90t通过了可靠性测试，确保了其长期的可靠性和稳定性。\n",
      "* 可扩展性：M90t支持多种扩展选项，包括PCIe扩展卡、USB存储设备和网络适配器等。\n",
      "* 易于管理：M90t具有简单的管理界面，可轻松地配置和管理服务器。\n",
      "\n",
      "总之，M90t是一个强大的服务器，可满足企业用户的需求，并提供强大的计算、安全性和可靠性。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "type_doc = 'PPT'\n",
    "query = '根据这个产品需求文档详细介绍一下这个产品'\n",
    "prompt_chat = f\"\"\"基于以下{type_doc}的内容，专业的来回答用户关于此{type_doc}的问题。\n",
    "## {type_doc}内容:\n",
    "{cont}\n",
    "## 问题:\n",
    "{query}\n",
    "## 回答：\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(prompt_chat, return_tensors='pt').to('cuda')\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=6000,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria_ppt])\n",
    "    )\n",
    "# print(tokenizer.decode(out[0]))\n",
    "answer = tokenizer.decode(out[0]).split('## 回答：')[1]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "这个市场需求文档是关于 ThinkCentre Tower FY2122 的产品发布计划和市场推广策略的文档。其中，第1页介绍了产品的型号、规格和版本，第2页详细介绍了产品的市场定位、产品愿景、客户反馈和市场概述，第3页列出了产品的配置要求、硬件定义、配置要求、样本要求、BIOS需求和OS/App要求。第4页到第10页详细介绍了产品的不同方面的详细要求，包括硬件、软件、安全、外观设计和尺寸、颜色、电源、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、电压、功率、尺寸、重量、接口、\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1页：\n",
      " ThinkCentre Tower FY2122 Intel Alder Lake MRD V1.3 \n",
      "第2页：\n",
      "Contents Revision History Executive Summary Marketing Strategy Product Vision Customer Feedback Market Overview Roadmap Product Overview Product Launch Plan Country / Region Scope Volume Call Marketing Requirement Hardware Definition Configuration Requirement Sample Requirement Software Definition BIOS Requirement OS / App Requirement Building Block Accessory Service Package Regulation 2 \n",
      "第3页：\n",
      "Revision History 3 | Date | Version | What’s Changed |\n",
      "| --- | --- | --- |\n",
      "| Date | Version | What’s Changed |\n",
      "| 2020-08-03 | 0.1 | Preliminary release |\n",
      "| 2020-09-10 | 0.2 | Spec update |\n",
      "|  |  | Add Anti Virus/Bacterium bezel |\n",
      "| 2020-09-17 | 0.3 | Add OS scope |\n",
      "|  |  | Update country scope |\n",
      "|  |  | Update M757t spec |\n",
      "|  | 0.4 | Add self-healing feature(HDD backup) for KT , P19 |\n",
      "|  |  | Clarify 17L EOU request, p16 |\n",
      "|  |  | Update New BIOS request for FY22 ADL generation, P21 |\n",
      "|  | 0.5 | Correct OS scope |\n",
      "| 2020-12-10 | 0.6 | HW definition updating\n",
      "M80t move to DDR5, F USB move to 1*10G TypeC\n",
      "M90t F USB move to 2*10G TypeA+2*5G TypeA\n",
      "SBB table added in |\n",
      "| 2020-12-15 | 0.7 | Commodity update (Thunderbolt, HDMI2.1 dongle, Smart Fan, HDD Bracket)\n",
      "M70t Q670 PSYG, SS target CQ1 2022\n",
      "NationZ need have 公安部销售许可\n",
      "Removing Secure Wipe from KT M9/M7\n",
      "TFX 230W PSU\n",
      "SW Plan updating |\n",
      "|  |  |  || Date | Version | What’s Changed |\n",
      "| --- | --- | --- |\n",
      "| Date | Version | What’s Changed |\n",
      "| 2021-01-26 | 0.8 | Removing Kensington lock\n",
      "Updating DP to HDMI dongle\n",
      "Updating the format of certification slides\n",
      "M70t gen3 would not support PCIE 4x addon cards\n",
      "TFX230W  TFX260W PSU\n",
      "Anti-Dust Certification requires\n",
      "OEM-MT reserves for M70t |\n",
      "| 2021-03-15 | 0.9 | Roadmap\n",
      "Removing Smart Fan\n",
      "Updating GFX scope\n",
      "Volume for M750t & M757t\n",
      "RAID 0,1 on M90t & M80t & M70t gen3 |\n",
      "| 2021-03-19 | 1.0 | BC version for M750t & M757t\n",
      "Correcting typo of 2.5G Lan for M757t |\n",
      "| 2021-04-01 | 1.1 | Renaming M757t to M800t.\n",
      "Up to RTX3070Ti level GFX. |\n",
      "| 2021-04-12 | 1.2 | SBB WIFI description clarified |\n",
      "| 2021-04-23 | 1.3 | Removing M995t\n",
      "TFX180W PSU added in\n",
      "Euterpe wireless KB/MS removed\n",
      "NAS card, Collaboration card added in\n",
      "GFX scope updated basing on current situation.\n",
      "Volume updated |\n",
      "|  |  |  |\n",
      "|  |  |  |\n",
      "第4页：\n",
      "Marketing Strategy 4 \n",
      "第5页：\n",
      " ThinkCentre M-Series(ROW) – TWR 9 8 5 7 \n",
      "第6页：\n",
      " ThinkCentre M-Series(PRC) – TWR 9 6 7 \n",
      "第7页：\n",
      " Product Launch Plan 7 | Product | M90t Gen 3 | M80t Gen 3 | M70t Gen 3 | M950t | M800t | M750t |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Product | M90t Gen 3 | M80t Gen 3 | M70t Gen 3 | M950t | M800t | M750t |\n",
      "| Form Factor | TDT 17L | TDT 13.6L | TDT 13.6L | TDT 17L | TDT 17L | TDT 17L |\n",
      "| Segment | Premium Commercial | Mainstream Commercial | Mainstream Commercial | Mainstream Commercial | Mainstream Commercial | Mainstream Commercial |\n",
      "| Target Market | Education\u000bGovernment\u000bLarge Enterprise | Education\u000bGovernment\u000bLarge Enterprise | Education\u000bGovernment\u000bLarge Enterprise | Education\u000bGovernment\u000bLE | Education\u000bGovernment\u000bLE | Education\u000bGovernment\u000bLE |\n",
      "| GEO Availability | WW\n",
      "Global Account | WW\n",
      "Global Account | WW\n",
      "Global Account | PRC | PRC | PRC |\n",
      "| Ship Support | Mar 2022 | Jan 2022 | Jan 2022 | Jan 2022 | Jan 2022 | Nov 2021 |\n",
      "| MFG Plan | LIPC/MTY/HGY/\n",
      "Pondi | LIPC/MTY/HGY/\n",
      "Pondi/ | LIPC/MTY/HGY/\n",
      "Pondi | HYP/CDP | HYP/CDP | HYP/CDP |\n",
      "| Life Cycle | 15 months | 15 months | 15 months | 15 months | 15 months | 15 months |\n",
      "| Operation | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB | MTM, CTO, SpB |\n",
      "| Volume | 39K | 85K | 197K | 140K | 100K | 275K |\n",
      "第8页：\n",
      " Country / Region Scope 8 | GEO | Scope | Sub-GEO | Detail Country/Region list |\n",
      "| --- | --- | --- | --- |\n",
      "| GEO | Scope | Sub-GEO | Detail Country/Region list |\n",
      "| AP | V | INDIA |  |\n",
      "|  | V | ANZ |  |\n",
      "|  | V | CAP |  |\n",
      "|  | V | JPN |  |\n",
      "| PRC | V | PRC |  |\n",
      "|  | V | China |  |\n",
      "| EMEA | V | Central |  |\n",
      "|  | V | North |  |\n",
      "|  | V | South |  |\n",
      "|  | V | UKI |  |\n",
      "|  | V | EET |  |\n",
      "|  | V | RUCIS |  |\n",
      "|  | V | MEA |  |\n",
      "| Americas | V | Brazil |  |\n",
      "|  | V | LAS |  |\n",
      "|  | V | NA |  |\n",
      "第9页：\n",
      "Marketing Requirement 9 \n",
      "第10页：\n",
      " ThinkCentre Portfolio 10 Keep M9/M8/M7 serials to enhance product competitiveness  13.6L ADL 65W, vPro Security 4 DIMM DDR5 2 x M.2 Slot for SSD 4 independent Display  17L ADL 125W/ 65w, 10C i9 CPU 4 DIMM DDR5 2 x M.2 Slot for SSD 4 independent Display Modern Standby 13.6L ADL 65W 4 DIMM DDR4 M70 t/s Mainstream  vPro vPro Q670 Q670 Q670 M80 t/s Mainstream , Vpro M90 t/s Premium , Vpro \n",
      "第11页：\n",
      " HW Definition | Product Name | M90t Gen3 | M80t Gen3 | M70t Gen3 | M950t | M800t | M750t |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Product Name | M90t Gen3 | M80t Gen3 | M70t Gen3 | M950t | M800t | M750t |\n",
      "| Form Factor | TDT 17L | TDT 13.6L | TDT 13.6L | TDT 17L | TDT 17L | TDT 17L |\n",
      "| Platform | Intel Q670 | Intel Q670 | Intel Q670 PSYG (As B660) | Intel Q670 | Intel Q670 PSYG (As B660) | Intel B660 |\n",
      "| vPro | Yes | Yes | No | Yes | No | No |\n",
      "| CPU | ADL-S, Up to I9 (125W / 65W) | ADL-S, Up to I9 (65W) | ADL-S, Up to I9 (65W) | ADL-S, Up to I9 (65W) | ADL-S, Up to I7 (65W) | ADL-S, Up to I7 (65W) |\n",
      "| DIMM | 4 DDR5 UDIMM 4000MHz, Up to 128GB | 4 DDR5 UDIMM 4000MHz, Up to 128GB | 4* DDR4 UDIMM 3200MHz, Up to 128GB | 4* DDR4 UDIMM 3200MHz, Up to 128GB | 4* DDR4 UDIMM 3200MHz, Up to 128GB | 4* DDR4 UDIMM 3200MHz, Up to 128GB |\n",
      "| GFX | UMA | UMA | UMA | UMA | UMA | UMA |\n",
      "|  | Intel DG1 2GB | Intel DG1 2GB | Intel DG1 2GB | Intel DG1 2GB | Intel DG1 2GB | Intel DG1 2GB |\n",
      "|  | GTX1660SP 4GB |  |  | GTX1660SP 4GB | GTX1660SP 4GB | GTX1660SP 4GB |\n",
      "|  | RTX3060 12GB |  |  | RTX3060 12GB |  |  |\n",
      "|  | RTX3070Ti 8GB |  |  |  |  |  |\n",
      "| PSU | ATX 750W 92% | TFX 380W 92% | TFX 380W 92% | ATX 500W 92% | ATX 500W 92% | ATX 500W 92% |\n",
      "|  | ATX 500W 92% | TFX 310W 92% | TFX 310W 92% | ATX 300W 90% | ATX 300W 90% | ATX 300W 90% |\n",
      "|  | ATX 300W 90% | TFX 260W 90% | TFX 260W 90% | ATX 180W 85% | ATX 180W 85% | ATX 180W 85% |\n",
      "|  |  | TFX 180W 85% | TFX 180W 85% |  |  |  |\n",
      "| Front IO | 1* USB3.2 Gen2 Type-C (15W charging) | 1* USB3.2 Gen2 Type-C (15W charging) | 1* USB3.2 Gen1 Type-C (15W charging) | 1* USB3.2 Gen1 Type-C (15W charging) | 1* USB3.2 Gen1 Type-C (15W charging) |  |\n",
      "|  | 2* USB3.2 Gen2 + 2* USB3.2 Gen1 | 2* USB3.2 Gen2 + 2* USB3.2 Gen1 | 2* USB3.2 Gen2 + 2* USB3.2 Gen1 | 2* USB3.2 Gen2 + 2* USB3.2 Gen1 | 2* USB3.2 Gen2 + 2* USB3.2 Gen1 | 2* USB3.2 Gen2 + 4* USB3.2 Gen1 |\n",
      "|  | Optional Card Reader(3 in 1) | Optional Card Reader(3 in 1) | Optional Card Reader(3 in 1) | Optional Card Reader(3 in 1) | Optional Card Reader(3 in 1) | Optional Card Reader(3 in 1) |\n",
      "|  | 1* Audio Combo + 1* Mic | 1* Audio Combo + 1* Mic | 1* Audio Combo + 1* Mic | 1* Audio Combo + 1* Mic | 1* Audio Combo + 1* Mic | 1* Audio Combo + 1* Mic |\n",
      "| Rear IO | 4* USB3.2 Gen1 | 4* USB3.2 Gen1 | 4* USB2.0 | 4* USB3.2 Gen1 | 4* USB2.0 | 4* USB2.0 |\n",
      "|  | 1* Flex IO (DP/HDMI2.0/Type-C/VGA) | 1* Flex IO (DP/HDMI2.0/Type-C/VGA) | 1* Flex IO (DP/HDMI2.0/Type-C/VGA) | 1* VGA | 1* VGA | 1* VGA |\n",
      "|  | 1* DP + 1* DP + 1* HDMI2.0 | 1* DP + 1* DP + 1* HDMI2.0 | 1* DP + 1* DP + 1* HDMI2.0 | 1* DP + 1* HDMI2.0 | 1* DP + 1* HDMI1.4 | 1* DP + 1* HDMI1.4 |\n",
      "|  | Optional 2* Serial | Optional 2* Serial | Optional 2* Serial | Optional 2* Serial | Optional 2* Serial | Optional 2* Serial |\n",
      "|  | 1* LAN (1G) | 1* LAN (1G) | 1* LAN (1G) | 1* LAN (1G) | 1* LAN (2.5G) | 1* LAN (1G) |\n",
      "|  | 1* Audio-out | 1* Audio-out | 1* Audio-out | 1* Audio-out + 1* Audio-in + 1* MIC | 1* Audio-out + 1* Audio-in + 1* MIC | 1* Audio-out + 1* Audio-in + 1* MIC |\n",
      "|  | Optional 2* PS2 | Optional 2* PS2 | Optional 2* PS2 | Optional 2* PS2 | Optional 2* PS2 | Optional 2* PS2 |\n",
      "|  | Optional 1* Parallel | Optional 1* Parallel | Optional 1* Parallel | Optional 1* Parallel | Optional 1* Parallel | Optional 1* Parallel |\n",
      "| Expansion Slot | 1* PCIe x16 Gen4 | 1* PCIe x16 Gen4 | 1* PCIe x16 Gen4 | 1* PCIe x16 Gen4 | 1* PCIe x16 Gen4 | 1* PCIe x16 Gen4 |\n",
      "|  | 1* PCIe x16 Gen4 (x4 link) | 1* PCIe x16 Gen4 (x4 link) | 1* PCIe x1 | 1* PCIe x16 (x4 link) | 1* PCIe x4 (x1 link) | 1* PCIe x1 |\n",
      "|  | 1* PCIe x1 | 1* PCIe x1 | 1* PCIe x1 | 1* PCIe x1 | 1* PCIe x4 (x1 link) | 1* PCIe x1 |\n",
      "|  | 1* PCIe x1 |  |  | 1* PCIe x1 |  | 1* PCI |\n",
      "| M.2 Slot | 2* M.2 SSD Gen4 | 2* M.2 SSD Gen4 | 1* M.2 SSD Gen4 | 2* M.2 SSD Gen4 | 1* M.2 SSD Gen4 | 1* M.2 SSD Gen4 |\n",
      "|  | 1* M.2 WiFi | 1* M.2 WiFi | 1* M.2 WiFi | 1* M.2 WiFi | 1* M.2 WiFi | 1* M.2 WiFi |\n",
      "| SATA | 4 | 4 | 3 | 4 | 3 | 3 |\n",
      "| Internal Bay | 2* 3.5” HDD | 1* 3.5” HDD | 1* 3.5” HDD | 2* 3.5” HDD | 2* 3.5” HDD | 2* 3.5” HDD |\n",
      "|  | 1* 2.5” HDD | 1* 2.5” HDD | 1* 2.5” HDD | 1* 2.5” HDD | 1* 2.5” HDD | 1* 2.5” HDD |\n",
      "| External Bay | 1* Slim ODD | 1* Slim ODD | 1* Slim ODD | 1* Slim ODD | 1* Slim ODD | 1* Slim ODD |\n",
      "| Security | HW TPM2.0 | HW TPM2.0 | HW TPM2.0 | FW TPM2.0 | HW TPM2.0 (NationZ) | FW TPM2.0 |\n",
      "|  | Self-healing Lv2 | Self-healing Lv2 | Self-healing Lv2 |  |  |  |\n",
      "|  | Secure Wipe | Secure Wipe | Secure Wipe |  |  |  |\n",
      "| EOU | Yes ( w/new storage cage) | Yes | Yes | Yes ( w/new storage cage) | Yes ( w/new storage cage) | Yes ( w/new storage cage) |\n",
      "| Smart Power On | Yes | Yes | Yes | Yes | Yes | Yes |\n",
      "| TCO 8.0* | Yes | Yes | Yes |  |  |  |\n",
      "| EPEAT Gold | Yes | Yes | Yes | No | No | No |\n",
      "| Modern Standby | Yes | No | No | No | No | No |\n",
      "| RAID 0,1 | Yes | Yes | Yes |  |  |  |\n",
      "| MIL-STD 810G | Yes | Yes | Yes | No | No | No |\n",
      "第12页：\n",
      " Configuration Requirement Energy Star White Paper, Web release. 12 | For all commercial TDT | CPU | PSU | Storage | Memory | ODD | WIFI | OS |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| For all commercial TDT | CPU | PSU | Storage | Memory | ODD | WIFI | OS |\n",
      "| Typical configuration | Intel I5 / AMD Althon220GE/Ryzen5; 在此要求之下，如存在多颗CPU，按照能耗测试结果，使用能耗最低CPU | 通过测试筛选，选出来功耗最低的搭配测试 | 256GB SSD | 8G | TW/SFF加; \n",
      "Tiny不加 | TW/SFF不加; \n",
      "Tiny加 | New Win\n",
      "Win10 Pro |\n",
      "| Best configuration | 按照能耗测试结果，使用能耗最低CPU | 通过测试筛选，选出来功耗最低的搭配测试 | 256G PCIE-SSD (to support C8 enable) | 8G | TW/SFF加; \n",
      "Tiny不加 | TW/SFF不加; \n",
      "Tiny加 | New Win\n",
      "Win10 Pro |\n",
      "| Remarks | card reader 小众部件，不加 |  |  |  |  |  |  |\n",
      "第13页：\n",
      " Sample Requirement 13 SIT samples reservation for GEO. | Description | Units | Remark |\n",
      "| --- | --- | --- |\n",
      "| Description | Units | Remark |\n",
      "| M90t | 10 | W/package |\n",
      "| M80t | 10 | W/package |\n",
      "| M70t | 10 | W/package |\n",
      "| M950t | 2 | W/package |\n",
      "| M800t, M750t | 2,2 | W/package |\n",
      "第14页：\n",
      " Self-Healing Request 14  NIST requests protection, detection, and recovery. Backup BIOS in either SPI ROM or storage is acceptable.   In 2022, the available self healing is Intel Boot Guard based RoT (Level 2). It is introduced in M9/M8/M7 as key upselling feature   Backup in storage is the most efficient BIOS self healing solution for PRC Product. No cost or hardware change required         2018 Lenovo Internal. All rights reserved. 14 2022 Alder Lake  \n",
      "第15页：\n",
      " BIOS Requirement | Feature Set | Feature Details |\n",
      "| --- | --- |\n",
      "| Feature Set | Feature Details |\n",
      "| Common Image/BIOS | Yes, across all the platforms |\n",
      "| Language | Multi-language (including English, French, Chinese, Russian) |\n",
      "| Traditional BIOS Features | Refer to “Traditional BIOS features” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Management Features | Refer to “BIOS Management Features” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Security Features | Refer to “BIOS Security Features” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Application support | Refer to “Application Support” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Family/Series Features | Refer to “M Series Feature” of “Lenovo Desktop BIOS Common Features List” |\n",
      "| Processors/Chipset Features | Refer to “Intel CPU & Chipset Features” of “Lenovo Desktop BIOS Common Features List” |Notice:  #1. “Lenovo Desktop BIOS Common Features List” and “Lenovo Desktop BIOS New feature List” are the key and official documentation maintained together by Portfolio MKT BIOS PM and Dev BIOS Team, and rolling update #2. Explanation of feature numbers \tTaking “2.1” in Management for example: \t“2” is tab sequence number in “BIOS Feature List” document, referring to “BIOS Management Features” tab \t“1” is feature number in “Management BIOS Features” tab, referring to “DASH 1.0” Required Optimized on Security for Virtualization  \n",
      "第16页：\n",
      " BIOS & Security Requirement – ThinkCentre FY2122 16 | New Hardware & BIOS Requirements |\n",
      "| --- |\n",
      "| New Hardware & BIOS Requirements |\n",
      "| Certificate Based Bios Authentication & Management(similar to DFCI & Sure Admin)\n",
      "Self-Healing Platform(FW Resilience 2.0)\n",
      "All FW to be capsule update capable\n",
      "Enhanced EC Capabilities\n",
      "NationZ TPM for PRC\n",
      "BIOS Modification and Event Log\n",
      "Secure Cloud Boot(Https boot)\n",
      "System Deployment Mode(AMD & Intel)\n",
      "Enhance Tamper Protection\n",
      "High performance mode\n",
      "E-privacy screen control\n",
      "Tile HW integration\n",
      "Bluetooth Beaconing Asset Tag\n",
      "FIDO 2.0 supervised bios access\n",
      "UI Improvement - No DOS / Text screens || 2021 Carry Forward Requirements |\n",
      "| --- |\n",
      "| 2021 Carry Forward Requirements |\n",
      "| All Legacy features from 2020/2021\n",
      "ASCII password support\n",
      "Subscription Certificate Storage (AIA)\n",
      "Odometer \n",
      "Secure Boot PEK/KEK key customization\n",
      "Trusted Device Setup support\n",
      "Transparent Supply Chain support \n",
      "Secure core PC/L3 security requirements || Intel vPro Brand Requirements |\n",
      "| --- |\n",
      "| Intel vPro Brand Requirements |\n",
      "| All vPro Brand Requirements including\n",
      "Total memory Encryption*\n",
      "Active Management Technology*\n",
      "Wifi 6E*\n",
      "Trust Execution Technology\n",
      "System Security Report\n",
      "Secure Boot\n",
      "TPM 2.0\n",
      "Modern standby(NB) |*. Indicates implementation update from TGL  \n",
      "第17页：\n",
      " Secured Core PC Requirement For all ThinkCentre M9 models, claim Secured Core PC 17 \n",
      "第18页：\n",
      "Service MT required         Reference warranty term (not finalized) 18 |  | R model | ES - R model | ES - T model | Non- ES - R model | Non- ES - T model | OEM-MT |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "|  | R model | ES - R model | ES - T model | Non- ES - R model | Non- ES - T model | OEM-MT |\n",
      "| M90t |  | V | V | V | V |  |\n",
      "| M80t |  | V | V | V | V |  |\n",
      "| M70 t |  | V | V | V | V | V |\n",
      "| M950t, M800t, M750t | V |  |  |  |  |  |\n",
      "第19页：\n",
      " Updated WW Standardized Warranty Terms 19 | Product Detail | Base Warranty Term\n",
      "(To-Be) | Sales Configurator Default |\n",
      "| --- | --- | --- |\n",
      "| Product Detail | Base Warranty Term\n",
      "(To-Be) | Sales Configurator Default |\n",
      "| ThinkCentre M9xx | Up to 3yr On-site for ALL MTs\n",
      "1 Year On-site option for all MTs | Base Warranty |\n",
      "| ThinkCentre M8xx M7xx | Up to 3yr On-site for ALL MTs\n",
      "1 Year On-site option for all MTs\n",
      "1 Year Courier/Carryin for all MTs  (for EMEA only) | Base Warranty |\n",
      "| ThinkCentre Nano | Up to 3yr On-site for ALL MTs\n",
      "1 Year On-site option for all MTs\n",
      "1 Year Courier/Carryin for all MTs | Base Warranty |\n",
      "| All other ThinkCentre M6xx | Up to 1yr On-site for ALL MTs | Base Warranty |\n",
      "| PRC | 3年尊享 |  |To be updated before BC. \n",
      "第20页：\n",
      "Regulation 20 | Countries | Item | Mark |\n",
      "| --- | --- | --- |\n",
      "| Countries | Item | Mark |\n",
      "| Germany (Voluntary) | GS |  |\n",
      "| Brazil (Voluntary) | Inmetro | GEO own |\n",
      "| North America Governmental Purchase | EnergyStar | Must have ES config. |\n",
      "| North America Governmental Purchase Program | EPEAT | Must have EPEAT Gold config. |\n",
      "| China Governmental Purchase Program | CECP/CELP | Per ChinaGeo requirement, follow governmental purchase program\u000bfourth per year |\n",
      "| Taiwan Unique requirement | Taiwan GreenMark | GEO own |\n",
      "| Lenovo Requirement | Low Halogen | HDD/SSD/ODD/Chassis/Speaker/Camera/Panel/Memory/AIO GPU/Touch shall meet Low halogen requirement |\n",
      "| Lenovo Requirement | Product Carbon Footprint | All WW-selling product need finish PCF calculation before SS |\n",
      "| Commercial Business opportunity | TCO | All Commercial TDT (ROW models) |\n",
      "| Lenovo Requirement & EPEAT Requirement | Commercial M-series TDT | use 85% PCC in all chassis plastics including HDD tray; \n",
      "use 30% PCC in fan frame;\n",
      "use 85% PCC in fan duct; use 30% PCC in PSU fan; |\n",
      "| Commercial Business opportunity | TUV Ultra Low Noise Certificate\n",
      "TUV Low noise Certificate | All WW-selling Commercial TDT\n",
      "TUV Low Noise Certificate with two levels: ‘Ultra Low Noise’ and ‘Low Noise’, depends on configuration |\n",
      "| Commercial Business opportunity | Anti-Dust Certification | All TDT with Dust Shield supporting. |MKT requires certification items in below table. Mandatory certification list sharing from Cer team is attached for aware. \n",
      "第21页：\n",
      " Software Offering Scope 21 | Category | Content |  | ROW | PRC | Comments |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| Category | Content |  | ROW | PRC | Comments |\n",
      "| OS | Windows 10 64-bit & Windows October 2021 release Preload and Drivers |  |  |  |  |\n",
      "|  | Windows October 2021 release Pro with Downgrade Facilitation |  |  |  | CPPP reserved for PRC on OEM & Base |\n",
      "|  | Windows October 2021 release Pro |  |  |  | PRC: CS & EN |\n",
      "|  | Windows October 2021 release Home |  |  |  | PRC KT EN only |\n",
      "|  | Windows October 2021 release Home Single Language |  |  |  |  |\n",
      "|  | Windows October 2021 release Home Chinese Language Edition |  |  |  | PM confirm whether ADL project request this SKU on ROW |\n",
      "|  | Windows 10 CMIT Government Edition (GSKU) |  |  |  | V-next , CMIT RTM release TBD. |\n",
      "|  | Windows 10 IoT LTSC |  |  | N/A | Pure OS  Special bid only |\n",
      "| Service | Drivers on web, WU, SCCM and Lenovo Vantage System update function |  |  |  | Web release only for LTSC |\n",
      "|  | Recovery – USB Key/ digital download for Win10 |  |  |  | 3 waves for RoW，W10，Only W10 CS+OKR for PRC |\n",
      "| Preloaded Apps | Lenovo Apps | Commercial Vantage |  |  | As optional in KT |\n",
      "|  |  | ICE |  |  | Merge with Commercial Vantage, Under Evaluation |\n",
      "|  | 3rd party Apps | MS Office Trial (except Japan) |  |  | Jumpstart requirement, Not for JP |\n",
      "|  |  | MS Office JP and Premium JP - royalty $ |  |  | JP only |\n",
      "|  |  | PowerDVD/Power2GO – UWP |  |  |  |\n",
      "|  |  | My Office |  |  | Russia only to meet Russian Law |\n",
      "|  |  | Kaspersky |  |  | Russia only to meet Russian Law |\n",
      "|  |  | Yandex Browser |  |  | Russia only to meet Russian Law |\n",
      "|  | Special Bid Only | Enterprise Ready Enablement |  |  |  |\n",
      "|  |  | Shape the Future |  |  |  |\n",
      "|  |  | LAA |  |  | M9 only, Support TDS Feature |\n",
      "|  |  | L3 Security Support |  |  |  |\n",
      "|  | Optional | EDU |  |  | Web release，flash license into BIOS |\n",
      "|  |  | UEFI OKR |  |  | Preload pre-install, flash license into BIOS |\n",
      "|  |  | Lenovo Remote Management Pro and Basic |  |  |  |\n",
      "| Cloud Apps | NA | Mcafee/Norton/Adobe CTO APPs delivered by Modern preload 1.3 |  | N/A |  |\n",
      "| Other OS | Ubuntu Preloads and Certification - Eng., |  |  |  | Tiny KT follow Tiny ROW Ubuntu preload |\n",
      "|  | Redhat certification |  |  |  |  |\n",
      "|  | NO OS |  |  |  |  |\n",
      "|  | VM DOS |  |  |  |  |\n",
      "第22页：\n",
      " Additional Software Requirement 22 Note N-2 driver support is a regular requirement from 2019 onwards RAID 0/1 supported when dual HDD or SSD (Spb only) -- Depends on platform capability All OS must support ITC customization |  |  | RoW | PRC |\n",
      "| --- | --- | --- | --- |\n",
      "|  |  | RoW | PRC |\n",
      "| Other OS | Ubuntu Preloads and Certification - Eng., English for EM, CS |  | N/A |\n",
      "|  | Redhat Certification |  | N/A |\n",
      "| OS requirements | Systems can ship with no OS |  | N/A |\n",
      "|  | Windows 10 Enterprise Edition & Windows October 2021  and Windows 10 Enterprise LTSC (RS5 OR ABOVE) are supported by the Win10 drivers |  | N/A |\n",
      "|  | System BCT testing for Win 10 OS refreshes & Windows October 2021 required for 2 years from the last preload OS refresh. |  |  |\n",
      "|  | Linux LVFS support: Deliver all BIOS, EC, ME updates via LVFS (Linux distribution includes Ubuntu, RedHat) |  | N/A |\n",
      "| Driver requirements | SCCM – refresh by preload update cadence |  |  |\n",
      "|  | Driver BCT testing required for 2 years from last sw update (CSUP.txt) |  |  |\n",
      "|  | Driver must be posted on Windows update by ship support. Driver updates and BIOS updates must be submitted to be posted on windows update 14 days after posting to first party web site. |  |  |\n",
      "|  | All drivers must be DCH compliant |  |  |\n",
      "|  | All driver must support current OS with 2 previous releases (release single driver and  SCCM package on webpage) (Pro-Enterprise) |  |  |\n",
      "|  | HW OD will require Linux Vendor Firmware Service (LVFS) Support for BIOS/EC/ME |  | N/A |\n",
      "|  | All SBBs should have drivers compliant to OS scope |  |  |\n",
      "|  | HSA driver package |  |  |\n",
      "第23页：\n",
      "Commodities 23 \n",
      "第24页：\n",
      " Building Block Offering | CPU | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| CPU | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| Intel ADL-S 125W I5, I7, I9 | V |  |  |  |  |  | MTM, CTO, SPB |\n",
      "| Intel ADL-S 65W I5, I7, I9 | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Intel ADL-S 65W Cel, Pen, I3 |  | V | V | V | V | V | MTM, CTO, SPB || Graphics | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Graphics | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| UMA | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Intel DG1 2GB | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| NV GTX1660SP 6GB | V |  |  | V | V | V | MTM, CTO, SPB |\n",
      "| NV RTX3060 12GB | V |  |  | V |  |  | MTM, CTO, SPB |\n",
      "| NV RTX3070Ti 8GB | V |  |  |  |  |  | SPB || Memory | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Memory | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| 8GB DDR5 UDIMM | V | V |  |  |  |  | MTM, CTO, SPB |\n",
      "| 16GB DDR5 UDIMM | V | V |  |  |  |  | MTM, CTO, SPB |\n",
      "| 32GB DDR5 UDIMM | V | V |  |  |  |  | MTM, CTO, SPB |\n",
      "| 4GB DDR4 3200 UDIMM |  |  | V | V | V | V | MTM, CTO, SPB |\n",
      "| 8GB DDR4 3200 UDIMM |  |  | V | V | V | V | MTM, CTO, SPB |\n",
      "| 16GB DDR4 3200 UDIMM |  |  | V | V | V | V | MTM, CTO, SPB |\n",
      "| 32GB DDR4 3200 UDIMM |  |  | V | V | V | V | MTM, CTO, SPB |\n",
      "第25页：\n",
      " Building Block Offering | WIFI | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| WIFI | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| Intel WIFI6E AX211 2*2ax+BT5.x\n",
      "vPro CNVi | V | V | V | V |  |  | MTM, CTO, SPB |\n",
      "| RTL8852CE WIFI6E 2*2ax+BT5.x | V | V | V | V |  |  | MTM, CTO, SPB |\n",
      "| Intel WIFI6 AX201 2*2ax+BT5.x\n",
      "vPro CNVi | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| RTL8852AE WIFI 6 2*2ax+BT5.1 | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 2x2 AC |  | V | V |  |  |  | MTM, CTO, SPB || ODD | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| ODD | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| SATA Slim ODD DVD-ROM | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| SATA Slim ODD DVD Rambo | V | V | V | V | V | V | MTM, CTO, SPB || PSU | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| PSU | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| ATX 750W 92% | V |  |  |  |  |  | MTM, CTO, SPB |\n",
      "| ATX 500W 92% | V |  |  | V | V | V | MTM, CTO, SPB |\n",
      "| ATX 300W 90% | V |  |  | V | V | V | MTM, CTO, SPB |\n",
      "| ATX 180W 85% |  |  |  | V | V | V | MTM, CTO, SPB |\n",
      "| TFX 380W 92% |  | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| TFX 310W 92% |  | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| TFX 260W 90% |  | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| TFX 180W 85% |  | V | V |  |  |  | MTM, CTO, SPB |\n",
      "第26页：\n",
      " Building Block Offering | HDD | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| HDD | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| 1TB 7200RPM 3.5” SATA | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 2TB 7200RPM 3.5” SATA | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 500GB 7200RPM 2.5” SATA | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| 1TB 7200RPM 2.5” SATA | V | V | V |  |  |  | MTM, CTO, SPB || SSD* | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| SSD* | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| 128GB 2242 Gen3 Value | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 256GB 2280 Gen4 Value | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 512GB 2280 Gen4 Value | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 1TB 2280 Gen4 Value | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 512GB 2280 Gen4 Performance | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 1TB 2280 Gen4 Performance | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| 2TB 2280 Gen4 Performance | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "第27页：\n",
      " Building Block Offering | Keyboard & Mouse | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Keyboard & Mouse | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| USB KB&MS, Calliope, Black | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Wireless KB&MS, Calliope, Black | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| Wireless 抗菌 (New) | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| USB KB, Traditional, Black | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Fingerprint MS | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| PS2 KB&MS |  |  |  | V | V | V | MTM, CTO, SPB |\n",
      "| EOS KB |  |  |  | V | V | V | MTM, CTO, SPB || PCIE Add-On Card | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| PCIE Add-On Card | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| 4* Serial, PCIE x1 | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 2* USB3.2 Gen1 TypeA, PCIE x1 | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 1* 2.5G Lan, PCIE x1 | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 1* 1G Lan, PCIE x1 | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 4* 1G Lan, PCIE x4 | V | V |  | V |  |  | MTM, CTO, SPB |\n",
      "| 2* USB3.2 Gen2x1 TypeC, PCIE x4 | V | V |  | V |  |  | MTM, CTO, SPB |\n",
      "| 1* USB3.2 Gen2x2 TypeC, PCIE x4 | V | V |  | V |  |  | MTM, CTO, SPB |\n",
      "| 1* Thunderbolt 4.0, PCIE x4 | V | V |  |  |  |  | MTM, CTO, SPB |\n",
      "| 1* NAS Card, PCIE x1 | V |  |  | V |  |  | MTM, CTO, SPB |\n",
      "| 1* Collaboration Card, PCIE x1 | V |  |  | V |  |  | MTM, CTO, SPB |\n",
      "第28页：\n",
      " Optional Part Request | IO Adaptor / Dongle | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| IO Adaptor / Dongle | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| VGA B2B Card (Flex IO) | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| HDMI2.0 B2B Card (Flex IO) | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| DP1.4 B2B Card (Flex IO) | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| Type-C B2B Card (Flex IO) | V | V | V |  |  |  | MTM, CTO, SPB |\n",
      "| 2* PS/2 Internal Cable | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 1* Serial Port Internal Cable | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 1* Parallel Port Internal Cable | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 2* USB2.0 Ports Internal Cable | V | V |  | V |  |  | MTM, CTO, SPB |\n",
      "| DP1.4 to dual DP1.4 dongle | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| DP1.4 to HDMI2.1 dongle | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| DP1.2 to DVI dongle | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| DP1.2 to VGA dongle | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| HDMI1.4 to VGA dongle | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| HDMI1.4 to HDMI1.4 1.5M Cable | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "第29页：\n",
      " Building Block Offering | Mechanical Building Block | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Mechanical Building Block | M90t gen3 | M80t gen3 | M70t gen3 | M950t | M800t | M750t | Remark |\n",
      "| Chassis Intrusion Switch | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Internal Speaker | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Smart Cable Clip | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Dust Shield | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Pad Lock | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| Chassis E-lock | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "| 光触媒风扇（后置） |  |  |  | V | V | V | MTM, CTO, SPB |\n",
      "| 2.5“ HDD Bracket | V | V | V | V | V | V | MTM, CTO, SPB |\n",
      "第31页：\n",
      " RTX3070Ti Support Estimation in 17L Chassis Bigger size GFX like RTX3070Ti can be assembled in 17L FF via mechanical tooling EC. U Cage: Open a window in front side. GFX bracket: EMI shielding, GFX holder and make up for strength of chassis. Around $0.7 cost adder. Front Bezel: To make room for GFX.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  unstructured.partition.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 进行事件提醒 (Score: 0.4242)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 0.42422258853912354)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.search(query='今天天气', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Creating Document Embeddings\n",
      "['Noise-aware Speech Separation with Contrastive Learning', 'In this paper, we propose NASS method to imp', 'The decoder acts as an inverse operation of the encoder, w']\n",
      "[INFO]Finishing Saving Indexing to ../data/pdf_search.index\n"
     ]
    }
   ],
   "source": [
    "test.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(document_corpus='./data/app.txt', new_embed=False, document_embed='./data/document_embed.npy', k=1, device='cuda', model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', index_location='./data/pdf_search.index', search=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
