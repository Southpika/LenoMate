{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ModelWhale 是什么"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "def display_answer(query, history = []):\n",
    "    # resp, history = get_knowledge_based_answer(query=query,\n",
    "    #                                            vector_store=vector_store,\n",
    "    #                                            chat_history=history)\n",
    "    display(Markdown(query))\n",
    "display_answer(query=\"ModelWhale 是什么\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 22 15:55:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.24       Driver Version: 528.24       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 18%   32C    P8    14W / 215W |      0MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile('./data/document_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 319/319 [00:00<?, ?B/s] \n",
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Opti7080\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 2.06MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 856/856 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 409M/409M [00:22<00:00, 18.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"shibing624/text2vec-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-4.4399e-04, -2.9735e-01,  8.5790e-01,  ..., -5.2770e-01,\n",
      "         -1.4316e-01, -1.0008e-01],\n",
      "        [ 6.5362e-01, -7.6667e-02,  9.5962e-01,  ..., -6.0122e-01,\n",
      "         -1.6797e-03,  2.1458e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "\n",
    "@nb.vectorize()\n",
    "def nb_vec_sin(a):\n",
    "    return sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25494302  0.35510558  0.65787946 ...  0.23777344  0.89951332\n",
      "  -0.69283581]\n",
      " [-0.21394402 -0.02146462  0.73679789 ...  0.53819302  0.95069866\n",
      "  -0.97943308]\n",
      " [ 0.73773584  0.5728007   0.56698357 ...  0.6696758  -0.91263461\n",
      "  -0.87383107]\n",
      " ...\n",
      " [-0.53538724 -0.62236235  0.46455885 ... -0.72469746 -0.43196779\n",
      "   0.1900131 ]\n",
      " [ 0.30064271 -0.95715459  0.9723309  ...  0.68655798  0.98785573\n",
      "  -0.93550842]\n",
      " [-0.49164463 -0.22650817  0.64530563 ...  0.73041068  0.87517238\n",
      "   0.74054606]]\n"
     ]
    }
   ],
   "source": [
    "from text2vec import SentenceModel, cos_sim\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "a = np.random.randn(100000,768)\n",
    "b = np.random.randn(1,768)\n",
    "start_time = time.time()\n",
    "c = cos_sim(a,b)\n",
    "duration_1 = time.time()-start_time\n",
    "start_time = time.time()\n",
    "print(nb_vec_sin(a))\n",
    "duration_2 = time.time()-start_time\n",
    "\n",
    "a = torch.tensor(a).to('cuda')\n",
    "b = torch.tensor(b).to('cuda')\n",
    "s_time = time.time()\n",
    "c_ = cos_sim(a,b)\n",
    "duration_3 = time.time()-s_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19809770584106445, 0.3446838855743408, 0.0020568370819091797)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_1,duration_2,duration_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91 µs ± 2.31 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "@jit(nopython=True) \n",
    "def t():\n",
    "    x = 0\n",
    "    for i in np.arange(5000):\n",
    "        x += i\n",
    "    return x\n",
    "%timeit t() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515 µs ± 768 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "232 µs ± 2.36 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(a)\n",
    "%timeit nb_vec_sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "from argparse import Namespace\n",
    "model_args = Namespace(do_mlm=None,pooler_type='cls', temp=0.05, mlp_only_train=False,init_embeddings_model=None)\n",
    "tokenizer = AutoTokenizer.from_pretrained('silk-road/luotuo-bert')\n",
    "model = AutoModel.from_pretrained('silk-road/luotuo-bert',trust_remote_code=True,model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "text_left = ['如何更换花呗绑定银行卡']\n",
    "\n",
    "queries = [\n",
    "    '如何更换花呗绑定银行卡',\n",
    "    'A man is eating pasta.',\n",
    "    'Someone in a gorilla costume is playing a set of drums.',\n",
    "    'A cheetah chases prey on across a field.']\n",
    "\n",
    "text_right = [\n",
    "    '花呗更改绑定银行卡',\n",
    "    '我什么时候开通了花呗',\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A man is riding a horse.',\n",
    "    'A woman is playing violin.',\n",
    "    'Two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]\n",
    "text_left = queries\n",
    "inputs = tokenizer(text_left, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_left = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
    "inputs = tokenizer(text_right, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_right = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
    "    \n",
    "cos_sim_matrix = torch.matmul(embeddings_left, embeddings_right.t())\n",
    "cos_sim_matrix /= torch.matmul(torch.norm(embeddings_left, dim=1, keepdim=True), torch.norm(embeddings_right, dim=1, keepdim=True).t())\n",
    "tensor_cpu = cos_sim_matrix.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    '如何更换花呗绑定银行卡',\n",
    "    'A man is eating pasta.',\n",
    "    'Someone in a gorilla costume is playing a set of drums.',\n",
    "    'A cheetah chases prey on across a field.']\n",
    "\n",
    "text_right = [\n",
    "    '花呗更改绑定银行卡',\n",
    "    '我什么时候开通了花呗',\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A man is riding a horse.',\n",
    "    'A woman is playing violin.',\n",
    "    'Two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)iguration_chatglm.py: 100%|██████████| 4.38k/4.38k [00:00<?, ?B/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)/modeling_chatglm.py: 100%|██████████| 59.4k/59.4k [00:00<00:00, 192kB/s]\n",
      "Downloading (…)main/quantization.py: 100%|██████████| 31.0k/31.0k [00:00<00:00, 581kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.89G/3.89G [04:53<00:00, 13.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (word_embeddings): Embedding(130528, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): QuantizedLinear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): QuantizedLinear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): QuantizedLinear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): QuantizedLinear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Load default cpu kernel failed:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Opti7080/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization.py\", line 178, in __init__\n",
      "    kernels = ctypes.cdll.LoadLibrary(kernel_file)\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\ctypes\\__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: [WinError 193] %1 不是有效的 Win32 应用程序。\n",
      "\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息,简洁和专业的来回答用户的问题。\n",
      "\n",
      "##已知内容:\n",
      "本机物理MAC地址是:cc:d9:ac:d8:57:06,\n",
      "本机电脑名是:DESKTOP-JHRQTGU.lenovo.com\n",
      "本机IP是:10.176.130.235\n",
      "本机硬盘序列号是:E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
      "本机CPU序列号是:BFEBFBFF000A0655\n",
      "本机主板序列号是:/41JYJ53/CNFCW0007K00LH/\n",
      "本机bios序列号是:41JYJ53\n",
      "##问题:\n",
      "我的GPU型号是什么\n",
      "##回答: 根据您提供的信息,您的GPU型号是 NVIDIA GeForce GTX 1050 Ti。\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"本机物理MAC地址是：cc:d9:ac:d8:57:06,\n",
    "本机电脑名是：DESKTOP-JHRQTGU.lenovo.com\n",
    "本机IP是：10.176.130.235\n",
    "本机硬盘序列号是：E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
    "本机CPU序列号是：BFEBFBFF000A0655\n",
    "本机主板序列号是：/41JYJ53/CNFCW0007K00LH/\n",
    "本机bios序列号是：41JYJ53\"\"\"\n",
    "inp = '我的GPU型号是什么'\n",
    "input_text = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n",
    "如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。\n",
    "##已知内容:\n",
    "{context}\n",
    "##问题:\n",
    "{inp}\n",
    "##回答：\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=300,\n",
    "        temperature=0,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息,简洁和专业的提取出数字,回答仅需填写一个数字。\n",
      "\n",
      "##已知内容:\n",
      "电脑当前音量为50\n",
      "##问题:\n",
      "请帮我降低音量,请问调整后音量的数字是多少\n",
      "##回答:\n",
      " 将音量降低到50,则调整后的音量为50。\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"电脑当前音量为50\"\"\"\n",
    "inp = '请帮我降低音量'\n",
    "input_text = f\"\"\"基于以下已知信息，简洁和专业的提取出数字，回答仅需填写一个数字。\n",
    "\n",
    "##已知内容:\n",
    "{context}\n",
    "##问题:\n",
    "{inp}，请问调整后音量的数字是多少\n",
    "##回答：\n",
    "\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        temperature=0,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.3\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>:假设你可以直接控制电脑,我将给你两个例子,请按照例子的形式进行回答。\n",
      "例子:\n",
      "已知电脑当前屏幕亮度为30,用户需要“帮我调高屏幕亮度”,则直接回答“由于您现在电脑当前亮度为30,已调至60”\n",
      "已知电脑当前屏幕亮度为50,用户需要“帮我调低屏幕亮度”,则直接回答“由于您现在电脑当前亮度为50,已调至20”\n",
      "问题:已知电脑当前屏幕亮度为80,用户需求“帮我调低屏幕亮度”,请模仿\"“由于您现在电脑当前亮度为X,已调至Y”\"进行回答:\n",
      "<bot>: 由于您现在电脑当前亮度为80,已调至50\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"电脑当前屏幕亮度为80\"\"\"\n",
    "inp = '帮我调低屏幕亮度'\n",
    "input_text = f\"\"\"<用户>：假设你可以直接控制电脑，我将给你两个例子，请按照例子的形式进行回答。\n",
    "例子：\n",
    "已知电脑当前屏幕亮度为30，用户需要“帮我调高屏幕亮度”，则直接回答“由于您现在电脑当前亮度为30，已调至60”\n",
    "已知电脑当前屏幕亮度为50，用户需要“帮我调低屏幕亮度”，则直接回答“由于您现在电脑当前亮度为50，已调至20”\n",
    "问题：已知{context}，用户需求“{inp}”，请模仿\"“由于您现在电脑当前亮度为X，已调至Y”\"进行回答：\n",
    "<bot>:\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.3,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.3\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai_user=pN1vaNEM/JFkEod5DPoEha|2023-06-08T02:24:16.518Z; ARRAffinity=4b55dda02c61fa6825af1a96193ff027e26cd4d1081dd1ee2ed88d88d93d93e9; ARRAffinitySameSite=4b55dda02c61fa6825af1a96193ff027e26cd4d1081dd1ee2ed88d88d93d93e9; .AspNetCore.Session=CfDJ8O%2ByR7SqPqxFlVz8eFNh6JFGsWDGqDT7Lzpzs4ImYU0jrnMFe6eWHIsbE52VYc8FRZV37J6ndDaxwjSxbvzE1fmeq53Mql%2B14DTKglIfKe9XpbajQ9UhJNbNo0v%2FYuluHZ5PoiEKJfG47olCOAp9n9QgGSclIv1Iak3PMKe9BclD; .AspNetCore.CookiesSSWeb=CfDJ8O-yR7SqPqxFlVz8eFNh6JEUTNyTeMBuOFEOcOPMwTcO-l0vrOFk0N9BKHtm_bRVY3W0u88P3I8fkCISyX_O7u15qf6PwF_D61PzBTgXJqHfBxQyHjf6rKXSu6YOduUWvsZG0qc4GNtZ7Jerm9yDkshdbEorp2nLmRpQ8obd91phvpKuqUxVl0avaXLOsBH8tOYY4FU7Zx6FtyNflVe6_-IkXRszZuU-9RLtwLSoKfMPjwMkfpUzUHECpB_puDrx4Hm6a6prhGkGKWifGk68HUZVUA9STh43cJklDVNwfwl6eBvUp8joU8TkaGoF1SHqN63HuOSpYv5gyjjS4Vd3Nls; ai_session=k434v/YYgul1PRSZB9GkaJ|1686191056621|1686192769861'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"请你帮我根据以下instruction的要求提取出关键信息：\n",
    "##instruction：要求格式为：\n",
    "- 是否为电脑操作：\n",
    "- 要求数字（未提及写未知）：\n",
    "##input：\n",
    "调整屏幕分辨率为1920*1080\n",
    "##output:\n",
    "\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.3,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>:请帮我打开微信\n",
      "<ChatGLM-6B>: 很抱歉,作为一个语言模型,我无法直接打开微信应用程序。但是,您可以使用您的设备上的微信应用程序图标来打开它。如果该图标没有打开,您可以尝试在设备的设置中找到微信并打开它。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：请帮我打开微信\n",
    "<ChatGLM-6B>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.1,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39m'\u001b[39m\u001b[39m./operation/exe_location.json\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     hash_map \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(\u001b[39m'\u001b[39;49m\u001b[39m./operation/exe_location.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if os.path.isfile('./operation/exe_location.json'):\n",
    "    hash_map = json.loads('./operation/exe_location.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = {\n",
    "    'QQMusic.exe':'C:/Program Files (x86)\\Tencent\\QQMusic',\n",
    "    'msedge.exe':'C:/Program Files (x86)\\Microsoft\\Edge\\Application'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./operation/exe_location.json', 'w') as write_f:\n",
    "\tjson.dump(hash_map, write_f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QQMusic.exe': 'C:/Program Files (x86)\\\\Tencent\\\\QQMusic', 'msedge.exe': 'C:/Program Files (x86)\\\\Microsoft\\\\Edge\\\\Application'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open('./operation/exe_location.json','r') as load_f:\n",
    "    hash_map = json.load(load_f)\n",
    "    print(hash_map)\n",
    "    print(type(load_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import screen_brightness_control as sbc\n",
    "sbc.get_brightness()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbc.get_brightness()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.set_brightness(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes libbitsandbytes_cuda116.dll\n",
      "function 'cadam32bit_grad_fp32' not found\n",
      "CUDA SETUP: Loading binary c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like THUDM/chatglm-6b-int4 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:1291\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1291\u001b[0m         \u001b[39mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1292\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConnection error, and we cannot find the requested files in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m the disk cache. Please try again or make sure your Internet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m connection is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[39m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel,LoraConfig,get_peft_model\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mTHUDM/chatglm-6b-int4\u001b[39;49m\u001b[39m\"\u001b[39;49m, load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mTHUDM/chatglm-6b-int4\u001b[39m\u001b[39m\"\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39msupports_gradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:444\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[39mif\u001b[39;00m kwargs_copy\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    442\u001b[0m         _ \u001b[39m=\u001b[39m kwargs_copy\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 444\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    445\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    446\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    447\u001b[0m         trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    449\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_copy,\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[0;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    927\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    929\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    576\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    627\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    630\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    631\u001b[0m         configuration_file,\n\u001b[0;32m    632\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    633\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    634\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    635\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    636\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    637\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    638\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    639\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    640\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    641\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    644\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\hub.py:452\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[0;32m    451\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    453\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this file, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory containing a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like THUDM/chatglm-6b-int4 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from peft import PeftModel,LoraConfig,get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM', inference_mode=True,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model, \"./tzh_model/medical_glm\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QQMusic.exe': 'C:/Program Files (x86)\\\\Tencent\\\\QQMusic'}\n"
     ]
    }
   ],
   "source": [
    "from operation.operation import Operation3\n",
    "inps = {'inputs':input('Input: ')}\n",
    "test = Operation3(**inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: Matching Task [-h] [--document-corpus DOCUMENT_CORPUS]\n",
      "                     [--new-embed NEW_EMBED] [--document-embed DOCUMENT_EMBED]\n",
      "                     [--k K] [--device DEVICE] [--model-name MODEL_NAME]\n",
      "                     [--index_location INDEX_LOCATION]\n",
      "Matching Task: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"6374c16d-bbe1-4643-92c1-507310150e28\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\Opti7080\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-21952m58WaRixzUOk.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "test.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.wav\n",
      "02.wav\n",
      "03.wav\n",
      "Untitled-1.ipynb\n",
      "__pycache__\n",
      "data\n",
      "exe_location.json\n",
      "generate_summary.py\n",
      "inference.py\n",
      "langchain-ChatGLM\n",
      "main.py\n",
      "map.py\n",
      "operation\n",
      "operation_idx.csv\n",
      "play.py\n",
      "recognition.py\n",
      "record.py\n",
      "renew_corpus.py\n",
      "requirements.txt\n",
      "result.wav\n",
      "search_doc.py\n",
      "search_doc_faiss.py\n",
      "search_doc_hf.py\n",
      "stop_criterion.py\n",
      "synthesis.py\n",
      "test_langchain.ipynb\n",
      "test_thred.py\n",
      "text2vec.ipynb\n",
      "zh.wav\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_que = model(**inputs)\n",
    "    embeddings_ans = model(**tokenizer(text_right, padding=True, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 22, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_que.last_hidden_state.shape,embeddings_que.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19582476, -0.22542259,  0.5878763 , ...,  0.51889557,\n",
       "        -0.97035754,  0.72239023],\n",
       "       [-0.36700755,  0.29916915,  0.31173083, ...,  1.2112145 ,\n",
       "         0.05619805,  0.93658125],\n",
       "       [-0.9832982 , -0.7832372 , -1.5102569 , ...,  1.4223759 ,\n",
       "        -1.4264588 ,  0.75840706],\n",
       "       [-1.4391133 ,  0.4106356 , -0.12989327, ..., -0.03974489,\n",
       "        -1.0173508 , -0.79071593]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_que.last_hidden_state[:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "device = 'cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")\n",
    "model = AutoModel.from_pretrained(\"GanymedeNil/text2vec-large-chinese\").to(device)\n",
    "\n",
    "def l2_normalization(data):\n",
    "    if data.ndim == 1:\n",
    "        return data / np.linalg.norm(data).reshape(-1,1)\n",
    "    else:\n",
    "        return data/np.linalg.norm(data,axis=1).reshape(-1,1)\n",
    "def cls_pooling(model_output,return_tensors=False):\n",
    "    if not return_tensors:\n",
    "        return l2_normalization(model_output.last_hidden_state[:,0].cpu().numpy())\n",
    "    else:\n",
    "        return l2_normalization(model_output.last_hidden_state[:,0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open('../data/document_corpus.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        corpus.append(line.strip('\\n'))\n",
    "def mean_pooling(model_output, attention_mask, return_tensors=False):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    if not return_tensors:\n",
    "        return output.cpu().numpy()\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "model.eval()\n",
    "input_data = tokenizer(corpus, padding=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    corpus_embeddings = model(**input_data.to(device))\n",
    "# corpus_embeddings = cls_pooling(corpus_embeddings)\n",
    "corpus_embeddings = mean_pooling(corpus_embeddings,attention_mask=input_data['attention_mask'])\n",
    "corpus_embeddings = l2_normalization(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  '帮我记录一下明天上午十点要买菜'\n",
    "input_data = tokenizer(query, padding=True, return_tensors=\"pt\")  \n",
    "model.eval()     \n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**input_data.to('cuda'))\n",
    "    # query_embeddings = cls_pooling(query_embeddings)\n",
    "    query_embeddings = mean_pooling(query_embeddings,attention_mask=input_data['attention_mask'])\n",
    "    feature_search = l2_normalization(query_embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23497748],\n",
       "       [0.2247234 ],\n",
       "       [0.27397212],\n",
       "       [0.2500224 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings@feature_search.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '进行事件提醒', '打开某个软件']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "features = l2_normalization(np.load(\"./data/document_embed.npy\"))\n",
    "dim = features.shape[1]\n",
    "index_ip = faiss.IndexFlatIP(dim)\n",
    "index_ip.add(features)\n",
    "print(index_ip.ntotal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['调节屏幕亮度', '调整屏幕分辨率', '打开软件', '询问显存', '询问GPU占用', '创建记事本备忘录'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::IndexIDMapTemplate<struct faiss::Index>::IndexIDMapTemplate(struct faiss::Index *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIDMap.cpp:32: Error: 'index->ntotal == 0' failed: index must be empty on input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\3782806113.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maux_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIDMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_ip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\faiss\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplacement_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0moriginal_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreferenced_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparameter_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   9120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9121\u001b[1;33m         \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIDMap_swiginit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_IndexIDMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in __cdecl faiss::IndexIDMapTemplate<struct faiss::Index>::IndexIDMapTemplate(struct faiss::Index *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIDMap.cpp:32: Error: 'index->ntotal == 0' failed: index must be empty on input"
     ]
    }
   ],
   "source": [
    "aux_index = faiss.IndexIDMap(index_ip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Loading Model\n",
      "[INFO]Creating Document Embeddings\n",
      "['调节屏幕亮度', '询问显存占用', '进行事件提醒', '打开软件', '询问GPU占用']\n",
      "[INFO]Finishing Saving Indexing\n"
     ]
    }
   ],
   "source": [
    "from search_doc_faiss import faiss_corpus\n",
    "torch.cuda.empty_cache()\n",
    "corpus = faiss_corpus()\n",
    "corpus.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建备忘录进行事项提醒', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01932951,  0.0241784 ,  0.02330983, ...,  0.02350627,\n",
       "        -0.03408533,  0.0198594 ], dtype=float32),\n",
       " array([-0.04149882,  0.04571409, -0.02394647, ..., -0.03245361,\n",
       "         0.04087428, -0.01606109], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.reconstruct(0),index_ip.reconstruct(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03973337,  0.00838783,  0.01816207, ..., -0.00785893,\n",
       "         -0.05274578,  0.00404025]], dtype=float32),\n",
       " array([-0.03973337,  0.00838783,  0.01816207, ..., -0.00785893,\n",
       "        -0.05274578,  0.00404025], dtype=float32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_normalization_1(features[1,:]),l2_normalization(features[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "query = '提醒我明天十点抢电影票'\n",
    "index = faiss.read_index('./data/test.index')\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to(device))\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search = query_embeddings\n",
    "# 检索最相似的topK个特征\n",
    "topK = 5\n",
    "D, I = index.search(feature_search, topK)\n",
    "# 返回的D表示相似度（或者距离）, I表示检索的topK个特征id（索引）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '帮我记录一下明天十点提醒帮我买菜'\n",
    "corpus_1 = \"\"\"进行事件提醒\"\"\"\n",
    "corpus_2 = \"\"\"调节屏幕亮度\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to(device))\n",
    "    feature_search = model(**tokenizer(corpus_1, padding=True, return_tensors=\"pt\").to(device))\n",
    "    feature_search_2 = model(**tokenizer(corpus_2, padding=True, return_tensors=\"pt\").to(device))\n",
    "feature_search = cls_pooling(feature_search)\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search_2 = cls_pooling(feature_search_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42430413]], dtype=float32), array([[0.3600703]], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings@feature_search.T,query_embeddings@feature_search_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1024), (1, 1024))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape,feature_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.38714254, 0.37287977, 0.3670662 , 0.3359719 , 0.30211502]],\n",
       "       dtype=float32),\n",
       " array([[0, 2, 3, 1, 4]], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建备忘录进行事项提醒', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.38714254, 0.3670662 , 0.36462313, 0.3359719 , 0.30211502]],\n",
       "       dtype=float32),\n",
       " array([[0, 3, 2, 1, 4]], dtype=int64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_ip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 保存索引\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m faiss\u001b[39m.\u001b[39mwrite_index(index_ip, \u001b[39m'\u001b[39m\u001b[39m./data/test.index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# 读取索引\u001b[39;00m\n\u001b[0;32m      5\u001b[0m index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mread_index(\u001b[39m'\u001b[39m\u001b[39m./data/test.index\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_ip' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存索引\n",
    "faiss.write_index(index_ip, './data/test.index')\n",
    "\n",
    "# 读取索引\n",
    "index = faiss.read_index('./data/test.index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_pooling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     query_embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer(query, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m query_embeddings \u001b[39m=\u001b[39m cls_pooling(query_embeddings)\n\u001b[0;32m      6\u001b[0m feature_search \u001b[39m=\u001b[39m query_embeddings\n\u001b[0;32m      7\u001b[0m \u001b[39m# 检索最相似的topK个特征\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cls_pooling' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "query = '提醒我明天十点抢电影票'\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to('cuda'))\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search = query_embeddings\n",
    "# 检索最相似的topK个特征\n",
    "topK = 5\n",
    "D, I = index.search(feature_search, topK)\n",
    "# 返回的D表示相似度（或者距离）, I表示检索的topK个特征id（索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x000001A3B59EF900> >"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.7988719 , 0.56085855, 0.4258615 , 0.41730604, 0.3314243 ]],\n",
       "       dtype=float32),\n",
       " array([[1, 5, 3, 0, 2]], dtype=int64))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建记事本备忘录', '调整屏幕分辨率', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38731813"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    " \n",
    " \n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=1)\n",
    "    sigma = np.std(data, axis=1)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "\n",
    "\n",
    "np.sum(l2_normalization(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(l2_normalization(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19582501, -0.2254237 ,  0.5878755 , ...,  0.51889545,\n",
       "        -0.9703573 ,  0.7223894 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[835.5797 , 564.0149 , 397.68317, 339.4082 , 246.25342]],\n",
       "       dtype=float32),\n",
       " array([[0, 1, 4, 3, 2]], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ids = faiss.IndexFlatIP(2048)\n",
    "index_ids = faiss.IndexIDMap(index_ids)\n",
    "\n",
    "# 添加特征，并指定id，注意添加的id类型为int64\n",
    "ids = 20\n",
    "feature_ids = np.random.random((1, 2048)).astype('float32')\n",
    "index_ids.add_with_ids(feature_ids, np.array((ids,)).astype('int64'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x000001D6C6EFF570> >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9465, 0.6756, 0.2798, 0.3837, 0.4044, 0.2968, 0.2742, 0.3308, 0.3206,\n",
      "         0.4471, 0.4081],\n",
      "        [0.3055, 0.2763, 0.7725, 0.6567, 0.3291, 0.7498, 0.2632, 0.4862, 0.5845,\n",
      "         0.4109, 0.4650],\n",
      "        [0.3777, 0.3504, 0.3729, 0.5051, 0.5157, 0.4793, 0.5053, 0.4642, 0.5326,\n",
      "         0.6118, 0.5007],\n",
      "        [0.3411, 0.3380, 0.3204, 0.4363, 0.4000, 0.4164, 0.3400, 0.4188, 0.5530,\n",
      "         0.3807, 0.5776]])\n"
     ]
    }
   ],
   "source": [
    "from text2vec import cos_sim\n",
    "cos_similarity = cos_sim(embeddings_que.last_hidden_state[:,0],embeddings_ans.last_hidden_state[:,0])\n",
    "print(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2vec import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9719, 0.8737, 0.7577, 0.7623, 0.7647, 0.7870, 0.7640, 0.7554, 0.7812,\n",
       "         0.7794, 0.7791],\n",
       "        [0.7596, 0.8073, 0.9713, 0.9700, 0.8659, 0.9233, 0.8840, 0.8633, 0.9166,\n",
       "         0.8967, 0.9084],\n",
       "        [0.7571, 0.7907, 0.8844, 0.8987, 0.8838, 0.9239, 0.9227, 0.8731, 0.9319,\n",
       "         0.9432, 0.9264],\n",
       "        [0.7539, 0.7718, 0.8788, 0.9024, 0.8693, 0.9033, 0.8655, 0.8638, 0.9149,\n",
       "         0.8890, 0.9299]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import screen_brightness_control as sbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbc.get_brightness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.set_brightness(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 8192.0 MB\n",
      "Free memory: 6604.09375 MB\n",
      "Used memory: 1587.90625 MB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(f\"Total memory: {info.total/1024**2} MB\")\n",
    "print(f\"Free memory: {info.free/1024**2} MB\")\n",
    "print(f\"Used memory: {info.used/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 8192.00 MB\n",
      "Free memory: 6604.09 MB\n",
      "Used memory: 1587.91 MB\n"
     ]
    }
   ],
   "source": [
    "# from operation.screen_brightness import operation\n",
    "# opt = operation(50)\n",
    "# opt.fit()\n",
    "from operation.display_graphic import operation\n",
    "opt = operation()\n",
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operation import operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = 0\n",
    "inp = {\"brightness\":100}\n",
    "opt = eval(f\"operation.Operation{selected_idx}\")(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'已调至100%'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_suffix():\n",
    "    return time.strftime(\"%Y%m%d%H%M%S\", time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230529144958'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "time_suffix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LenoMate\\\\20230529145105\\\\.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('LenoMate',time_suffix(),'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary='love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LenoMate_love_20230529145339.txt'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'LenoMate_'+summary+'_'+time_suffix()+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_args_kwargs(arg1, arg2, arg3,arg4=5):\n",
    "    print(\"arg1:\", arg1)\n",
    "    print(\"arg2:\", arg2)\n",
    "    print(\"arg3:\", arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg1: 5\n",
      "arg2: two\n",
      "arg3: 3\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"arg3\": 3, \"arg2\": \"two\", \"arg1\": 5}\n",
    "test_args_kwargs(**kwargs)\n",
    "\n",
    "#result\n",
    "arg1: 5\n",
    "arg2: 'two'\n",
    "arg3: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "default_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "class operation():\n",
    "    def __init__(self,inputs,path=default_path,summary='notebook'):\n",
    "        self.path = path\n",
    "        self.inputs = inputs\n",
    "        self.summary = summary\n",
    "    def fit(self):\n",
    "        file_name = 'LenoMate_'+self.summary+'_'+time_suffix()+'.txt'\n",
    "        file_name = os.path.join(default_path,file_name)\n",
    "        with open(file_name,'w',encoding='utf-8') as f:\n",
    "            f.write(self.inputs)\n",
    "        f.close()\n",
    "inp = {'inputs':'s','summary':'love'}\n",
    "tst = operation(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10', '10']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string=\"\"\" instruction:\n",
    "- 任务: 调整屏幕亮度\n",
    "- 要求数字: 10\n",
    "output:\n",
    "- 将屏幕亮度调低到10\"\"\"\n",
    "re.findall(r\"\\d+\\.?\\d*\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载中...\n",
      "模型加载完成\n",
      "模型输出： 2\n",
      "模型输出： 4\n",
      "模型输出： 6\n",
      "模型输出： 8\n",
      "模型输出： 10\n",
      "模型输出： 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3 (load_and_run_model):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Opti7080\\AppData\\Local\\Temp\\ipykernel_8504\\1912693514.py\", line 19, in load_and_run_model\n",
      "  File \"<string>\", line 0\n",
      "    \n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "\n",
    "# 模拟加载和运行模型的函数\n",
    "def load_and_run_model(input_queue):\n",
    "    # 模型加载过程，可以根据实际情况进行编写\n",
    "    print(\"模型加载中...\")\n",
    "    # 模型加载完成\n",
    "    print(\"模型加载完成\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 从输入队列中获取输入数据\n",
    "            input_data = input_queue.get()\n",
    "            # 在这里进行模型处理\n",
    "            # 模型处理逻辑...\n",
    "            # 将输入数据转换为numpy数组\n",
    "            input_array = eval(input_data)\n",
    "            # 运行模型，这里我们简单地将输入数据乘以2\n",
    "            output_array = input_array * 2\n",
    "            # 将结果转换为字符串\n",
    "            output_data = str(output_array)\n",
    "\n",
    "            # 处理完成后，可以将结果返回给主线程，或者进行其他操作\n",
    "            print(\"模型输出：\", output_data)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "# 创建一个输入队列\n",
    "input_queue = queue.Queue()\n",
    "\n",
    "# 创建一个线程，用于加载和运行模型\n",
    "model_thread = threading.Thread(target=load_and_run_model, args=(input_queue,))\n",
    "# 设置线程为后台线程，使程序可以退出\n",
    "model_thread.daemon = True\n",
    "# 启动线程\n",
    "model_thread.start()\n",
    "\n",
    "while True:\n",
    "    # 接收用户输入\n",
    "    input_data = input(\"请输入数据（输入'exit'退出）：\")\n",
    "    if input_data == \"exit\":\n",
    "        break\n",
    "    # 将输入数据放入输入队列\n",
    "    input_queue.put(input_data)\n",
    "\n",
    "# 退出程序时，清空输入队列并等待模型线程结束\n",
    "input_queue.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['要求', '1111'], ['数字:'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern =  \"数字(?:\\(.+?\\))?:\\s*\"\n",
    "text = '要求数字(没有则写不含):1111'\n",
    "re.split(pattern,text),re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "answer = \"\"\"\n",
    "数字:未知\n",
    "\"\"\"\n",
    "try:\n",
    "    num = int(re.findall(r\"\\d+\\.?\\d*\",answer.split('数字:')[1])[0])\n",
    "except:\n",
    "    num = answer.split('数字:')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m volume_slider\u001b[39m.\u001b[39mpack(pady\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[39m# 运行GUI主循环\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m window\u001b[39m.\u001b[39;49mmainloop()\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\tkinter\\__init__.py:1458\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmainloop\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m   1457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtk\u001b[39m.\u001b[39;49mmainloop(n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from pycaw.pycaw import AudioUtilities, ISimpleAudioVolume\n",
    "\n",
    "# 创建一个Tkinter窗口\n",
    "window = tk.Tk()\n",
    "window.title(\"音量控制\")\n",
    "window.geometry(\"300x100\")\n",
    "\n",
    "# 获取默认的音频会话管理器\n",
    "sessions = AudioUtilities.GetAllSessions()\n",
    "for session in sessions:\n",
    "    volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "    if session.Process and session.Process.name() == \"python.exe\":\n",
    "        volume.SetMasterVolume(0.5, None)  # 初始化音量为50%\n",
    "\n",
    "# 函数：调节音量\n",
    "def set_volume(vol):\n",
    "    for session in sessions:\n",
    "        volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "        if session.Process and session.Process.name() == \"python.exe\":\n",
    "            volume.SetMasterVolume(vol / 100, None)\n",
    "\n",
    "# 创建一个音量控制滑块\n",
    "volume_slider = ttk.Scale(window, from_=0, to=100, orient=\"horizontal\", command=set_volume)\n",
    "volume_slider.set(50)\n",
    "volume_slider.pack(pady=20)\n",
    "\n",
    "# 运行GUI主循环\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "sorted([f for f in os.listdir('../') if re.match('pytorch_model-(\\d+)-of-(\\d+).bin',f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " '.idea',\n",
       " 'audio',\n",
       " 'data',\n",
       " 'lenochat.py',\n",
       " 'main.py',\n",
       " 'maintext.py',\n",
       " 'operation',\n",
       " 'QA.html',\n",
       " 'README.md',\n",
       " 'requirements.txt',\n",
       " 'stop_criterion.py',\n",
       " 'test_file',\n",
       " 'utils',\n",
       " 'web.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get -10.814563751220703\n",
      "get -56.9900016784668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# 获取音量值，0.0代表最大，-65.25代表最小\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)\n",
    "\n",
    "\n",
    "# 设置静音，mute为1代表是静音，为0代表不是静音\n",
    "volume.SetMute(0, None)\n",
    "volume.SetMasterVolumeLevel(-65.25 + 8.26, None)\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)\n",
    "# 设置音量大小为60%\n",
    "# volume.SetMasterVolumeLevel(-7.63, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get -9.064839363098145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get -8.649999618530273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# 获取音量值，0.0代表最大，-65.25代表最小\n",
    "vl = volume.GetMasterVolumeLevel()\n",
    "print('get',vl)\n",
    "\n",
    "\n",
    "# 设置静音，mute为1代表是静音，为0代表不是静音\n",
    "volume.SetMute(0, None)\n",
    "volume.SetMasterVolumeLevel(-8.65, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.649999618530273"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume.GetMasterVolumeLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_sim =  AutoModel.from_pretrained(\"GanymedeNil/text2vec-large-chinese\").to('cuda')\n",
    "tokenizer_sim = AutoTokenizer.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")\n",
    "\n",
    "class vol_ctrl:\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"初始化音量对应\"\"\"   \n",
    "        # 音量对应关系\n",
    "        # 0% --- -65.25\n",
    "        # 55% --- -8.92\n",
    "        # 100% --- 0.0\n",
    "        self.volume_match={0: -65.25, 1: -56.99, 2: -51.67, 3: -47.74, 4: -44.62, 5: -42.03, 6: -39.81, 7: -37.89, 8: -36.17, 9: -34.63, 10: -33.24,\n",
    "            11: -31.96, 12: -30.78, 13: -29.68, 14: -28.66, 15: -27.7, 16: -26.8, 17: -25.95, 18: -25.15, 19: -24.38, 20: -23.65,\n",
    "            21: -22.96, 22: -22.3, 23: -21.66, 24: -21.05, 25: -20.46, 26: -19.9, 27: -19.35, 28: -18.82, 29: -18.32, 30: -17.82,\n",
    "            31: -17.35, 32: -16.88, 33: -16.44, 34: -16.0, 35: -15.58, 36: -15.16, 37: -14.76, 38: -14.37, 39: -13.99, 40: -13.62,\n",
    "            41: -13.26, 42: -12.9, 43: -12.56, 44: -12.22, 45: -11.89, 46: -11.56, 47: -11.24, 48: -10.93, 49: -10.63, 50: -10.33,\n",
    "            51: -10.04, 52: -9.75, 53: -9.47, 54: -9.19, 55: -8.92, 56: -8.65, 57: -8.39, 58: -8.13, 59: -7.88, 60: -7.63,\n",
    "            61: -7.38, 62: -7.14, 63: -6.9, 64: -6.67, 65: -6.44, 66: -6.21, 67: -5.99, 68: -5.76, 69: -5.55, 70: -5.33,\n",
    "            71: -5.12, 72: -4.91, 73: -4.71, 74: -4.5, 75: -4.3, 76: -4.11, 77: -3.91, 78: -3.72, 79: -3.53, 80: -3.34,\n",
    "            81: -3.15, 82: -2.97, 83: -2.79, 84: -2.61, 85: -2.43, 86: -2.26, 87: -2.09, 88: -1.91, 89: -1.75, 90: -1.58,\n",
    "            91: -1.41, 92: -1.25, 93: -1.09, 94: -0.93, 95: -0.77, 96: -0.61, 97: -0.46, 98: -0.3, 99: -0.15, 100: 0.0}\n",
    "        #音量值反对应\n",
    "        self.volume_match_2={-65.25:0, -56.99:1, -51.67:2, -47.74:3, -44.62:4, -42.03:5, -39.81:6,-37.89:7, -36.17:8, -34.63:9, -33.24:10,\n",
    "            -31.96:11, -30.78:12, -29.68:13, -28.66:14, -27.7:15, -26.8:16, -25.95:17, -25.15:18,  -24.38:19, -23.65:20,\n",
    "            -22.96:21, -22.3:22, -21.66:23, -21.05:24, -20.46:25, -19.9:26, -19.35:27,  -18.82:28,  -18.32:29,  -17.82:30,\n",
    "            -17.35:31, -16.88:32, -16.44:33, -16.0:34, -15.58:35,-15.16:36, -14.76:37, -14.37:38,  -13.99:39, -13.62:40,\n",
    "            -13.26:41, -12.9:42, -12.56:43, -12.22:44,-11.89:45, -11.56:46, -11.24:47, -10.93:48,  -10.63:49, -10.33:50,\n",
    "            -10.04:51, -9.75:52, -9.47:53, -9.19:54, -8.92:55, -8.65:56, -8.39:57, -8.13:58,  -7.88:59,  -7.63:60,\n",
    "            -7.38:61, -7.14:62, -6.9:63, -6.67:64, -6.44:65, -6.21:66, -5.99:67,  -5.76:68,  -5.55:69, -5.33:70,\n",
    "            -5.12:71,-4.91:72, -4.71:73, -4.5:74, -4.3:75, -4.11:76,  -3.91:77, -3.72:78,  -3.53:79,  -3.34:80,\n",
    "            -3.15:81, -2.97:82, -2.79:83, -2.61:84, -2.43:85, -2.26:86, -2.09:87,  -1.91:88, -1.75:89, -1.58:90,\n",
    "           -1.41:91, -1.25:92, -1.09:93, -0.93:94, -0.77:95, -0.61:96, -0.46:97, -0.3:98,  -0.15:99,  0.0:100}\n",
    "        self.devices = AudioUtilities.GetSpeakers()\n",
    "        self.interface = self.devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "        self.volume = cast(self.interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "    def mute_all(self,mute=True):\n",
    "        self.mute = mute\n",
    "        if self.mute: self.volume.SetMute(1, None)\n",
    "        else: self.volume.SetMute(0, None)\n",
    "\n",
    "    def alter(self,num):\n",
    "        if num > 0:\n",
    "            self.mute_all(mute = False)\n",
    "        vl = self.volume.GetMasterVolumeLevel()\n",
    "        print('get current volumn',vl)\n",
    "        self.volume.SetMasterVolumeLevel(self.volume_match[int(num)], None)\n",
    "        print('alter to ',num)\n",
    "\n",
    "class Operation5():\n",
    "    def __init__(self,inputs,model_sim,tokenizer_sim) -> None:\n",
    "        \n",
    "        self.vol_ctrl = vol_ctrl()\n",
    "        self.inputs = inputs\n",
    "        self.model_sim = model_sim\n",
    "        self.tokenizer_sim = tokenizer_sim\n",
    "\n",
    "    def fit(self,model,tokenizer):\n",
    "        self.judge()\n",
    "        if not self.selected: self.vol_ctrl.mute_all()\n",
    "        pass\n",
    "\n",
    "    def judge(self):\n",
    "        _mute = '静音，取消静音'\n",
    "        _normal = '调节音量'\n",
    "        corpus = [_mute,_normal]\n",
    "        self.model_sim.eval()\n",
    "        input_data = self.tokenizer_sim(self.inputs, return_tensors=\"pt\")\n",
    "        corp = self.tokenizer_sim(corpus, return_tensors=\"pt\",padding=True)\n",
    "        with torch.no_grad():\n",
    "            corpus_embeddings = self.model_sim(**corp.to('cuda'))\n",
    "            input_embeddings = self.model_sim(**input_data.to('cuda'))\n",
    "        corpus_embeddings = self._pooling(corpus_embeddings,attention_mask=corp['attention_mask'])\n",
    "        input_embeddings = self._pooling(input_embeddings,attention_mask=input_data['attention_mask'])\n",
    "        self.selected = corpus[(corpus_embeddings@input_embeddings.T).argmax(axis=0)[0]]\n",
    "        print(f\"[音量调节功能]音量功能匹配为'{self.selected}'\")\n",
    "\n",
    "    def _pooling(self,model_output,attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return self._l2_normalization(output.cpu().numpy())\n",
    "    \n",
    "    def _l2_normalization(self,data):\n",
    "        if data.ndim == 1:\n",
    "            return data / np.linalg.norm(data).reshape(-1,1)\n",
    "        else:\n",
    "            return data/ np.linalg.norm(data,axis=1).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI bot: 联想集团（Lenovo Group Limited）是一家总部位于中国北京的跨国科技公司，成立于1984年。它在全球范围内提供消费类电子产品，如个人电脑、笔记本电脑、平板电脑、智能手机、服务器、工作站、网络存储设备和智能电视等。联想是全球最大的个人电脑制造商之一，同时也是全球最大的智能手机供应商之一。\n",
      "\n",
      "联想的创始人兼董事局主席是柳传志，他是中国著名的企业家之一。在过去的几十年里，联想通过不断创新和扩大市场份额，逐渐成为全球科技产业的领导者。2005年，联想收购了IBM的个人电脑业务，使其在全球个人电脑市场中占据了重要地位。此次收购使联想的品牌知名度大大提高，同时也为公司在全球市场的发展奠定了基础。\n",
      "\n",
      "近年来，联想还不断扩大其业务领域，涉足包括人工智能、大数据、云计算和物联网等新兴科技领域。联想致力于为客户提供高质量、高性能的产品和解决方案，满足不同客户的需求。\n",
      "\n",
      "总的来说，联想集团是一家具有全球影响力的科技公司，为全球数以亿计的用户提供了优质的科技产品和服务。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://openaipoc-kepler.openai.azure.com/\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_key = '14639a3cad0f4ba38483194f82c6920b'\n",
    "\n",
    "messageshistory = [\n",
    "                    {\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                    {\"role\":\"user\",\"content\":\"请给我介绍一下联想企业\"},\n",
    "                    ]\n",
    "response = openai.ChatCompletion.create(\n",
    "      engine=\"Kepler1\",\n",
    "      messages = messageshistory,\n",
    "      temperature=0.5,\n",
    "      max_tokens=300,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "print('AI bot:',response['choices'][0]['message']['content'])\n",
    "bot = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpu': {'used': 10.5, 'used_list': [3.7, 8.9, 14.6, 6.5, 8.5, 1.8, 5.2, 2.2, 2.2, 4.9, 2.7, 1.2, 2.3, 1.3, 5.0, 2.9, 2.4, 6.1, 6.0, 2.4], 'cpu_count': 1, 'cpu_name': 'Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', 'cpu_core': 10, 'cpu_threads': 20}, 'load': {'one': 0, 'five': 0, 'fifteen': 0, 'max': 40, 'limit': 40, 'safe': 30.0}, 'mem': {'memTotal': 32480, 'memFree': 21799, 'memRealUsed': 10681, 'menUsedPercent': 32.88534911199906}, 'disk': [{'path': 'C:/', 'size': {'total': 508453597184, 'used': 251668832256, 'free': 256784764928, 'percent': 49.5}, 'fstype': 'NTFS', 'inodes': False}], 'network': {'up': 0, 'down': 0, 'upTotal': 83814717, 'downTotal': 390884500, 'downPackets': 466817, 'upPackets': 446254}, 'io': {'write': 0, 'read': 0}, 'boot': {'timestamp': 1687137044.5692391, 'runtime': 90476.80121970177, 'datetime': '2023-06-20 10:18:41'}, 'time': 1687227521.3704588}\n",
      "{'cpu_count': 1, 'cpu_name': 'Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', 'cpu_core': 10, 'cpu_threads': 20}\n",
      "{'cpu': {'used': 10.8, 'used_list': [3.1, 7.1, 67.3, 5.1, 19.4, 4.1, 24.5, 14.3, 9.2, 3.1, 14.3, 0.0, 6.1, 2.0, 6.1, 10.2, 2.0, 10.2, 6.1, 1.0], 'cpu_count': 1, 'cpu_name': 'Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', 'cpu_core': 10, 'cpu_threads': 20}, 'load': {'one': 0, 'five': 0, 'fifteen': 0, 'max': 40, 'limit': 40, 'safe': 30.0}, 'mem': {'memTotal': 32480, 'memFree': 21753, 'memRealUsed': 10727, 'menUsedPercent': 33.027008337317085}, 'disk': [{'path': 'C:/', 'size': {'total': 508453597184, 'used': 251668832256, 'free': 256784764928, 'percent': 49.5}, 'fstype': 'NTFS', 'inodes': False}]}\n",
      "{'up': 5.92, 'down': 1.87, 'upTotal': 83823941, 'downTotal': 390887417, 'downPackets': 466836, 'upPackets': 446278}\n",
      "{'write': 217088, 'read': 4096}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "@name: 系统信息 / SystemInfo\n",
    "@author: PurePeace\n",
    "@time: 2020年8月17日\n",
    "@version: 0.1\n",
    "'''\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import platform\n",
    "import hashlib\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "from cachelib import SimpleCache\n",
    "cache = SimpleCache()\n",
    "\n",
    "\n",
    "UNIX: bool = os.name == 'posix'\n",
    "SYS: str = platform.system()\n",
    "\n",
    "\n",
    "class CpuConstants:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        初始化CPU常量（多平台）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.\n",
    "\n",
    "        '''\n",
    "        self.WMI = None\n",
    "        self.initialed: bool = False\n",
    "        self.cpuList: list = [] # windows only\n",
    "\n",
    "        self.cpuCount: int = 0 # 物理cpu数量\n",
    "        self.cpuCore: int = 0 # cpu物理核心数\n",
    "        self.cpuThreads: int = 0 # cpu逻辑核心数\n",
    "        self.cpuName: str = '' # cpu型号\n",
    "\n",
    "        self.Update(True)\n",
    "\n",
    "\n",
    "    def Update(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        更新cpu数据\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        '''\n",
    "        if UNIX: self.GetCpuConstantsUnix(update)\n",
    "        else: self.GetCpuConstantsWindows(update)\n",
    "\n",
    "        self.initialed: bool = True\n",
    "\n",
    "\n",
    "    @property\n",
    "    def getDict(self) -> Dict[int, str]:\n",
    "        '''\n",
    "        以字典格式获取当前cpu常量\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[int, str]\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if not self.initialed: self.Update()\n",
    "        return {\n",
    "            'cpu_count': self.cpuCount,\n",
    "            'cpu_name': self.cpuName,\n",
    "            'cpu_core': self.cpuCore,\n",
    "            'cpu_threads': self.cpuThreads\n",
    "        }\n",
    "\n",
    "\n",
    "    def GetCpuConstantsUnix(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        获取unix下的cpu信息\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        update : bool, optional\n",
    "            DESCRIPTION. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if update or not self.initialed:\n",
    "            ids: list = re.findall(\"physical id.+\", readFile('/proc/cpuinfo'))\n",
    "\n",
    "            # 物理cpu个数\n",
    "            self.cpuCount: int = len(set(ids))\n",
    "\n",
    "            # cpu型号（名称）\n",
    "            self.cpuName: str = self.getCpuTypeUnix()\n",
    "\n",
    "\n",
    "            self.GetCpuConstantsBoth()\n",
    "\n",
    "\n",
    "    def InitWmi(self) -> None:\n",
    "        '''\n",
    "        初始化wmi（for windows）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        import wmi\n",
    "        self.WMI = wmi.WMI()\n",
    "\n",
    "\n",
    "    def GetCpuConstantsBoth(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        获取多平台共用的cpu信息\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        update : bool, optional\n",
    "            强制更新数据. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if update or not self.initialed:\n",
    "\n",
    "            # cpu逻辑核心数\n",
    "            self.cpuThreads: int = psutil.cpu_count()\n",
    "\n",
    "            # cpu物理核心数\n",
    "            self.cpuCore: int = psutil.cpu_count(logical=False)\n",
    "\n",
    "\n",
    "    def GetCpuConstantsWindows(self, update: bool = False) -> None:\n",
    "        '''\n",
    "        获取windows平台的cpu信息\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        update : bool, optional\n",
    "            强制更新数据. The default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            DESCRIPTION.\n",
    "\n",
    "        '''\n",
    "        if update or not self.initialed:\n",
    "\n",
    "            # 初始化wmi\n",
    "            if self.WMI == None: self.InitWmi()\n",
    "\n",
    "            # cpu列表\n",
    "            self.cpuList: list = self.WMI.Win32_Processor()\n",
    "\n",
    "            # 物理cpu个数\n",
    "            self.cpuCount: int = len(self.cpuList)\n",
    "\n",
    "            # cpu型号（名称）\n",
    "            self.cpuName: str = self.cpuList[0].Name\n",
    "\n",
    "\n",
    "            self.GetCpuConstantsBoth()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def getCpuTypeUnix() -> str:\n",
    "        '''\n",
    "        获取CPU型号（unix）\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            CPU型号.\n",
    "\n",
    "        '''\n",
    "        cpuinfo: str = readFile('/proc/cpuinfo')\n",
    "        rep: str = 'model\\s+name\\s+:\\s+(.+)'\n",
    "        tmp = re.search(rep,cpuinfo,re.I)\n",
    "        cpuType: str = ''\n",
    "        if tmp:\n",
    "            cpuType: str = tmp.groups()[0]\n",
    "        else:\n",
    "            cpuinfo = ExecShellUnix('LANG=\"en_US.UTF-8\" && lscpu')[0]\n",
    "            rep = 'Model\\s+name:\\s+(.+)'\n",
    "            tmp = re.search(rep,cpuinfo,re.I)\n",
    "            if tmp: cpuType = tmp.groups()[0]\n",
    "        return cpuType\n",
    "\n",
    "\n",
    "def GetCpuInfo(interval: int = 1) -> Dict[str, Any]:\n",
    "    '''\n",
    "    获取CPU信息\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    interval : int, optional\n",
    "        DESCRIPTION. The default is 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[float, list, dict]\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "    # cpu总使用率\n",
    "    used: float = psutil.cpu_percent(interval)\n",
    "\n",
    "    # 每个逻辑cpu使用率\n",
    "    usedList: List[float] = psutil.cpu_percent(percpu=True)\n",
    "\n",
    "\n",
    "    return {'used': used, 'used_list': usedList, **cpuConstants.getDict}\n",
    "\n",
    "\n",
    "def readFile(filename: str) -> str:\n",
    "    '''\n",
    "    读取文件内容\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        文件名.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        文件内容.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file: return file.read()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "def GetLoadAverage() -> dict:\n",
    "    '''\n",
    "    获取服务器负载状态（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    try: c: list = os.getloadavg()\n",
    "    except: c: list = [0,0,0]\n",
    "    data: dict = {i: c[idx] for idx, i in enumerate(('one', 'five', 'fifteen'))}\n",
    "    data['max'] = psutil.cpu_count() * 2\n",
    "    data['limit'] = data['max']\n",
    "    data['safe'] = data['max'] * 0.75\n",
    "    return data\n",
    "\n",
    "\n",
    "def GetMemInfo() -> dict:\n",
    "    '''\n",
    "    获取内存信息（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    if UNIX: return GetMemInfoUnix()\n",
    "    return GetMemInfoWindows()\n",
    "\n",
    "\n",
    "def GetMemInfoUnix() -> Dict[str, int]:\n",
    "    '''\n",
    "    获取内存信息（unix）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    mem = psutil.virtual_memory()\n",
    "    memInfo: dict = {\n",
    "        'memTotal': ToSizeInt(mem.total, 'MB'),\n",
    "        'memFree': ToSizeInt(mem.free, 'MB'),\n",
    "        'memBuffers': ToSizeInt(mem.buffers, 'MB'),\n",
    "        'memCached': ToSizeInt(mem.cached, 'MB'),\n",
    "    }\n",
    "    memInfo['memRealUsed'] = \\\n",
    "        memInfo['memTotal'] - \\\n",
    "        memInfo['memFree'] - \\\n",
    "        memInfo['memBuffers'] - \\\n",
    "        memInfo['memCached']\n",
    "\n",
    "    memInfo['memUsedPercent'] = memInfo['memRealUsed'] / memInfo['memTotal'] * 100\n",
    "\n",
    "    return memInfo\n",
    "\n",
    "\n",
    "def GetMemInfoWindows() -> dict:\n",
    "    '''\n",
    "    获取内存信息（windows）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    mem = psutil.virtual_memory()\n",
    "    memInfo: dict = {\n",
    "        'memTotal': ToSizeInt(mem.total, 'MB'),\n",
    "        'memFree': ToSizeInt(mem.free, 'MB'),\n",
    "        'memRealUsed': ToSizeInt(mem.used, 'MB'),\n",
    "        'menUsedPercent': mem.used / mem.total * 100\n",
    "    }\n",
    "\n",
    "    return memInfo\n",
    "\n",
    "\n",
    "def ToSizeInt(byte: int, target: str) -> int:\n",
    "    '''\n",
    "    将字节大小转换为目标单位的大小\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    byte : int\n",
    "        int格式的字节大小（bytes size）\n",
    "    target : str\n",
    "        目标单位，str.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        转换为目标单位后的字节大小.\n",
    "\n",
    "    '''\n",
    "    return int(byte/1024**(('KB','MB','GB','TB').index(target) + 1))\n",
    "\n",
    "\n",
    "def ToSizeString(byte: int) -> str:\n",
    "    '''\n",
    "    获取字节大小字符串\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    byte : int\n",
    "        int格式的字节大小（bytes size）.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        自动转换后的大小字符串，如：6.90 GB.\n",
    "\n",
    "    '''\n",
    "    units: tuple = ('b','KB','MB','GB','TB')\n",
    "    re = lambda: '{:.2f} {}'.format(byte, u)\n",
    "    for u in units:\n",
    "        if byte < 1024: return re()\n",
    "        byte /= 1024\n",
    "    return re()\n",
    "\n",
    "\n",
    "def GetDiskInfo() -> list:\n",
    "    '''\n",
    "    获取磁盘信息（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        列表.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        if UNIX: return GetDiskInfoUnix()\n",
    "        return GetDiskInfoWindows()\n",
    "    except Exception as err:\n",
    "        print('获取磁盘信息异常（unix: {}）：'.format(UNIX), err)\n",
    "        return []\n",
    "\n",
    "\n",
    "def GetDiskInfoWindows() -> list:\n",
    "    '''\n",
    "    获取磁盘信息Windows\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    diskInfo : list\n",
    "        列表.\n",
    "\n",
    "    '''\n",
    "    diskIo: list = psutil.disk_partitions()\n",
    "    diskInfo: list = []\n",
    "    for disk in diskIo:\n",
    "        tmp: dict = {}\n",
    "        try:\n",
    "            tmp['path'] = disk.mountpoint.replace(\"\\\\\",\"/\")\n",
    "            usage = psutil.disk_usage(disk.mountpoint)\n",
    "            tmp['size'] = {\n",
    "                'total': usage.total,\n",
    "                'used': usage.used,\n",
    "                'free': usage.free,\n",
    "                'percent': usage.percent\n",
    "            }\n",
    "            tmp['fstype'] = disk.fstype\n",
    "            tmp['inodes'] = False\n",
    "            diskInfo.append(tmp)\n",
    "        except:\n",
    "            pass\n",
    "    return diskInfo\n",
    "\n",
    "\n",
    "def GetDiskInfoUnix() -> list:\n",
    "     '''\n",
    "    获取硬盘分区信息（unix）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "     temp: list = (\n",
    "         ExecShellUnix(\"df -h -P|grep '/'|grep -v tmpfs\")[0]).split('\\n')\n",
    "     tempInodes: list = (\n",
    "         ExecShellUnix(\"df -i -P|grep '/'|grep -v tmpfs\")[0]).split('\\n')\n",
    "     diskInfo: list = []\n",
    "     n: int = 0\n",
    "     cuts: list = [\n",
    "         '/mnt/cdrom',\n",
    "         '/boot',\n",
    "         '/boot/efi',\n",
    "         '/dev',\n",
    "         '/dev/shm',\n",
    "         '/run/lock',\n",
    "         '/run',\n",
    "         '/run/shm',\n",
    "         '/run/user'\n",
    "     ]\n",
    "     for tmp in temp:\n",
    "         n += 1\n",
    "         try:\n",
    "             inodes: list = tempInodes[n-1].split()\n",
    "             disk: list = tmp.split()\n",
    "             if len(disk) < 5: continue\n",
    "             if disk[1].find('M') != -1: continue\n",
    "             if disk[1].find('K') != -1: continue\n",
    "             if len(disk[5].split('/')) > 10: continue\n",
    "             if disk[5] in cuts: continue\n",
    "             if disk[5].find('docker') != -1: continue\n",
    "             arr = {}\n",
    "             arr['path'] = disk[5]\n",
    "             tmp1 = [disk[1],disk[2],disk[3],disk[4]]\n",
    "             arr['size'] = tmp1\n",
    "             arr['inodes'] = [inodes[1],inodes[2],inodes[3],inodes[4]]\n",
    "             diskInfo.append(arr)\n",
    "         except Exception as ex:\n",
    "             print('信息获取错误：', str(ex))\n",
    "             continue\n",
    "     return diskInfo\n",
    "\n",
    "\n",
    "\n",
    "def md5(strings: str) -> str:\n",
    "    '''\n",
    "    生成md5\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strings : TYPE\n",
    "        要进行hash处理的字符串\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str[32]\n",
    "        hash后的字符串.\n",
    "\n",
    "    '''\n",
    "\n",
    "    m = hashlib.md5()\n",
    "    m.update(strings.encode('utf-8'))\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def GetErrorInfo() -> str:\n",
    "    '''\n",
    "    获取traceback中的错误\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    import traceback\n",
    "    errorMsg = traceback.format_exc()\n",
    "    return errorMsg\n",
    "\n",
    "\n",
    "def ExecShellUnix(cmdstring: str, shell=True):\n",
    "    '''\n",
    "    执行Shell命令（Unix）\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cmdstring : str\n",
    "        DESCRIPTION.\n",
    "    shell : TYPE, optional\n",
    "        DESCRIPTION. The default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : TYPE\n",
    "        DESCRIPTION.\n",
    "    e : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    a: str = ''\n",
    "    e: str = ''\n",
    "    import subprocess,tempfile\n",
    "\n",
    "    try:\n",
    "        rx: str = md5(cmdstring)\n",
    "        succ_f = tempfile.SpooledTemporaryFile(\n",
    "            max_size = 4096,\n",
    "            mode = 'wb+',\n",
    "            suffix = '_succ',\n",
    "            prefix = 'btex_' + rx ,\n",
    "            dir = '/dev/shm'\n",
    "        )\n",
    "        err_f = tempfile.SpooledTemporaryFile(\n",
    "            max_size = 4096,\n",
    "            mode = 'wb+',\n",
    "            suffix = '_err',\n",
    "            prefix = 'btex_' + rx ,\n",
    "            dir = '/dev/shm'\n",
    "        )\n",
    "        sub = subprocess.Popen(\n",
    "            cmdstring,\n",
    "            close_fds = True,\n",
    "            shell = shell,\n",
    "            bufsize = 128,\n",
    "            stdout = succ_f,\n",
    "            stderr = err_f\n",
    "        )\n",
    "        sub.wait()\n",
    "        err_f.seek(0)\n",
    "        succ_f.seek(0)\n",
    "        a = succ_f.read()\n",
    "        e = err_f.read()\n",
    "        if not err_f.closed: err_f.close()\n",
    "        if not succ_f.closed: succ_f.close()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    try:\n",
    "        if type(a) == bytes: a = a.decode('utf-8')\n",
    "        if type(e) == bytes: e = e.decode('utf-8')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "    return a,e\n",
    "\n",
    "\n",
    "def GetNetWork() -> dict:\n",
    "    '''\n",
    "    获取系统网络信息\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    networkIo: list = [0,0,0,0]\n",
    "    cache_timeout: int = 86400\n",
    "    try:\n",
    "        networkIo = psutil.net_io_counters()[:4]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    otime = cache.get(\"otime\")\n",
    "    if not otime:\n",
    "        otime = time.time()\n",
    "        cache.set('up',networkIo[0],cache_timeout)\n",
    "        cache.set('down',networkIo[1],cache_timeout)\n",
    "        cache.set('otime',otime ,cache_timeout)\n",
    "\n",
    "    ntime = time.time()\n",
    "    networkInfo: dict = {'up': 0, 'down': 0}\n",
    "    networkInfo['upTotal']   = networkIo[0]\n",
    "    networkInfo['downTotal'] = networkIo[1]\n",
    "    try:\n",
    "        networkInfo['up'] = round(\n",
    "            float(networkIo[0] - cache.get(\"up\")) / 1024 / (ntime - otime),\n",
    "            2\n",
    "        )\n",
    "        networkInfo['down'] = round(\n",
    "            float(networkIo[1] - cache.get(\"down\")) / 1024 / (ntime -  otime),\n",
    "            2\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    networkInfo['downPackets'] = networkIo[3]\n",
    "    networkInfo['upPackets'] = networkIo[2]\n",
    "\n",
    "    cache.set('up',networkIo[0],cache_timeout)\n",
    "    cache.set('down',networkIo[1],cache_timeout)\n",
    "    cache.set('otime', time.time(),cache_timeout)\n",
    "\n",
    "    return networkInfo\n",
    "\n",
    "\n",
    "def GetSystemInfo() -> dict:\n",
    "    systemInfo: dict = {}\n",
    "    systemInfo['cpu'] = GetCpuInfo()\n",
    "    systemInfo['load'] = GetLoadAverage()\n",
    "    systemInfo['mem'] = GetMemInfo()\n",
    "    systemInfo['disk'] = GetDiskInfo()\n",
    "\n",
    "    return systemInfo\n",
    "\n",
    "\n",
    "\n",
    "def GetIoReadWrite() -> Dict[str, int]:\n",
    "    '''\n",
    "    获取系统IO读写\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    ioDisk = psutil.disk_io_counters()\n",
    "    ioTotal: dict = {}\n",
    "    ioTotal['write'] = GetIoWrite(ioDisk.write_bytes)\n",
    "    ioTotal['read'] = GetIoRead(ioDisk.read_bytes)\n",
    "    return ioTotal\n",
    "\n",
    "\n",
    "def GetIoWrite(ioWrite: int) -> int:\n",
    "    '''\n",
    "    获取IO写\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ioWrite : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    diskWrite: int = 0\n",
    "    oldWrite: int = cache.get('io_write')\n",
    "    if not oldWrite:\n",
    "        cache.set('io_write', ioWrite)\n",
    "        return diskWrite;\n",
    "\n",
    "    oldTime: float = cache.get('io_time')\n",
    "    newTime: float = time.time()\n",
    "    if not oldTime: oldTime = newTime\n",
    "    ioEnd: int = (ioWrite - oldWrite)\n",
    "    timeEnd: float = (time.time() - oldTime)\n",
    "    if ioEnd > 0:\n",
    "        if timeEnd < 1: timeEnd = 1\n",
    "        diskWrite = ioEnd / timeEnd\n",
    "    cache.set('io_write',ioWrite)\n",
    "    cache.set('io_time',newTime)\n",
    "    if diskWrite > 0: return int(diskWrite)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def GetIoRead(ioRead):\n",
    "    '''\n",
    "    读取IO读\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ioRead : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    diskRead: int = 0\n",
    "    oldRead: int = cache.get('io_read')\n",
    "    if not oldRead:\n",
    "        cache.set('io_read',ioRead)\n",
    "        return diskRead;\n",
    "    oldTime: float = cache.get('io_time')\n",
    "    newTime: float = time.time()\n",
    "    if not oldTime: oldTime = newTime\n",
    "    ioEnd: int = (ioRead - oldRead)\n",
    "    timeEnd: float = (time.time() - oldTime)\n",
    "    if ioEnd > 0:\n",
    "        if timeEnd < 1: timeEnd = 1;\n",
    "        diskRead = ioEnd / timeEnd;\n",
    "    cache.set('io_read', ioRead)\n",
    "    if diskRead > 0: return int(diskRead)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def GetRegValue(key: str, subkey: str, value: str) -> Any:\n",
    "    '''\n",
    "    获取系统注册表信息\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : str\n",
    "        类型.\n",
    "    subkey : str\n",
    "        路径.\n",
    "    value : str\n",
    "        key.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value : Any\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    import winreg\n",
    "    key = getattr(winreg, key)\n",
    "    handle = winreg.OpenKey(key, subkey)\n",
    "    (value, type) = winreg.QueryValueEx(handle, value)\n",
    "    return value\n",
    "\n",
    "\n",
    "def GetSystemVersion() -> str:\n",
    "    '''\n",
    "    获取操作系统版本（多平台）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        DESCRIPTIONm.\n",
    "\n",
    "    '''\n",
    "    if UNIX: return GetSystemVersionUnix()\n",
    "    return GetSystemVersionWindows()\n",
    "\n",
    "\n",
    "def GetSystemVersionWindows() -> str:\n",
    "    '''\n",
    "    获取操作系统版本（windows）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        import platform\n",
    "        bit: str = 'x86';\n",
    "        if 'PROGRAMFILES(X86)' in os.environ: bit = 'x64'\n",
    "\n",
    "        def get(key: str):\n",
    "            return GetRegValue(\n",
    "                \"HKEY_LOCAL_MACHINE\",\n",
    "                \"SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion\",\n",
    "                key\n",
    "            )\n",
    "\n",
    "        osName = get('ProductName')\n",
    "        build = get('CurrentBuildNumber')\n",
    "\n",
    "        version: str = '{} (build {}) {} (Py{})'.format(\n",
    "            osName, build, bit, platform.python_version())\n",
    "        return version\n",
    "    except Exception as ex:\n",
    "        print('获取系统版本失败，错误：' + str(ex))\n",
    "        return '未知系统版本.'\n",
    "\n",
    "\n",
    "def GetSystemVersionUnix() -> str:\n",
    "    '''\n",
    "    获取系统版本（unix）\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        系统版本.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        version: str = readFile('/etc/redhat-release')\n",
    "        if not version:\n",
    "            version = readFile(\n",
    "                '/etc/issue'\n",
    "            ).strip().split(\"\\n\")[0].replace('\\\\n','').replace('\\l','').strip()\n",
    "        else:\n",
    "            version = version.replace(\n",
    "                'release ',''\n",
    "            ).replace('Linux','').replace('(Core)','').strip()\n",
    "        v = sys.version_info\n",
    "        return version + '(Py {}.{}.{})'.format(v.major, v.minor, v.micro)\n",
    "    except Exception as err:\n",
    "        print('获取系统版本失败，错误：', err)\n",
    "        return '未知系统版本.'\n",
    "\n",
    "\n",
    "def GetBootTime() -> dict:\n",
    "    '''\n",
    "    获取当前系统启动时间\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    bootTime: float = psutil.boot_time()\n",
    "    return {\n",
    "        'timestamp': bootTime,\n",
    "        'runtime': time.time() - bootTime,\n",
    "        'datetime': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "    }\n",
    "\n",
    "\n",
    "def GetCpuConstants() -> dict:\n",
    "    '''\n",
    "    获取CPU常量信息\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cpuConstants : CpuConstants\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    return cpuConstants.getDict\n",
    "\n",
    "\n",
    "def GetFullSystemData() -> dict:\n",
    "    '''\n",
    "    获取完全的系统信息\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    systemData: dict = {\n",
    "        **GetSystemInfo(),\n",
    "        'network': { **GetNetWork() },\n",
    "        'io': { **GetIoReadWrite() },\n",
    "        'boot': { **GetBootTime() },\n",
    "        'time': time.time()\n",
    "    }\n",
    "    return systemData\n",
    "\n",
    "cpuConstants = CpuConstants()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(GetFullSystemData())\n",
    "    print(GetCpuConstants())\n",
    "    print(GetSystemInfo())\n",
    "    print(GetNetWork())\n",
    "    print(GetIoReadWrite())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mac in c.Win32_NetworkAdapter():\n",
    "    print(\"mac地址\", mac.MACAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "**************************************************\n",
      "['电脑名称: DESKTOP-JHRQTGU', '使 用 者: DESKTOP-JHRQTGU\\\\Opti7080', 'MAC地址: cc:d9:ac:d8:57:06', '使用日期: 1.19.0', '主板型号: 41JYJ53', 'CPU型号: Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz', '内存厂商: 80AD000080AD', '内存型号: HMA82GU6DJR8N-XN', '内存大小: 16.00GB', '内存厂商: 80AD000080AD', '内存型号: HMA82GU6DJR8N-XN', '内存大小: 16.00GB', '显卡名称: NVIDIA GeForce RTX 2070 SUPER', '显卡名称: Intel(R) UHD Graphics 630', 'IP地址: 10.176.130.235']\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import wmi\n",
    "import socket\n",
    "\n",
    "class information:\n",
    "    w = wmi.WMI() # 获取配置信息\n",
    "    list = []\n",
    "\n",
    "class INFO(information):\n",
    "    def __init__(self):\n",
    "        self.info()\n",
    "\n",
    "    # 获取配置信息\n",
    "    def info(self):\n",
    "    \n",
    "        for BIOSs in information.w.Win32_ComputerSystem():\n",
    "            information.list.append(\"电脑名称: %s\" % BIOSs.Caption)\n",
    "            information.list.append(\"使 用 者: %s\" % BIOSs.UserName)\n",
    "            \n",
    "        \n",
    "        address = hex(uuid.getnode())[2:]\n",
    "        address =  \":\".join([address[e:e+2] for e in range(0,11,2)])\n",
    "        information.list.append(\"MAC地址: %s\" % address)\n",
    "        \n",
    "        for BIOS in information.w.Win32_BIOS():\n",
    "            # information.list.append(\"使用日期: %s\" % BIOS.Description)\n",
    "            information.list.append(\"主板型号: %s\" % BIOS.SerialNumber)\n",
    "            \n",
    "        for processor in information.w.Win32_Processor():\n",
    "            information.list.append(\"CPU型号: %s\" % processor.Name.strip())\n",
    "            \n",
    "        for memModule in information.w.Win32_PhysicalMemory():\n",
    "            print('*'*50)\n",
    "            totalMemSize = int(memModule.Capacity)\n",
    "            information.list.append(\"内存厂商: %s\" % memModule.Manufacturer)\n",
    "            information.list.append(\"内存型号: %s\" % memModule.PartNumber.strip())\n",
    "            information.list.append(\"内存大小: %.2fGB\" % (totalMemSize / 1024**3))\n",
    "            \n",
    "        for disk in information.w.Win32_DiskDrive(InterfaceType=\"IDE\"):\n",
    "            \n",
    "            diskSize = int(disk.size)\n",
    "            information.list.append(\"磁盘名称: %s\" % disk.Caption)\n",
    "            information.list.append(\"磁盘大小: %.2fGB\" % (diskSize / 1024**3))\n",
    "            \n",
    "        for xk in information.w.Win32_VideoController():\n",
    "            information.list.append(\"显卡名称: %s\" % xk.name)\n",
    "\n",
    "\n",
    "        addr = socket.gethostbyname(socket.gethostname())\n",
    "        information.list.append(\"IP地址: %s\" % addr)\n",
    "\n",
    "        return '\\n'.join(information.list)\n",
    "\n",
    "infor = information()\n",
    "INFOs = INFO()\n",
    "print(information.list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑名称: DESKTOP-JHRQTGU\n",
      "使 用 者: DESKTOP-JHRQTGU\\Opti7080\n",
      "MAC地址: cc:d9:ac:d8:57:06\n",
      "使用日期: 1.19.0\n",
      "主板型号: 41JYJ53\n",
      "CPU型号: Intel(R) Core(TM) i9-10900K CPU @ 3.70GHz\n",
      "内存厂商: 80AD000080AD\n",
      "内存型号: HMA82GU6DJR8N-XN\n",
      "内存大小: 16.00GB\n",
      "内存厂商: 80AD000080AD\n",
      "内存型号: HMA82GU6DJR8N-XN\n",
      "内存大小: 16.00GB\n",
      "显卡名称: NVIDIA GeForce RTX 2070 SUPER\n",
      "显卡名称: Intel(R) UHD Graphics 630\n",
      "IP地址: 10.176.130.235\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/BAAI--COIG to C:/Users/Opti7080/.cache/huggingface/datasets/BAAI___json/BAAI--COIG-d717e8bd6e2de3e7/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1033.84it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 282.25it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Failed to read file 'C:\\Users\\Opti7080\\.cache\\huggingface\\datasets\\downloads\\f333eaf7bddee100eed8b350de14d70471681ce0d62c310589a8464d46c13a29' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/textbox_q_context) changed from string to array in row 3\n",
      "                                                        \r"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:134\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 134\u001b[0m         dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39ma JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mkwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m     parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m     parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 2022)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:1858\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1857\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m-> 1858\u001b[0m \u001b[39mfor\u001b[39;00m _, table \u001b[39min\u001b[39;00m generator:\n\u001b[0;32m   1859\u001b[0m     \u001b[39mif\u001b[39;00m max_shard_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m writer\u001b[39m.\u001b[39m_num_bytes \u001b[39m>\u001b[39m max_shard_size:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:137\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    136\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to read file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m with error \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    138\u001b[0m \u001b[39m# If possible, parse the file as a list of json objects and exit the loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:113\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     pa_table \u001b[39m=\u001b[39m paj\u001b[39m.\u001b[39;49mread_json(\n\u001b[0;32m    114\u001b[0m         io\u001b[39m.\u001b[39;49mBytesIO(batch), read_options\u001b[39m=\u001b[39;49mpaj\u001b[39m.\u001b[39;49mReadOptions(block_size\u001b[39m=\u001b[39;49mblock_size)\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\pyarrow\\_json.pyx:258\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: JSON parse error: Column(/textbox_q_context) changed from string to array in row 3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mBAAI/COIG\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m#BAAI/COIG  FreedomIntelligence/phoenix-sft-data-v1 silk-road/Wizard-LM-Chinese-instruct-evol\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:985\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[0;32m    983\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split(split_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m    986\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    987\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    988\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    990\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m    992\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:1746\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1744\u001b[0m job_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1745\u001b[0m \u001b[39mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1746\u001b[0m     \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1747\u001b[0m         gen_kwargs\u001b[39m=\u001b[39mgen_kwargs, job_id\u001b[39m=\u001b[39mjob_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1748\u001b[0m     ):\n\u001b[0;32m   1749\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   1750\u001b[0m             result \u001b[39m=\u001b[39m content\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\builder.py:1891\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, SchemaInferenceError) \u001b[39mand\u001b[39;00m e\u001b[39m.\u001b[39m__context__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1890\u001b[0m         e \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m__context__\n\u001b[1;32m-> 1891\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetGenerationError(\u001b[39m\"\u001b[39m\u001b[39mAn error occurred while generating the dataset\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[39myield\u001b[39;00m job_id, \u001b[39mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[39m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"BAAI/COIG\")\n",
    "#BAAI/COIG  FreedomIntelligence/phoenix-sft-data-v1 silk-road/Wizard-LM-Chinese-instruct-evol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'id'],\n",
       "    num_rows: 464510\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[190284] [Type: Instruction] [Lang: multi-lingual] [Dataset: Alpaca-gpt4-post-translation]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][190000]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset['train'][1]['output_zh']:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_list = pd.DataFrame()\n",
    "output_list['instruction'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1635,\n",
       " 'category': 'information_extraction',\n",
       " 'instruction_zh': '根据以下关于瑞典经济的文章，哪个经济部门占据了最大的产出？',\n",
       " 'context_zh': '瑞典是一个出口导向的混合经济体，拥有现代化的分销系统、优秀的内外部通讯和熟练的劳动力。木材、水力和铁矿石构成了一个经济体的资源基础，这个经济体严重依赖对外贸易。瑞典的工程部门占产出和出口的50%。电信、汽车工业和制药工业也非常重要。农业占GDP和就业的2%。军火工业具有高度先进的技术声誉。',\n",
       " 'response': 'According to this passage, the engineering sector accounts for the largest output, generating 50% of output and exports.',\n",
       " 'instruction': 'Based on the following passage regarding the economy of Sweden, what is the economic sector that accounts for the largest output?',\n",
       " 'context': \"Sweden is an export-oriented mixed economy featuring a modern distribution system, excellent internal and external communications, and a skilled labor force. Timber, hydropower and iron ore constitute the resource base of an economy heavily oriented toward foreign trade. Sweden's engineering sector accounts for 50% of output and exports. Telecommunications, the automotive industry and the pharmaceutical industries are also of great importance. Agriculture accounts for 2 percent of GDP and employment. The armaments industry has a technologically highly advanced reputation.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def has_chinese(text):\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fa5]')  # 匹配中文字符的正则表达式\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 示例用法\n",
    "sentence1 = \"shij!\"  # 包含中文字符\n",
    "print(has_chinese(sentence1))  # 输出: True\n",
    "\n",
    "sentence2 = \"Hello, World!\"  # 不包含中文字符\n",
    "print(has_chinese(sentence2))  # 输出: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.66k/1.66k [00:00<00:00, 1.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/silk-road--Wizard-LM-Chinese-instruct-evol to C:/Users/Opti7080/.cache/huggingface/datasets/silk-road___json/silk-road--Wizard-LM-Chinese-instruct-evol-e6be7ef9d3a16fba/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 243M/243M [00:25<00:00, 9.47MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:29<00:00, 29.57s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 83.97it/s]\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Opti7080/.cache/huggingface/datasets/silk-road___json/silk-road--Wizard-LM-Chinese-instruct-evol-e6be7ef9d3a16fba/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 44.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"silk-road/Wizard-LM-Chinese-instruct-evol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messageshistory = [\n",
    "                    {\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information.\"},\n",
    "                    {\"role\":\"user\",\"content\":\"请给我介绍一下联想企业\"},\n",
    "                    ]\n",
    "response = openai.ChatCompletion.create(\n",
    "      engine=\"Kepler1\",\n",
    "      messages = messageshistory,\n",
    "      temperature=0.5,\n",
    "      max_tokens=300,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "print('AI bot:',response['choices'][0]['message']['content'])\n",
    "bot = response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# print(response)\n",
    "# print(response['choices'][0]['message']['content'])\n",
    "\n",
    "while True :\n",
    "    response = openai.ChatCompletion.create(\n",
    "      engine=\"Kepler1\",\n",
    "      messages = messageshistory,\n",
    "      temperature=0.5,\n",
    "      max_tokens=800,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "    print('AI bot:',response['choices'][0]['message']['content'])  # print the reply of the bot.\n",
    "    bot = response['choices'][0]['message']['content']\n",
    "    a = 1\n",
    "\n",
    "    messageshistory.append({\"role\":\"assistant\",\"content\":bot}) # add bot output to history for chatbot to understand and respond\n",
    "    i = input('User: ')\n",
    "    messageshistory.append({\"role\":\"user\",\"content\":i}) # add user input to history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音量功能匹配为调节音量\n"
     ]
    }
   ],
   "source": [
    "opt5 = Operation5('请帮我降低音量，',model_sim,tokenizer_sim)\n",
    "opt5.judge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03121485,  0.00247283, -0.03576345, ..., -0.00866181,\n",
       "          0.02807012,  0.04373301],\n",
       "        [-0.02591462, -0.01915512, -0.04096009, ..., -0.02056012,\n",
       "          0.00361972,  0.01807105]], dtype=float32),\n",
       " array([372], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt5.corpus_embeddings,opt5.input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(opt5.corpus_embeddings@opt5.input_embeddings.T).argmax(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume.SetMute(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume.SetMute(1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'volume' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m volume\u001b[39m.\u001b[39mSetMasterVolumeLevel(\u001b[39m-\u001b[39m\u001b[39m65.25\u001b[39m \u001b[39m+\u001b[39m \u001b[39m8.26\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'volume' is not defined"
     ]
    }
   ],
   "source": [
    "volume.SetMasterVolumeLevel(-65.25 + 8.26, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from pycaw.pycaw import AudioUtilities, ISimpleAudioVolume\n",
    "\n",
    "# 创建一个Tkinter窗口\n",
    "window = tk.Tk()\n",
    "\n",
    "window.title(\"音量控制\")\n",
    "window.geometry(\"300x100\")\n",
    "\n",
    "# 获取默认的音频会话管理器\n",
    "sessions = AudioUtilities.GetAllSessions()\n",
    "for session in sessions:\n",
    "    volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "    if session.Process and session.Process.name() == \"python.exe\":\n",
    "        volume.SetMasterVolume(0.5, None)  # 初始化音量为50%\n",
    "\n",
    "# 函数：调节音量\n",
    "def set_volume(vol):\n",
    "    for session in sessions:\n",
    "        volume = session._ctl.QueryInterface(ISimpleAudioVolume)\n",
    "        if session.Process and session.Process.name() == \"python.exe\":\n",
    "            volume.SetMasterVolume(vol / 100, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('ffmpeg -y -i data/voice1.wav -ac 1 -ar 16000 data/voice.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc:d9:ac:d8:57:06\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def get_mac_address():\n",
    "    mac=uuid.UUID(int = uuid.getnode()).hex[-12:]\n",
    "    return \":\".join([mac[e:e+2] for e in range(0,11,2)])\n",
    "\n",
    "print(get_mac_address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑名: DESKTOP-JHRQTGU.lenovo.com\n",
      "ip地址: 10.176.130.235\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "def get_computer_name_ip():\n",
    "    #获取本机电脑名\n",
    "    name = socket.getfqdn(socket.gethostname())\n",
    "    #获取本机ip\n",
    "    addr = socket.gethostbyname(socket.gethostname())\n",
    "    return name,addr\n",
    "\n",
    "myname,myaddr = get_computer_name_ip()\n",
    "print('电脑名:',myname)\n",
    "print('ip地址:',myaddr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "硬盘序列号 E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
      "CPU序列号 BFEBFBFF000A0655\n",
      "主板序列号 /41JYJ53/CNFCW0007K00LH/\n",
      "mac地址 None\n",
      "mac地址 CC:D9:AC:D8:57:02\n",
      "mac地址 A4:BB:6D:CE:5A:8C\n",
      "mac地址 CC:D9:AC:D8:57:06\n",
      "mac地址 CC:D9:AC:D8:57:03\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 BC:9B:20:52:41:53\n",
      "mac地址 BA:7F:20:52:41:53\n",
      "mac地址 BA:8A:20:52:41:53\n",
      "mac地址 CE:D9:AC:D8:57:02\n",
      "bios序列号 41JYJ53\n"
     ]
    }
   ],
   "source": [
    "import wmi\n",
    "\n",
    "c = wmi.WMI()\n",
    "\n",
    "# # 硬盘序列号\n",
    "for physical_disk in c.Win32_DiskDrive():\n",
    "    print(\"硬盘序列号\", physical_disk.SerialNumber)\n",
    "\n",
    "# CPU序列号\n",
    "for cpu in c.Win32_Processor():\n",
    "    print(\"CPU序列号\", cpu.ProcessorId.strip())\n",
    "\n",
    "# 主板序列号\n",
    "for board_id in c.Win32_BaseBoard():\n",
    "    print(\"主板序列号\", board_id.SerialNumber)\n",
    "\n",
    "# mac地址\n",
    "for mac in c.Win32_NetworkAdapter():\n",
    "    print(\"mac地址\", mac.MACAddress)\n",
    "\n",
    "# bios序列号\n",
    "for bios_id in c.Win32_BIOS():\n",
    "    print(\"bios序列号\", bios_id.SerialNumber.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
