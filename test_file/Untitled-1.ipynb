{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ModelWhale 是什么"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "def display_answer(query, history = []):\n",
    "    # resp, history = get_knowledge_based_answer(query=query,\n",
    "    #                                            vector_store=vector_store,\n",
    "    #                                            chat_history=history)\n",
    "    display(Markdown(query))\n",
    "display_answer(query=\"ModelWhale 是什么\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 22 15:55:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.24       Driver Version: 528.24       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 18%   32C    P8    14W / 215W |      0MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isfile('./data/document_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled-1.ipynb\n",
      "acquire_document.py\n",
      "data\n",
      "generate_summary.py\n",
      "search_doc.py\n",
      "text2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 319/319 [00:00<?, ?B/s] \n",
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Opti7080\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 110k/110k [00:00<00:00, 2.06MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 856/856 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 409M/409M [00:22<00:00, 18.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shibing624/text2vec-base-chinese\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"shibing624/text2vec-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-4.4399e-04, -2.9735e-01,  8.5790e-01,  ..., -5.2770e-01,\n",
      "         -1.4316e-01, -1.0008e-01],\n",
      "        [ 6.5362e-01, -7.6667e-02,  9.5962e-01,  ..., -6.0122e-01,\n",
      "         -1.6797e-03,  2.1458e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Loading Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-23 17:00:46.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtext2vec.sentence_model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m74\u001b[0m - \u001b[34m\u001b[1mUse device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from search_doc import embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<text2vec.sentence_model.SentenceModel at 0x1e8de53e2c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "\n",
    "@nb.vectorize()\n",
    "def nb_vec_sin(a):\n",
    "    return sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(100000,768)\n",
    "b = np.random.randn(1,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2vec import SentenceModel, cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25494302  0.35510558  0.65787946 ...  0.23777344  0.89951332\n",
      "  -0.69283581]\n",
      " [-0.21394402 -0.02146462  0.73679789 ...  0.53819302  0.95069866\n",
      "  -0.97943308]\n",
      " [ 0.73773584  0.5728007   0.56698357 ...  0.6696758  -0.91263461\n",
      "  -0.87383107]\n",
      " ...\n",
      " [-0.53538724 -0.62236235  0.46455885 ... -0.72469746 -0.43196779\n",
      "   0.1900131 ]\n",
      " [ 0.30064271 -0.95715459  0.9723309  ...  0.68655798  0.98785573\n",
      "  -0.93550842]\n",
      " [-0.49164463 -0.22650817  0.64530563 ...  0.73041068  0.87517238\n",
      "   0.74054606]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "a = np.random.randn(100000,768)\n",
    "b = np.random.randn(1,768)\n",
    "start_time = time.time()\n",
    "c = cos_sim(a,b)\n",
    "duration_1 = time.time()-start_time\n",
    "start_time = time.time()\n",
    "print(nb_vec_sin(a))\n",
    "duration_2 = time.time()-start_time\n",
    "\n",
    "a = torch.tensor(a).to('cuda')\n",
    "b = torch.tensor(b).to('cuda')\n",
    "s_time = time.time()\n",
    "c_ = cos_sim(a,b)\n",
    "duration_3 = time.time()-s_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0379],\n",
       "         [-0.0269],\n",
       "         [ 0.0244],\n",
       "         ...,\n",
       "         [ 0.0316],\n",
       "         [ 0.0020],\n",
       "         [-0.0534]], dtype=torch.float64),\n",
       " tensor([[ 0.0379],\n",
       "         [-0.0269],\n",
       "         [ 0.0244],\n",
       "         ...,\n",
       "         [ 0.0316],\n",
       "         [ 0.0020],\n",
       "         [-0.0534]], device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c,c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19809770584106445, 0.3446838855743408, 0.0020568370819091797)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_1,duration_2,duration_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%timeit(np.sin(a))` not found.\n"
     ]
    }
   ],
   "source": [
    "%timeit(np.sin(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91 µs ± 2.31 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import jit\n",
    "@jit(nopython=True) \n",
    "def t():\n",
    "    x = 0\n",
    "    for i in np.arange(5000):\n",
    "        x += i\n",
    "    return x\n",
    "%timeit t() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515 µs ± 768 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "232 µs ± 2.36 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(a)\n",
    "%timeit nb_vec_sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "from argparse import Namespace\n",
    "model_args = Namespace(do_mlm=None,pooler_type='cls', temp=0.05, mlp_only_train=False,init_embeddings_model=None)\n",
    "tokenizer = AutoTokenizer.from_pretrained('silk-road/luotuo-bert')\n",
    "model = AutoModel.from_pretrained('silk-road/luotuo-bert',trust_remote_code=True,model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "text_left = ['如何更换花呗绑定银行卡']\n",
    "\n",
    "queries = [\n",
    "    '如何更换花呗绑定银行卡',\n",
    "    'A man is eating pasta.',\n",
    "    'Someone in a gorilla costume is playing a set of drums.',\n",
    "    'A cheetah chases prey on across a field.']\n",
    "\n",
    "text_right = [\n",
    "    '花呗更改绑定银行卡',\n",
    "    '我什么时候开通了花呗',\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A man is riding a horse.',\n",
    "    'A woman is playing violin.',\n",
    "    'Two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]\n",
    "text_left = queries\n",
    "inputs = tokenizer(text_left, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_left = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
    "inputs = tokenizer(text_right, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_right = model(**inputs, output_hidden_states=True, return_dict=True, sent_emb=True).pooler_output\n",
    "    \n",
    "cos_sim_matrix = torch.matmul(embeddings_left, embeddings_right.t())\n",
    "cos_sim_matrix /= torch.matmul(torch.norm(embeddings_left, dim=1, keepdim=True), torch.norm(embeddings_right, dim=1, keepdim=True).t())\n",
    "tensor_cpu = cos_sim_matrix.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def open_app(app_dir):\n",
    "  os.startfile(app_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled-1.ipynb\n",
      "__pycache__\n",
      "data\n",
      "generate_summary.py\n",
      "inference.py\n",
      "langchain-ChatGLM\n",
      "main.py\n",
      "map.py\n",
      "operation\n",
      "operation_idx.csv\n",
      "renew_corpus.py\n",
      "requirements.txt\n",
      "search_doc.py\n",
      "search_doc_faiss.py\n",
      "search_doc_hf.py\n",
      "stop_criterion.py\n",
      "test_langchain.ipynb\n",
      "text2vec.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    '如何更换花呗绑定银行卡',\n",
    "    'A man is eating pasta.',\n",
    "    'Someone in a gorilla costume is playing a set of drums.',\n",
    "    'A cheetah chases prey on across a field.']\n",
    "\n",
    "text_right = [\n",
    "    '花呗更改绑定银行卡',\n",
    "    '我什么时候开通了花呗',\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A man is riding a horse.',\n",
    "    'A woman is playing violin.',\n",
    "    'Two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes libbitsandbytes_cuda116.dll\n",
      "function 'cadam32bit_grad_fp32' not found\n",
      "CUDA SETUP: Loading binary c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 21 files: 100%|██████████| 21/21 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Opti7080\\\\.cache\\\\huggingface\\\\hub\\\\models--THUDM--chatglm-6b\\\\snapshots\\\\1d240ba371910e9282298d4592532d7f0f3e9f3e'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"THUDM/chatglm-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)iguration_chatglm.py: 100%|██████████| 4.38k/4.38k [00:00<?, ?B/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading (…)/modeling_chatglm.py: 100%|██████████| 59.4k/59.4k [00:00<00:00, 192kB/s]\n",
      "Downloading (…)main/quantization.py: 100%|██████████| 31.0k/31.0k [00:00<00:00, 581kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.89G/3.89G [04:53<00:00, 13.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (word_embeddings): Embedding(130528, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GLMBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SelfAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): QuantizedLinear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): QuantizedLinear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GLU(\n",
       "          (dense_h_to_4h): QuantizedLinear(in_features=4096, out_features=16384, bias=True)\n",
       "          (dense_4h_to_h): QuantizedLinear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Opti7080\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Load default cpu kernel failed:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Opti7080/.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization.py\", line 178, in __init__\n",
      "    kernels = ctypes.cdll.LoadLibrary(kernel_file)\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\ctypes\\__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\ctypes\\__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: [WinError 193] %1 不是有效的 Win32 应用程序。\n",
      "\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息,简洁和专业的来回答用户的问题。\n",
      "如果无法从中得到答案,请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\",不允许在答案中添加编造成分,答案请使用中文。\n",
      "##已知内容:\n",
      "本机物理MAC地址是:cc:d9:ac:d8:57:06,\n",
      "本机电脑名是:DESKTOP-JHRQTGU.lenovo.com\n",
      "本机IP是:10.176.130.235\n",
      "本机硬盘序列号是:E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
      "本机CPU序列号是:BFEBFBFF000A0655\n",
      "本机主板序列号是:/41JYJ53/CNFCW0007K00LH/\n",
      "本机bios序列号是:41JYJ53\n",
      "##问题:\n",
      "我的MAC地址是什么\n",
      "##回答: 根据已知信息,您的物理MAC地址是:cc:d9:ac:d8:57:06。\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"本机物理MAC地址是：cc:d9:ac:d8:57:06,\n",
    "本机电脑名是：DESKTOP-JHRQTGU.lenovo.com\n",
    "本机IP是：10.176.130.235\n",
    "本机硬盘序列号是：E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
    "本机CPU序列号是：BFEBFBFF000A0655\n",
    "本机主板序列号是：/41JYJ53/CNFCW0007K00LH/\n",
    "本机bios序列号是：41JYJ53\"\"\"\n",
    "inp = '我的MAC地址是什么'\n",
    "input_text = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n",
    "如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。\n",
    "##已知内容:\n",
    "{context}\n",
    "##问题:\n",
    "{inp}\n",
    "##回答：\"\"\"\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=300,\n",
    "        temperature=0,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ChatGLMForConditionalGeneration.stream_chat at 0x0000021269AA1E70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stream_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai_user=pN1vaNEM/JFkEod5DPoEha|2023-06-08T02:24:16.518Z; ARRAffinity=4b55dda02c61fa6825af1a96193ff027e26cd4d1081dd1ee2ed88d88d93d93e9; ARRAffinitySameSite=4b55dda02c61fa6825af1a96193ff027e26cd4d1081dd1ee2ed88d88d93d93e9; .AspNetCore.Session=CfDJ8O%2ByR7SqPqxFlVz8eFNh6JFGsWDGqDT7Lzpzs4ImYU0jrnMFe6eWHIsbE52VYc8FRZV37J6ndDaxwjSxbvzE1fmeq53Mql%2B14DTKglIfKe9XpbajQ9UhJNbNo0v%2FYuluHZ5PoiEKJfG47olCOAp9n9QgGSclIv1Iak3PMKe9BclD; .AspNetCore.CookiesSSWeb=CfDJ8O-yR7SqPqxFlVz8eFNh6JEUTNyTeMBuOFEOcOPMwTcO-l0vrOFk0N9BKHtm_bRVY3W0u88P3I8fkCISyX_O7u15qf6PwF_D61PzBTgXJqHfBxQyHjf6rKXSu6YOduUWvsZG0qc4GNtZ7Jerm9yDkshdbEorp2nLmRpQ8obd91phvpKuqUxVl0avaXLOsBH8tOYY4FU7Zx6FtyNflVe6_-IkXRszZuU-9RLtwLSoKfMPjwMkfpUzUHECpB_puDrx4Hm6a6prhGkGKWifGk68HUZVUA9STh43cJklDVNwfwl6eBvUp8joU8TkaGoF1SHqN63HuOSpYv5gyjjS4Vd3Nls; ai_session=k434v/YYgul1PRSZB9GkaJ|1686191056621|1686192769861'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"请你帮我根据以下instruction的要求提取出关键信息：\n",
    "##instruction：要求格式为：\n",
    "- 是否为电脑操作：\n",
    "- 要求数字（未提及写未知）：\n",
    "##input：\n",
    "调整屏幕分辨率为1920*1080\n",
    "##output:\n",
    "\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.3,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<用户>:请帮我打开微信\n",
      "<ChatGLM-6B>: 很抱歉,作为一个语言模型,我无法直接打开微信应用程序。但是,您可以使用您的设备上的微信应用程序图标来打开它。如果该图标没有打开,您可以尝试在设备的设置中找到微信并打开它。\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"<用户>：请帮我打开微信\n",
    "<ChatGLM-6B>:\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=200,\n",
    "        temperature=0.1,\n",
    "        top_p = 0.95,\n",
    "        # repetition_penalty = 1.15,\n",
    "        # stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
    "        # do_sample = True\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0])\n",
    "    print(answer)\n",
    "    # item['infer_answer'] = answer\n",
    "    # print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39m'\u001b[39m\u001b[39m./operation/exe_location.json\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     hash_map \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(\u001b[39m'\u001b[39;49m\u001b[39m./operation/exe_location.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if os.path.isfile('./operation/exe_location.json'):\n",
    "    hash_map = json.loads('./operation/exe_location.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_map = {\n",
    "    'QQMusic.exe':'C:/Program Files (x86)\\Tencent\\QQMusic',\n",
    "    'msedge.exe':'C:/Program Files (x86)\\Microsoft\\Edge\\Application'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./operation/exe_location.json', 'w') as write_f:\n",
    "\tjson.dump(hash_map, write_f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QQMusic.exe': 'C:/Program Files (x86)\\\\Tencent\\\\QQMusic', 'msedge.exe': 'C:/Program Files (x86)\\\\Microsoft\\\\Edge\\\\Application'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open('./operation/exe_location.json','r') as load_f:\n",
    "    hash_map = json.load(load_f)\n",
    "    print(hash_map)\n",
    "    print(type(load_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import screen_brightness_control as sbc\n",
    "sbc.get_brightness()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbc.get_brightness()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.set_brightness(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes libbitsandbytes_cuda116.dll\n",
      "function 'cadam32bit_grad_fp32' not found\n",
      "CUDA SETUP: Loading binary c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like THUDM/chatglm-6b-int4 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:1291\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1291\u001b[0m         \u001b[39mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[0;32m   1292\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConnection error, and we cannot find the requested files in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m the disk cache. Please try again or make sure your Internet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m connection is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[39m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[1;31mLocalEntryNotFoundError\u001b[0m: Connection error, and we cannot find the requested files in the disk cache. Please try again or make sure your Internet connection is on.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel,LoraConfig,get_peft_model\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mTHUDM/chatglm-6b-int4\u001b[39;49m\u001b[39m\"\u001b[39;49m, load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mTHUDM/chatglm-6b-int4\u001b[39m\u001b[39m\"\u001b[39m, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39msupports_gradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:444\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[39mif\u001b[39;00m kwargs_copy\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    442\u001b[0m         _ \u001b[39m=\u001b[39m kwargs_copy\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 444\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    445\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    446\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    447\u001b[0m         trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    448\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    449\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_copy,\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[0;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    927\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    929\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    576\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    627\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    630\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    631\u001b[0m         configuration_file,\n\u001b[0;32m    632\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    633\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    634\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    635\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    636\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    637\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    638\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    639\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    640\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    641\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    642\u001b[0m     )\n\u001b[0;32m    643\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    644\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\hub.py:452\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[0;32m    451\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    453\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to load this file, couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find it in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m cached files and it looks like \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory containing a file named\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[0;32m    459\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like THUDM/chatglm-6b-int4 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from peft import PeftModel,LoraConfig,get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, AutoConfig,AutoModel\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
    "\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "peft_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM', inference_mode=True,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model, \"./tzh_model/medical_glm\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'QQMusic.exe': 'C:/Program Files (x86)\\\\Tencent\\\\QQMusic'}\n"
     ]
    }
   ],
   "source": [
    "from operation.operation import Operation3\n",
    "inps = {'inputs':input('Input: ')}\n",
    "test = Operation3(**inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: Matching Task [-h] [--document-corpus DOCUMENT_CORPUS]\n",
      "                     [--new-embed NEW_EMBED] [--document-embed DOCUMENT_EMBED]\n",
      "                     [--k K] [--device DEVICE] [--model-name MODEL_NAME]\n",
      "                     [--index_location INDEX_LOCATION]\n",
      "Matching Task: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"6374c16d-bbe1-4643-92c1-507310150e28\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=c:\\Users\\Opti7080\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-21952m58WaRixzUOk.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "test.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01.wav\n",
      "02.wav\n",
      "03.wav\n",
      "Untitled-1.ipynb\n",
      "__pycache__\n",
      "data\n",
      "exe_location.json\n",
      "generate_summary.py\n",
      "inference.py\n",
      "langchain-ChatGLM\n",
      "main.py\n",
      "map.py\n",
      "operation\n",
      "operation_idx.csv\n",
      "play.py\n",
      "recognition.py\n",
      "record.py\n",
      "renew_corpus.py\n",
      "requirements.txt\n",
      "result.wav\n",
      "search_doc.py\n",
      "search_doc_faiss.py\n",
      "search_doc_hf.py\n",
      "stop_criterion.py\n",
      "synthesis.py\n",
      "test_langchain.ipynb\n",
      "test_thred.py\n",
      "text2vec.ipynb\n",
      "zh.wav\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = tokenizer(queries, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings_que = model(**inputs)\n",
    "    embeddings_ans = model(**tokenizer(text_right, padding=True, truncation=True, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 22, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_que.last_hidden_state.shape,embeddings_que.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19582476, -0.22542259,  0.5878763 , ...,  0.51889557,\n",
       "        -0.97035754,  0.72239023],\n",
       "       [-0.36700755,  0.29916915,  0.31173083, ...,  1.2112145 ,\n",
       "         0.05619805,  0.93658125],\n",
       "       [-0.9832982 , -0.7832372 , -1.5102569 , ...,  1.4223759 ,\n",
       "        -1.4264588 ,  0.75840706],\n",
       "       [-1.4391133 ,  0.4106356 , -0.12989327, ..., -0.03974489,\n",
       "        -1.0173508 , -0.79071593]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_que.last_hidden_state[:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "import torch\n",
    "device = 'cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GanymedeNil/text2vec-large-chinese\")\n",
    "model = AutoModel.from_pretrained(\"GanymedeNil/text2vec-large-chinese\").to(device)\n",
    "\n",
    "def l2_normalization(data):\n",
    "    if data.ndim == 1:\n",
    "        return data / np.linalg.norm(data).reshape(-1,1)\n",
    "    else:\n",
    "        return data/np.linalg.norm(data,axis=1).reshape(-1,1)\n",
    "def cls_pooling(model_output,return_tensors=False):\n",
    "    if not return_tensors:\n",
    "        return l2_normalization(model_output.last_hidden_state[:,0].cpu().numpy())\n",
    "    else:\n",
    "        return l2_normalization(model_output.last_hidden_state[:,0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open('../data/document_corpus.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        corpus.append(line.strip('\\n'))\n",
    "def mean_pooling(model_output, attention_mask, return_tensors=False):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    output = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    if not return_tensors:\n",
    "        return output.cpu().numpy()\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "model.eval()\n",
    "input_data = tokenizer(corpus, padding=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    corpus_embeddings = model(**input_data.to(device))\n",
    "# corpus_embeddings = cls_pooling(corpus_embeddings)\n",
    "corpus_embeddings = mean_pooling(corpus_embeddings,attention_mask=input_data['attention_mask'])\n",
    "corpus_embeddings = l2_normalization(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  '帮我记录一下明天上午十点要买菜'\n",
    "input_data = tokenizer(query, padding=True, return_tensors=\"pt\")  \n",
    "model.eval()     \n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**input_data.to('cuda'))\n",
    "    # query_embeddings = cls_pooling(query_embeddings)\n",
    "    query_embeddings = mean_pooling(query_embeddings,attention_mask=input_data['attention_mask'])\n",
    "    feature_search = l2_normalization(query_embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23497748],\n",
       "       [0.2247234 ],\n",
       "       [0.27397212],\n",
       "       [0.2500224 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings@feature_search.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '进行事件提醒', '打开某个软件']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "features = l2_normalization(np.load(\"./data/document_embed.npy\"))\n",
    "dim = features.shape[1]\n",
    "index_ip = faiss.IndexFlatIP(dim)\n",
    "index_ip.add(features)\n",
    "print(index_ip.ntotal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['调节屏幕亮度', '调整屏幕分辨率', '打开软件', '询问显存', '询问GPU占用', '创建记事本备忘录'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::IndexIDMapTemplate<struct faiss::Index>::IndexIDMapTemplate(struct faiss::Index *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIDMap.cpp:32: Error: 'index->ntotal == 0' failed: index must be empty on input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\3782806113.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maux_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIDMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_ip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\faiss\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplacement_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0moriginal_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreferenced_objects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparameter_no\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python_tool\\Anaconda\\envs\\torch\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   9120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9121\u001b[1;33m         \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIDMap_swiginit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_IndexIDMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in __cdecl faiss::IndexIDMapTemplate<struct faiss::Index>::IndexIDMapTemplate(struct faiss::Index *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIDMap.cpp:32: Error: 'index->ntotal == 0' failed: index must be empty on input"
     ]
    }
   ],
   "source": [
    "aux_index = faiss.IndexIDMap(index_ip) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Loading Model\n",
      "[INFO]Creating Document Embeddings\n",
      "['调节屏幕亮度', '询问显存占用', '进行事件提醒', '打开软件', '询问GPU占用']\n",
      "[INFO]Finishing Saving Indexing\n"
     ]
    }
   ],
   "source": [
    "from search_doc_faiss import faiss_corpus\n",
    "torch.cuda.empty_cache()\n",
    "corpus = faiss_corpus()\n",
    "corpus.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建备忘录进行事项提醒', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01932951,  0.0241784 ,  0.02330983, ...,  0.02350627,\n",
       "        -0.03408533,  0.0198594 ], dtype=float32),\n",
       " array([-0.04149882,  0.04571409, -0.02394647, ..., -0.03245361,\n",
       "         0.04087428, -0.01606109], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.reconstruct(0),index_ip.reconstruct(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03973337,  0.00838783,  0.01816207, ..., -0.00785893,\n",
       "         -0.05274578,  0.00404025]], dtype=float32),\n",
       " array([-0.03973337,  0.00838783,  0.01816207, ..., -0.00785893,\n",
       "        -0.05274578,  0.00404025], dtype=float32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_normalization_1(features[1,:]),l2_normalization(features[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "query = '提醒我明天十点抢电影票'\n",
    "index = faiss.read_index('./data/test.index')\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to(device))\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search = query_embeddings\n",
    "# 检索最相似的topK个特征\n",
    "topK = 5\n",
    "D, I = index.search(feature_search, topK)\n",
    "# 返回的D表示相似度（或者距离）, I表示检索的topK个特征id（索引）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '帮我记录一下明天十点提醒帮我买菜'\n",
    "corpus_1 = \"\"\"进行事件提醒\"\"\"\n",
    "corpus_2 = \"\"\"调节屏幕亮度\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to(device))\n",
    "    feature_search = model(**tokenizer(corpus_1, padding=True, return_tensors=\"pt\").to(device))\n",
    "    feature_search_2 = model(**tokenizer(corpus_2, padding=True, return_tensors=\"pt\").to(device))\n",
    "feature_search = cls_pooling(feature_search)\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search_2 = cls_pooling(feature_search_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42430413]], dtype=float32), array([[0.3600703]], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings@feature_search.T,query_embeddings@feature_search_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1024), (1, 1024))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape,feature_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.38714254, 0.37287977, 0.3670662 , 0.3359719 , 0.30211502]],\n",
       "       dtype=float32),\n",
       " array([[0, 2, 3, 1, 4]], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建备忘录进行事项提醒', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.38714254, 0.3670662 , 0.36462313, 0.3359719 , 0.30211502]],\n",
       "       dtype=float32),\n",
       " array([[0, 3, 2, 1, 4]], dtype=int64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_ip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 保存索引\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m faiss\u001b[39m.\u001b[39mwrite_index(index_ip, \u001b[39m'\u001b[39m\u001b[39m./data/test.index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# 读取索引\u001b[39;00m\n\u001b[0;32m      5\u001b[0m index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mread_index(\u001b[39m'\u001b[39m\u001b[39m./data/test.index\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_ip' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存索引\n",
    "faiss.write_index(index_ip, './data/test.index')\n",
    "\n",
    "# 读取索引\n",
    "index = faiss.read_index('./data/test.index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls_pooling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     query_embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer(query, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m query_embeddings \u001b[39m=\u001b[39m cls_pooling(query_embeddings)\n\u001b[0;32m      6\u001b[0m feature_search \u001b[39m=\u001b[39m query_embeddings\n\u001b[0;32m      7\u001b[0m \u001b[39m# 检索最相似的topK个特征\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cls_pooling' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "query = '提醒我明天十点抢电影票'\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**tokenizer(query, padding=True, return_tensors=\"pt\").to('cuda'))\n",
    "query_embeddings = cls_pooling(query_embeddings)\n",
    "feature_search = query_embeddings\n",
    "# 检索最相似的topK个特征\n",
    "topK = 5\n",
    "D, I = index.search(feature_search, topK)\n",
    "# 返回的D表示相似度（或者距离）, I表示检索的topK个特征id（索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x000001A3B59EF900> >"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.7988719 , 0.56085855, 0.4258615 , 0.41730604, 0.3314243 ]],\n",
       "       dtype=float32),\n",
       " array([[1, 5, 3, 0, 2]], dtype=int64))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['调节屏幕亮度', '询问显存占用', '创建记事本备忘录', '调整屏幕分辨率', '打开软件', '询问GPU占用']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38731813"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    " \n",
    " \n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=1)\n",
    "    sigma = np.std(data, axis=1)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "\n",
    "\n",
    "np.sum(l2_normalization(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(l2_normalization(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19582501, -0.2254237 ,  0.5878755 , ...,  0.51889545,\n",
       "        -0.9703573 ,  0.7223894 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[835.5797 , 564.0149 , 397.68317, 339.4082 , 246.25342]],\n",
       "       dtype=float32),\n",
       " array([[0, 1, 4, 3, 2]], dtype=int64))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_ids = faiss.IndexFlatIP(2048)\n",
    "index_ids = faiss.IndexIDMap(index_ids)\n",
    "\n",
    "# 添加特征，并指定id，注意添加的id类型为int64\n",
    "ids = 20\n",
    "feature_ids = np.random.random((1, 2048)).astype('float32')\n",
    "index_ids.add_with_ids(feature_ids, np.array((ids,)).astype('int64'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x000001D6C6EFF570> >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9465, 0.6756, 0.2798, 0.3837, 0.4044, 0.2968, 0.2742, 0.3308, 0.3206,\n",
      "         0.4471, 0.4081],\n",
      "        [0.3055, 0.2763, 0.7725, 0.6567, 0.3291, 0.7498, 0.2632, 0.4862, 0.5845,\n",
      "         0.4109, 0.4650],\n",
      "        [0.3777, 0.3504, 0.3729, 0.5051, 0.5157, 0.4793, 0.5053, 0.4642, 0.5326,\n",
      "         0.6118, 0.5007],\n",
      "        [0.3411, 0.3380, 0.3204, 0.4363, 0.4000, 0.4164, 0.3400, 0.4188, 0.5530,\n",
      "         0.3807, 0.5776]])\n"
     ]
    }
   ],
   "source": [
    "from text2vec import cos_sim\n",
    "cos_similarity = cos_sim(embeddings_que.last_hidden_state[:,0],embeddings_ans.last_hidden_state[:,0])\n",
    "print(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  2,  9, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2vec import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9719, 0.8737, 0.7577, 0.7623, 0.7647, 0.7870, 0.7640, 0.7554, 0.7812,\n",
       "         0.7794, 0.7791],\n",
       "        [0.7596, 0.8073, 0.9713, 0.9700, 0.8659, 0.9233, 0.8840, 0.8633, 0.9166,\n",
       "         0.8967, 0.9084],\n",
       "        [0.7571, 0.7907, 0.8844, 0.8987, 0.8838, 0.9239, 0.9227, 0.8731, 0.9319,\n",
       "         0.9432, 0.9264],\n",
       "        [0.7539, 0.7718, 0.8788, 0.9024, 0.8693, 0.9033, 0.8655, 0.8638, 0.9149,\n",
       "         0.8890, 0.9299]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import screen_brightness_control as sbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbc.get_brightness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc.set_brightness(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 8192.0 MB\n",
      "Free memory: 6604.09375 MB\n",
      "Used memory: 1587.90625 MB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(f\"Total memory: {info.total/1024**2} MB\")\n",
    "print(f\"Free memory: {info.free/1024**2} MB\")\n",
    "print(f\"Used memory: {info.used/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 8192.00 MB\n",
      "Free memory: 6604.09 MB\n",
      "Used memory: 1587.91 MB\n"
     ]
    }
   ],
   "source": [
    "# from operation.screen_brightness import operation\n",
    "# opt = operation(50)\n",
    "# opt.fit()\n",
    "from operation.display_graphic import operation\n",
    "opt = operation()\n",
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operation import operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = 0\n",
    "inp = {\"brightness\":100}\n",
    "opt = eval(f\"operation.Operation{selected_idx}\")(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'已调至100%'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_suffix():\n",
    "    return time.strftime(\"%Y%m%d%H%M%S\", time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230529144958'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "time_suffix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LenoMate\\\\20230529145105\\\\.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('LenoMate',time_suffix(),'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary='love'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LenoMate_love_20230529145339.txt'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'LenoMate_'+summary+'_'+time_suffix()+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_args_kwargs(arg1, arg2, arg3,arg4=5):\n",
    "    print(\"arg1:\", arg1)\n",
    "    print(\"arg2:\", arg2)\n",
    "    print(\"arg3:\", arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg1: 5\n",
      "arg2: two\n",
      "arg3: 3\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"arg3\": 3, \"arg2\": \"two\", \"arg1\": 5}\n",
    "test_args_kwargs(**kwargs)\n",
    "\n",
    "#result\n",
    "arg1: 5\n",
    "arg2: 'two'\n",
    "arg3: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "default_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "class operation():\n",
    "    def __init__(self,inputs,path=default_path,summary='notebook'):\n",
    "        self.path = path\n",
    "        self.inputs = inputs\n",
    "        self.summary = summary\n",
    "    def fit(self):\n",
    "        file_name = 'LenoMate_'+self.summary+'_'+time_suffix()+'.txt'\n",
    "        file_name = os.path.join(default_path,file_name)\n",
    "        with open(file_name,'w',encoding='utf-8') as f:\n",
    "            f.write(self.inputs)\n",
    "        f.close()\n",
    "inp = {'inputs':'s','summary':'love'}\n",
    "tst = operation(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10', '10']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string=\"\"\" instruction:\n",
    "- 任务: 调整屏幕亮度\n",
    "- 要求数字: 10\n",
    "output:\n",
    "- 将屏幕亮度调低到10\"\"\"\n",
    "re.findall(r\"\\d+\\.?\\d*\",string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载中...\n",
      "模型加载完成\n",
      "模型输出： 2\n",
      "模型输出： 4\n",
      "模型输出： 6\n",
      "模型输出： 8\n",
      "模型输出： 10\n",
      "模型输出： 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-3 (load_and_run_model):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Python_tool\\Anaconda\\envs\\torch\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Opti7080\\AppData\\Local\\Temp\\ipykernel_8504\\1912693514.py\", line 19, in load_and_run_model\n",
      "  File \"<string>\", line 0\n",
      "    \n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "\n",
    "# 模拟加载和运行模型的函数\n",
    "def load_and_run_model(input_queue):\n",
    "    # 模型加载过程，可以根据实际情况进行编写\n",
    "    print(\"模型加载中...\")\n",
    "    # 模型加载完成\n",
    "    print(\"模型加载完成\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # 从输入队列中获取输入数据\n",
    "            input_data = input_queue.get()\n",
    "            # 在这里进行模型处理\n",
    "            # 模型处理逻辑...\n",
    "            # 将输入数据转换为numpy数组\n",
    "            input_array = eval(input_data)\n",
    "            # 运行模型，这里我们简单地将输入数据乘以2\n",
    "            output_array = input_array * 2\n",
    "            # 将结果转换为字符串\n",
    "            output_data = str(output_array)\n",
    "\n",
    "            # 处理完成后，可以将结果返回给主线程，或者进行其他操作\n",
    "            print(\"模型输出：\", output_data)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "# 创建一个输入队列\n",
    "input_queue = queue.Queue()\n",
    "\n",
    "# 创建一个线程，用于加载和运行模型\n",
    "model_thread = threading.Thread(target=load_and_run_model, args=(input_queue,))\n",
    "# 设置线程为后台线程，使程序可以退出\n",
    "model_thread.daemon = True\n",
    "# 启动线程\n",
    "model_thread.start()\n",
    "\n",
    "while True:\n",
    "    # 接收用户输入\n",
    "    input_data = input(\"请输入数据（输入'exit'退出）：\")\n",
    "    if input_data == \"exit\":\n",
    "        break\n",
    "    # 将输入数据放入输入队列\n",
    "    input_queue.put(input_data)\n",
    "\n",
    "# 退出程序时，清空输入队列并等待模型线程结束\n",
    "input_queue.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "answer = \"\"\"\n",
    "数字:未知\n",
    "\"\"\"\n",
    "try:\n",
    "    num = int(re.findall(r\"\\d+\\.?\\d*\",answer.split('数字:')[1])[0])\n",
    "except:\n",
    "    num = answer.split('数字:')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m num\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num' is not defined"
     ]
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('ffmpeg -y -i data/voice1.wav -ac 1 -ar 16000 data/voice.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc:d9:ac:d8:57:06\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def get_mac_address():\n",
    "    mac=uuid.UUID(int = uuid.getnode()).hex[-12:]\n",
    "    return \":\".join([mac[e:e+2] for e in range(0,11,2)])\n",
    "\n",
    "print(get_mac_address())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "电脑名: DESKTOP-JHRQTGU.lenovo.com\n",
      "ip地址: 10.176.130.235\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "def get_computer_name_ip():\n",
    "    #获取本机电脑名\n",
    "    name = socket.getfqdn(socket.gethostname())\n",
    "    #获取本机ip\n",
    "    addr = socket.gethostbyname(socket.gethostname())\n",
    "    return name,addr\n",
    "\n",
    "myname,myaddr = get_computer_name_ip()\n",
    "print('电脑名:',myname)\n",
    "print('ip地址:',myaddr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "硬盘序列号 E823_8FA6_BF53_0001_001B_444A_4423_4BF2.\n",
      "CPU序列号 BFEBFBFF000A0655\n",
      "主板序列号 /41JYJ53/CNFCW0007K00LH/\n",
      "mac地址 None\n",
      "mac地址 CC:D9:AC:D8:57:02\n",
      "mac地址 A4:BB:6D:CE:5A:8C\n",
      "mac地址 CC:D9:AC:D8:57:06\n",
      "mac地址 CC:D9:AC:D8:57:03\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 None\n",
      "mac地址 BC:9B:20:52:41:53\n",
      "mac地址 BA:7F:20:52:41:53\n",
      "mac地址 BA:8A:20:52:41:53\n",
      "mac地址 CE:D9:AC:D8:57:02\n",
      "bios序列号 41JYJ53\n"
     ]
    }
   ],
   "source": [
    "import wmi\n",
    "\n",
    "c = wmi.WMI()\n",
    "\n",
    "# # 硬盘序列号\n",
    "for physical_disk in c.Win32_DiskDrive():\n",
    "    print(\"硬盘序列号\", physical_disk.SerialNumber)\n",
    "\n",
    "# CPU序列号\n",
    "for cpu in c.Win32_Processor():\n",
    "    print(\"CPU序列号\", cpu.ProcessorId.strip())\n",
    "\n",
    "# 主板序列号\n",
    "for board_id in c.Win32_BaseBoard():\n",
    "    print(\"主板序列号\", board_id.SerialNumber)\n",
    "\n",
    "# mac地址\n",
    "for mac in c.Win32_NetworkAdapter():\n",
    "    print(\"mac地址\", mac.MACAddress)\n",
    "\n",
    "# bios序列号\n",
    "for bios_id in c.Win32_BIOS():\n",
    "    print(\"bios序列号\", bios_id.SerialNumber.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/41JYJ53/CNFCW0007K00LH/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.Win32_BaseBoard()[0].SerialNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41JYJ53'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.Win32_BIOS()[0].SerialNumber.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DESKTOP-JHRQTGU.lenovo.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socket.getfqdn(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.176.130.235'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
